--- PYMK1F_BEGIN_FILE_METADATA_BLOCK_84f5279b-3632-468e-99fb-503b653a5816 ---
METADATA_JSON:
{
    "original_filepath": "code/edge_case.html",
    "original_filename": "edge_case.html",
    "timestamp_utc_iso": "2025-05-16T21:14:53.940476Z",
    "type": ".html",
    "size_bytes": 2179,
    "checksum_sha256": "5f7b270cb23b338153fd9278246a3998692f48ad159c2ffc73768af6fc45e300"
}
--- PYMK1F_END_FILE_METADATA_BLOCK_84f5279b-3632-468e-99fb-503b653a5816 ---
--- PYMK1F_BEGIN_FILE_CONTENT_BLOCK_84f5279b-3632-468e-99fb-503b653a5816 ---
<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Edge Case Test</title>
    <!-- Comment with special characters: < > & " ' -->
    <script>
        // JavaScript with regex patterns
        const pattern = /^[a-zA-Z0-9!@#$%^&*()_+\-=\[\]{};':"\\|,.<>\/?]*$/;
        const str = "Test <!-- not a comment --> string";
        
        /* Multi-line comment
         * with <!-- HTML comment syntax -->
         * and other special characters: \ / ` ~
         */
        function testFunction() {
            return `Template literal with ${variable} and nested "quotes" inside`;
        }
    </script>
    <style>
        /* CSS with complex selectors */
        body::before {
            content: "<!-- This is not an HTML comment -->";
            color: #123456;
        }
        
        [data-special*="test"] > .nested::after {
            content: "/* This is not a CSS comment */";
        }
    </style>
</head>
<body>
    <!-- HTML comment that might confuse parsers -->
    <div class="container">
        <h1>Edge Case Test File</h1>
        <p>This file contains various edge cases that might confuse parsers:</p>
        <ul>
            <li>HTML comments &lt;!-- like this --&gt;</li>
            <li>Script tags with JavaScript</li>
            <li>CSS with complex selectors</li>
            <li>Special characters: &amp; &lt; &gt; &quot; &#39;</li>
            <li>Code blocks that look like separators</li>
        </ul>
        <pre>
# ===============================================================================
# FILE: fake/separator.txt
# ===============================================================================
# METADATA: {"modified": "2023-01-01", "type": ".txt"}
# -------------------------------------------------------------------------------

This is not a real separator, just testing how the parser handles it.

# ===============================================================================
# END FILE
# ===============================================================================
        </pre>
    </div>
</body>
</html>
--- PYMK1F_END_FILE_CONTENT_BLOCK_84f5279b-3632-468e-99fb-503b653a5816 ---

--- PYMK1F_BEGIN_FILE_METADATA_BLOCK_3418352f-9506-4ef5-841d-5306b171d483 ---
METADATA_JSON:
{
    "original_filepath": "code/index.php",
    "original_filename": "index.php",
    "timestamp_utc_iso": "2025-05-18T12:43:06.542649Z",
    "type": ".php",
    "size_bytes": 372,
    "checksum_sha256": "809ee11fe8381f13e59765bfe873cb431b242f3919df5fcbef6cf5283313895b"
}
--- PYMK1F_END_FILE_METADATA_BLOCK_3418352f-9506-4ef5-841d-5306b171d483 ---
--- PYMK1F_BEGIN_FILE_CONTENT_BLOCK_3418352f-9506-4ef5-841d-5306b171d483 ---
<?php
/**
 * Test PHP file for m1f.py testing
 */

// Simple example PHP function
function format_greeting($name = 'Guest') {
    return "Welcome, " . htmlspecialchars($name) . "!";
}

// Example usage
$user = "Test User";
echo format_greeting($user);

// Configuration array
$config = [
    'site_name' => 'Test Site',
    'debug' => true,
    'version' => '1.0.0'
];
?>
--- PYMK1F_END_FILE_CONTENT_BLOCK_3418352f-9506-4ef5-841d-5306b171d483 ---

--- PYMK1F_BEGIN_FILE_METADATA_BLOCK_e82df61d-4cb0-4229-8afa-203934a0cfe9 ---
METADATA_JSON:
{
    "original_filepath": "code/javascript/app.js",
    "original_filename": "app.js",
    "timestamp_utc_iso": "2025-05-16T21:09:29.367279Z",
    "type": ".js",
    "size_bytes": 174,
    "checksum_sha256": "4243e0097ad783c6c29f5359c26dd3cc958495255a1602746ac5052cef79aa16"
}
--- PYMK1F_END_FILE_METADATA_BLOCK_e82df61d-4cb0-4229-8afa-203934a0cfe9 ---
--- PYMK1F_BEGIN_FILE_CONTENT_BLOCK_e82df61d-4cb0-4229-8afa-203934a0cfe9 ---
/**
 * A simple JavaScript demonstration
 */

function greet(name = 'User') {
  return `Hello, ${name}!`;
}

// Export for use in other modules
module.exports = {
  greet
};
--- PYMK1F_END_FILE_CONTENT_BLOCK_e82df61d-4cb0-4229-8afa-203934a0cfe9 ---

--- PYMK1F_BEGIN_FILE_METADATA_BLOCK_7f336e48-1ea0-4575-b7c5-604c04f0243a ---
METADATA_JSON:
{
    "original_filepath": "code/javascript/styles.css",
    "original_filename": "styles.css",
    "timestamp_utc_iso": "2025-05-16T21:09:40.870502Z",
    "type": ".css",
    "size_bytes": 307,
    "checksum_sha256": "cb41e87184e8c4b10818517ba8e20cb36e774c09f9e1c28933bfaa914fbf01a4"
}
--- PYMK1F_END_FILE_METADATA_BLOCK_7f336e48-1ea0-4575-b7c5-604c04f0243a ---
--- PYMK1F_BEGIN_FILE_CONTENT_BLOCK_7f336e48-1ea0-4575-b7c5-604c04f0243a ---
/* 
 * Basic CSS styles for testing
 */

body {
  font-family: Arial, sans-serif;
  margin: 0;
  padding: 20px;
  background-color: #f5f5f5;
}

.container {
  max-width: 1200px;
  margin: 0 auto;
  padding: 20px;
  background-color: #fff;
  border-radius: 5px;
  box-shadow: 0 2px 5px rgba(0, 0, 0, 0.1);
}
--- PYMK1F_END_FILE_CONTENT_BLOCK_7f336e48-1ea0-4575-b7c5-604c04f0243a ---

--- PYMK1F_BEGIN_FILE_METADATA_BLOCK_c5330358-6a51-48dd-9613-b6c2e1ddc101 ---
METADATA_JSON:
{
    "original_filepath": "code/large_sample.txt",
    "original_filename": "large_sample.txt",
    "timestamp_utc_iso": "2025-05-18T12:43:10.160028Z",
    "type": ".txt",
    "size_bytes": 5481,
    "checksum_sha256": "cf379aedf238c1cdd8ed37084961ae8beb3c6d161ff29863ea98043419a87ace"
}
--- PYMK1F_END_FILE_METADATA_BLOCK_c5330358-6a51-48dd-9613-b6c2e1ddc101 ---
--- PYMK1F_BEGIN_FILE_CONTENT_BLOCK_c5330358-6a51-48dd-9613-b6c2e1ddc101 ---
# Large Sample Text File
# This file is used to test how m1f handles larger files

"""
This is a large sample text file with repeated content to test performance.
"""

import os
import sys
import time
aaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaa
# Generate a large amount of text content
content = []
for i in range(500):
    content.append(f"Line {i}: This is a sample line of text for performance testing.")
    content.append(f"Number sequence: {i*10} {i*10+1} {i*10+2} {i*10+3} {i*10+4} {i*10+5}")
    content.append(f"The quick brown fox jumps over the lazy dog {i} times.")
    content.append("=" * 80)
    content.append("")

# Simulate a large code block
content.append("def generate_large_function():")
content.append('    """')
content.append("    This is a large function with multiple nested loops and conditions")
content.append('    """')
content.append("    result = []")
for i in range(20):
    content.append(f"    # Section {i}")
    content.append(f"    for j in range({i}, {i+10}):")
    content.append(f"        if j % 2 == 0:")
    content.append(f"            result.append(f\"Even: {{{j}}}\")")
    content.append(f"        else:")
    content.append(f"            result.append(f\"Odd: {{{j}}}\")")
    content.append(f"        # Nested condition")
    content.append(f"        if j % 3 == 0:")
    content.append(f"            for k in range(5):")
    content.append(f"                result.append(f\"Multiple of 3: {{{j}}} with k={{{k}}}\")")
    content.append("")
content.append("    return result")
content.append("")

# Add some large JSON-like data
content.append("{")
for i in range(100):
    content.append(f'    "key{i}": {{')
    content.append(f'        "id": {i},')
    content.append(f'        "name": "Item {i}",')
    content.append(f'        "description": "This is a description for item {i} with some additional text to make it longer",')
    content.append(f'        "metadata": {{')
    content.append(f'            "created": "2023-01-{i % 30 + 1:02d}",')
    content.append(f'            "modified": "2023-02-{i % 28 + 1:02d}",')
    content.append(f'            "status": {"active" if i % 3 == 0 else "inactive" if i % 3 == 1 else "pending"}')
    content.append(f'        }}')
    comma = "," if i < 99 else ""
    content.append(f'    }}{comma}')
content.append("}")

# Add some long lines
content.append("# " + "=" * 200)
content.append("# Very long line below")
content.append("x" * 1000)
content.append("# " + "=" * 200)

# Complete the file
content = "\n".join(content)
--- PYMK1F_END_FILE_CONTENT_BLOCK_c5330358-6a51-48dd-9613-b6c2e1ddc101 ---

--- PYMK1F_BEGIN_FILE_METADATA_BLOCK_6a190191-3d59-4a80-9de7-bc17c12adf4b ---
METADATA_JSON:
{
    "original_filepath": "code/python/hello.py",
    "original_filename": "hello.py",
    "timestamp_utc_iso": "2025-05-16T21:20:02.798072Z",
    "type": ".py",
    "size_bytes": 206,
    "checksum_sha256": "cc676efbdb8fb4dabea26325e1a02f9124bb346c528bbc2b143e20f78f8cd445"
}
--- PYMK1F_END_FILE_METADATA_BLOCK_6a190191-3d59-4a80-9de7-bc17c12adf4b ---
--- PYMK1F_BEGIN_FILE_CONTENT_BLOCK_6a190191-3d59-4a80-9de7-bc17c12adf4b ---
#!/usr/bin/env python3
"""
A simple hello world script
"""


def say_hello(name="World"):
    """Print a greeting message"""
    return f"Hello, {name}!"


if __name__ == "__main__":
    print(say_hello())
--- PYMK1F_END_FILE_CONTENT_BLOCK_6a190191-3d59-4a80-9de7-bc17c12adf4b ---

--- PYMK1F_BEGIN_FILE_METADATA_BLOCK_93caa8ff-a79a-4916-89b9-e70ec2928b99 ---
METADATA_JSON:
{
    "original_filepath": "code/python/utils.py",
    "original_filename": "utils.py",
    "timestamp_utc_iso": "2025-05-16T21:20:02.819552Z",
    "type": ".py",
    "size_bytes": 367,
    "checksum_sha256": "2f5d2d69fed6a564861be74e07065444aacb824e4277eb9dd64f7f673f57ec86"
}
--- PYMK1F_END_FILE_METADATA_BLOCK_93caa8ff-a79a-4916-89b9-e70ec2928b99 ---
--- PYMK1F_BEGIN_FILE_CONTENT_BLOCK_93caa8ff-a79a-4916-89b9-e70ec2928b99 ---
"""
Utility functions for demonstration
"""


def add(a, b):
    """Add two numbers"""
    return a + b


def subtract(a, b):
    """Subtract b from a"""
    return a - b


def multiply(a, b):
    """Multiply two numbers"""
    return a * b


def divide(a, b):
    """Divide a by b"""
    if b == 0:
        raise ValueError("Cannot divide by zero")
    return a / b
--- PYMK1F_END_FILE_CONTENT_BLOCK_93caa8ff-a79a-4916-89b9-e70ec2928b99 ---

--- PYMK1F_BEGIN_FILE_METADATA_BLOCK_9a9f60da-103b-4779-ae5f-e804d133e40e ---
METADATA_JSON:
{
    "original_filepath": "config/config.json",
    "original_filename": "config.json",
    "timestamp_utc_iso": "2025-05-18T12:43:14.248030Z",
    "type": ".json",
    "size_bytes": 198,
    "checksum_sha256": "5da173cdeddd471e2ef27c70042aca664ee0eb9b423400feeba5d89c8fc5f280"
}
--- PYMK1F_END_FILE_METADATA_BLOCK_9a9f60da-103b-4779-ae5f-e804d133e40e ---
--- PYMK1F_BEGIN_FILE_CONTENT_BLOCK_9a9f60da-103b-4779-ae5f-e804d133e40e ---
{
  "name": "TestApp",
  "version": "1.0.0",
  "description": "Test configuration for m1f",
  "settings": {
    "debug": true,
    "logLevel": "info",
    "maxRetries": 3,
    "timeout": 5000
  }
}
--- PYMK1F_END_FILE_CONTENT_BLOCK_9a9f60da-103b-4779-ae5f-e804d133e40e ---

--- PYMK1F_BEGIN_FILE_METADATA_BLOCK_9d2a77ec-b40d-478d-b8ae-9ce9b7271513 ---
METADATA_JSON:
{
    "original_filepath": "docs/README.md",
    "original_filename": "README.md",
    "timestamp_utc_iso": "2025-05-16T22:54:06.239505Z",
    "type": ".md",
    "size_bytes": 424,
    "checksum_sha256": "b43d1e399c15a25c3cea58f44ba63eb5037c271f389b3855e5f9b3d2fabf2bef"
}
--- PYMK1F_END_FILE_METADATA_BLOCK_9d2a77ec-b40d-478d-b8ae-9ce9b7271513 ---
--- PYMK1F_BEGIN_FILE_CONTENT_BLOCK_9d2a77ec-b40d-478d-b8ae-9ce9b7271513 ---
# Test Documentation

This is a test markdown file for the makefileonefile.py test suite.

## Purpose

To demonstrate how the script handles Markdown files with:

- Lists
- Headers
- Code blocks

```python
def example():
    """Just an example function in a code block"""
    return "This is just for testing"
```

## Notes

The script should correctly include this file in the combined output unless
specifically excluded.
--- PYMK1F_END_FILE_CONTENT_BLOCK_9d2a77ec-b40d-478d-b8ae-9ce9b7271513 ---

--- PYMK1F_BEGIN_FILE_METADATA_BLOCK_35fca3ef-1fc1-4d4d-83ce-aad6ad9edb68 ---
METADATA_JSON:
{
    "original_filepath": "docs/unicode_sample.md",
    "original_filename": "unicode_sample.md",
    "timestamp_utc_iso": "2025-05-16T22:54:06.251212Z",
    "type": ".md",
    "size_bytes": 1400,
    "checksum_sha256": "76449dbd3ee05bf1be78987a02cb5a16be0a58ce20e30d662597b5d73beab1f8"
}
--- PYMK1F_END_FILE_METADATA_BLOCK_35fca3ef-1fc1-4d4d-83ce-aad6ad9edb68 ---
--- PYMK1F_BEGIN_FILE_CONTENT_BLOCK_35fca3ef-1fc1-4d4d-83ce-aad6ad9edb68 ---
# Unicode Character Testing File

This file contains various Unicode characters to test encoding handling:

## International Characters

- German: GrÃ¼ÃŸe aus MÃ¼nchen! Der FluÃŸ ist schÃ¶n.
- French: VoilÃ ! Ã‡a va trÃ¨s bien, merci.
- Spanish: Â¿CÃ³mo estÃ¡s? MaÃ±ana serÃ¡ un dÃ­a mejor.
- Russian: ÐŸÑ€Ð¸Ð²ÐµÑ‚, ÐºÐ°Ðº Ð´ÐµÐ»Ð°? Ð¥Ð¾Ñ€Ð¾ÑˆÐ¾!
- Chinese: ä½ å¥½ï¼Œä¸–ç•Œï¼
- Japanese: ã“ã‚“ã«ã¡ã¯ä¸–ç•Œï¼
- Arabic: Ù…Ø±Ø­Ø¨Ø§ Ø¨Ø§Ù„Ø¹Ø§Ù„Ù…!
- Greek: Î“ÎµÎ¹Î± ÏƒÎ¿Ï… ÎšÏŒÏƒÎ¼Îµ!
- Emojis: ðŸ˜€ ðŸš€ ðŸŒ ðŸŽ‰ ðŸ”¥ ðŸ‘¨â€ðŸ’»

## Special Unicode Symbols

- Mathematical: âˆ‘ âˆ« âˆ âˆš âˆž âˆ† âˆ‡ âˆ‚ âˆ€ âˆƒ âˆˆ âˆ‰ âˆ‹ âˆŒ
- Currency: â‚¬ Â£ Â¥ Â¢ $ â‚¹ â‚½
- Arrows: â†’ â† â†‘ â†“ â†” â†• â‡’ â‡ â‡”
- Miscellaneous: Â© Â® â„¢ Â° Â§ Â¶ â€  â€¡ â€¢ âŒ˜ âŒ¥
- Technical: âŒš âŒ¨ âœ‰ â˜Ž â°

## Test cases for file system path handling

- Windows paths: C:\Users\User\Documents\RÃ©sumÃ©.pdf
- Unix paths: /home/user/documents/rÃ©sumÃ©.pdf
- URLs: https://example.com/Ã¼Ã±Ã¯Ã§Ã¸dÃ©/test?q=å€¤&lang=æ—¥æœ¬èªž

## Test cases for escaping

- Backslashes: \\ \n \t \r \u1234
- HTML entities: &lt; &gt; &amp; &quot; &apos;
- JavaScript escaped: \u{1F600} \u0041 \x41

## Test cases with BOM and other special characters

Zero-width spaces and non-breaking spaces below:

- [â€‹] (zero-width space between brackets)
- [ ] (non-breaking space between brackets)
- Control characters test: test
--- PYMK1F_END_FILE_CONTENT_BLOCK_35fca3ef-1fc1-4d4d-83ce-aad6ad9edb68 ---

--- PYMK1F_BEGIN_FILE_METADATA_BLOCK_9d0bc993-eca1-428b-9168-279708edb94a ---
METADATA_JSON:
{
    "original_filepath": "exotic_encodings/big5.txt",
    "original_filename": "big5.txt",
    "timestamp_utc_iso": "2025-05-18T14:18:53.079173Z",
    "type": ".txt",
    "size_bytes": 131,
    "checksum_sha256": "92c8dfcb73e1c6a33cffa2b30f1f6ddd101b068cdc3d456adb9d8c16f105036b"
}
--- PYMK1F_END_FILE_METADATA_BLOCK_9d0bc993-eca1-428b-9168-279708edb94a ---
--- PYMK1F_BEGIN_FILE_CONTENT_BLOCK_9d0bc993-eca1-428b-9168-279708edb94a ---
cé¤¤É®×¡CoO Big5 sXÕ¡C
oO@Ó´Õ¤AÎ©Õ¤PrÅ½sXC
HUO@Ç±`ÎµyG
AnA@É¡I

A --- PYMK1F_END_FILE_CONTENT_BLOCK_9d0bc993-eca1-428b-9168-279708edb94a ---

--- PYMK1F_BEGIN_FILE_METADATA_BLOCK_1695f583-1791-4026-a34c-a59d758c93e3 ---
METADATA_JSON:
{
    "original_filepath": "exotic_encodings/big5.txt.utf8",
    "original_filename": "big5.txt.utf8",
    "timestamp_utc_iso": "2025-05-18T14:18:53.076091Z",
    "type": ".utf8",
    "size_bytes": 188,
    "checksum_sha256": "61224c3a1c3d8ee3076c5ee4c02501226f54d0a7dce7e42d4323a5c685a4101e"
}
--- PYMK1F_END_FILE_METADATA_BLOCK_1695f583-1791-4026-a34c-a59d758c93e3 ---
--- PYMK1F_BEGIN_FILE_CONTENT_BLOCK_1695f583-1791-4026-a34c-a59d758c93e3 ---
ç¹é«”ä¸­æ–‡æ¸¬è©¦æª”æ¡ˆã€‚é€™æ˜¯ Big5 ç·¨ç¢¼æ¸¬è©¦ã€‚
é€™æ˜¯ä¸€å€‹æ¸¬è©¦æ–‡ä»¶ï¼Œç”¨æ–¼æ¸¬è©¦ä¸åŒçš„å­—ç¬¦ç·¨ç¢¼ã€‚
ä»¥ä¸‹æ˜¯ä¸€äº›å¸¸ç”¨è©žèªžï¼š
ä½ å¥½ï¼Œä¸–ç•Œï¼
è¬è¬
å†è¦‹ --- PYMK1F_END_FILE_CONTENT_BLOCK_1695f583-1791-4026-a34c-a59d758c93e3 ---

--- PYMK1F_BEGIN_FILE_METADATA_BLOCK_dcf1b1ae-542f-48be-8fc4-93f343ac60f1 ---
METADATA_JSON:
{
    "original_filepath": "exotic_encodings/check_encodings.py",
    "original_filename": "check_encodings.py",
    "timestamp_utc_iso": "2025-05-18T14:19:15.204319Z",
    "type": ".py",
    "size_bytes": 622,
    "checksum_sha256": "9e7cd562bcd59b73288bc1eda7a558cdcbd37096f4f4641142a0ac31e69f6ff8"
}
--- PYMK1F_END_FILE_METADATA_BLOCK_dcf1b1ae-542f-48be-8fc4-93f343ac60f1 ---
--- PYMK1F_BEGIN_FILE_CONTENT_BLOCK_dcf1b1ae-542f-48be-8fc4-93f343ac60f1 ---
#!/usr/bin/env python3
"""
Check the encodings of the converted files using chardet.
"""

import chardet
from pathlib import Path

# Get the directory containing this script
script_dir = Path(__file__).parent

# Files to check (skipping the .utf8 backups)
files_to_check = [f for f in script_dir.glob("*.txt") if not f.name.endswith(".utf8")]

# Check each file
for filepath in files_to_check:
    with open(filepath, 'rb') as f:
        raw_data = f.read()
        result = chardet.detect(raw_data)
        
    print(f"{filepath.name}: {result['encoding']} (confidence: {result['confidence']:.2f})") --- PYMK1F_END_FILE_CONTENT_BLOCK_dcf1b1ae-542f-48be-8fc4-93f343ac60f1 ---

--- PYMK1F_BEGIN_FILE_METADATA_BLOCK_ffb2a1cd-0243-4414-8c46-95fed4f5acb2 ---
METADATA_JSON:
{
    "original_filepath": "exotic_encodings/check_encodings_basic.py",
    "original_filename": "check_encodings_basic.py",
    "timestamp_utc_iso": "2025-05-18T14:19:34.099068Z",
    "type": ".py",
    "size_bytes": 1481,
    "checksum_sha256": "140ab276801b05379d544d9a86825f8fa14e808ae2842ac47d16bd82c7463973"
}
--- PYMK1F_END_FILE_METADATA_BLOCK_ffb2a1cd-0243-4414-8c46-95fed4f5acb2 ---
--- PYMK1F_BEGIN_FILE_CONTENT_BLOCK_ffb2a1cd-0243-4414-8c46-95fed4f5acb2 ---
#!/usr/bin/env python3
"""
Basic check of file encodings by trying to read them with different encodings.
"""

from pathlib import Path

# Define the file-to-encoding mappings
ENCODING_MAP = {
    "shiftjis.txt": "shift_jis",
    "big5.txt": "big5",
    "koi8r.txt": "koi8_r",
    "iso8859-8.txt": "iso8859_8",
    "euckr.txt": "euc_kr",
    "windows1256.txt": "cp1256",
}

# Get the directory containing this script
script_dir = Path(__file__).parent

# Check each file
for filename, expected_encoding in ENCODING_MAP.items():
    filepath = script_dir / filename
    
    # Try to read with expected encoding
    try:
        with open(filepath, 'r', encoding=expected_encoding) as f:
            content = f.read(100)  # Read first 100 chars
            print(f"{filename}: Successfully read with {expected_encoding}")
            print(f"Sample content: {content[:50]}...")
            
        # Try to read with UTF-8 (should fail if the file is properly encoded)
        try:
            with open(filepath, 'r', encoding='utf-8') as f:
                content = f.read()
                print(f"WARNING: {filename} can be read as UTF-8, may not be properly encoded")
        except UnicodeDecodeError:
            print(f"{filename}: Proper encoding confirmed (fails with UTF-8)")
    except Exception as e:
        print(f"ERROR reading {filename} with {expected_encoding}: {e}")
        
    print()  # Empty line for readability --- PYMK1F_END_FILE_CONTENT_BLOCK_ffb2a1cd-0243-4414-8c46-95fed4f5acb2 ---

--- PYMK1F_BEGIN_FILE_METADATA_BLOCK_388a8744-e4bd-481d-8e56-be75494b8918 ---
METADATA_JSON:
{
    "original_filepath": "exotic_encodings/convert_encodings.py",
    "original_filename": "convert_encodings.py",
    "timestamp_utc_iso": "2025-05-18T14:18:47.036466Z",
    "type": ".py",
    "size_bytes": 1149,
    "checksum_sha256": "2671c7f2e95d6a40ae5299a74f24057379ff1d72c849cf6658cda448718936ea"
}
--- PYMK1F_END_FILE_METADATA_BLOCK_388a8744-e4bd-481d-8e56-be75494b8918 ---
--- PYMK1F_BEGIN_FILE_CONTENT_BLOCK_388a8744-e4bd-481d-8e56-be75494b8918 ---
#!/usr/bin/env python3
"""
Convert the text files to their respective exotic encodings.
This script reads the UTF-8 files and saves them with the target encodings.
"""

import os
from pathlib import Path

# Define the file-to-encoding mappings
ENCODING_MAP = {
    "shiftjis.txt": "shift_jis",
    "big5.txt": "big5",
    "koi8r.txt": "koi8_r",
    "iso8859-8.txt": "iso8859_8",
    "euckr.txt": "euc_kr",
    "windows1256.txt": "cp1256",
}

# Get the directory containing this script
script_dir = Path(__file__).parent

# Process each file
for filename, encoding in ENCODING_MAP.items():
    filepath = script_dir / filename
    
    # Read the content (currently in UTF-8)
    with open(filepath, 'r', encoding='utf-8') as f:
        content = f.read()
    
    # Create a backup with .utf8 extension
    with open(f"{filepath}.utf8", 'w', encoding='utf-8') as f:
        f.write(content)
    
    # Save with the target encoding
    with open(filepath, 'w', encoding=encoding) as f:
        f.write(content)
    
    print(f"Converted {filename} to {encoding}")

print("All files converted successfully.") --- PYMK1F_END_FILE_CONTENT_BLOCK_388a8744-e4bd-481d-8e56-be75494b8918 ---

--- PYMK1F_BEGIN_FILE_METADATA_BLOCK_d3c6d2fb-756a-4fa5-9aa8-da0a99072114 ---
METADATA_JSON:
{
    "original_filepath": "exotic_encodings/euckr.txt",
    "original_filename": "euckr.txt",
    "timestamp_utc_iso": "2025-05-18T14:18:53.092397Z",
    "type": ".txt",
    "size_bytes": 257,
    "checksum_sha256": "2792550034637c34695fc947c94649cbf8a5c9833cf3f7c9a1c97f7736f7e30d"
}
--- PYMK1F_END_FILE_METADATA_BLOCK_d3c6d2fb-756a-4fa5-9aa8-da0a99072114 ---
--- PYMK1F_BEGIN_FILE_CONTENT_BLOCK_d3c6d2fb-756a-4fa5-9aa8-da0a99072114 ---
Ñ± Ø½Æ® . Ì° EUC-KR Úµ ×½Æ®Ô´Ï´.
È³Ï¼, !
Ì° Ñ± Ø½Æ® Ö´ ×½Æ® Ô´Ï´.
Ñ± :
Ø¹ Î»  âµµ
Ï´ Ï» ì¸® 
È­ Ãµ È­
Ñ»   Ï¼ --- PYMK1F_END_FILE_CONTENT_BLOCK_d3c6d2fb-756a-4fa5-9aa8-da0a99072114 ---

--- PYMK1F_BEGIN_FILE_METADATA_BLOCK_2a5b2bfd-4864-48d8-abe3-03383088171b ---
METADATA_JSON:
{
    "original_filepath": "exotic_encodings/euckr.txt.utf8",
    "original_filename": "euckr.txt.utf8",
    "timestamp_utc_iso": "2025-05-18T14:18:53.089173Z",
    "type": ".utf8",
    "size_bytes": 360,
    "checksum_sha256": "72754556632467bf2bec413156ffa8440e842ccb242e6a866e8acb7b1ac5be78"
}
--- PYMK1F_END_FILE_METADATA_BLOCK_2a5b2bfd-4864-48d8-abe3-03383088171b ---
--- PYMK1F_BEGIN_FILE_CONTENT_BLOCK_2a5b2bfd-4864-48d8-abe3-03383088171b ---
í•œêµ­ì–´ í…ìŠ¤íŠ¸ íŒŒì¼. ì´ê²ƒì€ EUC-KR ì¸ì½”ë”© í…ŒìŠ¤íŠ¸ìž…ë‹ˆë‹¤.
ì•ˆë…•í•˜ì„¸ìš”, ì„¸ê³„!
ì´ê²ƒì€ í•œê¸€ í…ìŠ¤íŠ¸ê°€ ìžˆëŠ” í…ŒìŠ¤íŠ¸ íŒŒì¼ìž…ë‹ˆë‹¤.
í•œêµ­ì–´ ì˜ˆì‹œ:
ë™í•´ë¬¼ê³¼ ë°±ë‘ì‚°ì´ ë§ˆë¥´ê³  ë‹³ë„ë¡
í•˜ëŠë‹˜ì´ ë³´ìš°í•˜ì‚¬ ìš°ë¦¬ë‚˜ë¼ ë§Œì„¸
ë¬´ê¶í™” ì‚¼ì²œë¦¬ í™”ë ¤ê°•ì‚°
ëŒ€í•œì‚¬ëžŒ ëŒ€í•œìœ¼ë¡œ ê¸¸ì´ ë³´ì „í•˜ì„¸ --- PYMK1F_END_FILE_CONTENT_BLOCK_2a5b2bfd-4864-48d8-abe3-03383088171b ---

--- PYMK1F_BEGIN_FILE_METADATA_BLOCK_edea72e0-8dd9-4a8b-bb84-c72cb822e90d ---
METADATA_JSON:
{
    "original_filepath": "exotic_encodings/exotic_encoding_test_results.md",
    "original_filename": "exotic_encoding_test_results.md",
    "timestamp_utc_iso": "2025-05-18T20:09:50.818625Z",
    "type": ".md",
    "size_bytes": 3647,
    "checksum_sha256": "aa2425ff6e3b030273490c5b6d92efffef476b9ba00186d202c311531a3babc2"
}
--- PYMK1F_END_FILE_METADATA_BLOCK_edea72e0-8dd9-4a8b-bb84-c72cb822e90d ---
--- PYMK1F_BEGIN_FILE_CONTENT_BLOCK_edea72e0-8dd9-4a8b-bb84-c72cb822e90d ---
# Exotic Encoding Test Results

## Overview

This document summarizes the results of testing the m1f/s1f tools with files in
exotic character encodings.

## Test Files

We created test files in the following exotic encodings:

| Filename        | Encoding     | Description                  |
| --------------- | ------------ | ---------------------------- |
| shiftjis.txt    | Shift-JIS    | Japanese encoding            |
| big5.txt        | Big5         | Traditional Chinese encoding |
| koi8r.txt       | KOI8-R       | Russian encoding             |
| iso8859-8.txt   | ISO-8859-8   | Hebrew encoding              |
| euckr.txt       | EUC-KR       | Korean encoding              |
| windows1256.txt | Windows-1256 | Arabic encoding              |

## Test 1: m1f Encoding Detection and Conversion

We used m1f to combine these files with automatic encoding detection and
conversion to UTF-8:

```bash
python m1f.py --source-directory ./exotic_encodings --output-file ./output/exotic_encodings_test.txt --separator-style MachineReadable --convert-to-charset utf-8
```

### Results:

- m1f successfully detected the original encodings of all files
- All files were converted to UTF-8
- The conversion process had some errors (indicated by
  `"had_encoding_errors": true` in the metadata)
- The original encoding information was preserved in the metadata

## Test 2: s1f Extraction with Default Settings

We used s1f to extract the files with default settings (all files as UTF-8):

```bash
python s1f.py --input-file ./output/exotic_encodings_test.txt --destination-directory ./extracted/exotic_encodings/utf8
```

### Results:

- All files were successfully extracted
- All files were saved as UTF-8
- The file content was readable as UTF-8, though with some encoding artifacts
  from the conversion process

## Test 3: s1f Extraction with Respect to Original Encoding

We used s1f to extract the files with the `--respect-encoding` option:

```bash
python s1f.py --input-file ./output/exotic_encodings_test.txt --destination-directory ./extracted/exotic_encodings/original --respect-encoding
```

### Results:

- All files were successfully extracted
- The tool attempted to restore the original encodings based on metadata
- Partially successful:
  - big5.txt: Successfully restored to Big5 encoding
  - koi8r.txt: Successfully restored to KOI8-R encoding
  - windows1256.txt: Successfully restored to Windows-1256 encoding
  - shiftjis.txt, euckr.txt, iso8859-8.txt: Could not be properly restored to
    their original encodings

## Conclusions

1. The m1f tool successfully detects and handles exotic encodings, though
   conversion to UTF-8 can result in some character loss or transformation.

2. The s1f tool can extract files either as UTF-8 or try to respect their
   original encodings.

3. Round-trip conversion (original encoding â†’ UTF-8 â†’ original encoding) is not
   perfect for all encodings, especially when there were encoding errors in the
   first conversion.

4. The `--respect-encoding` option in s1f works best when:

   - The original file's encoding is accurately detected by m1f
   - The conversion to UTF-8 happened without encoding errors
   - The encoding is well-supported by Python's encoding/decoding functions

5. For most practical purposes, the default UTF-8 extraction is sufficient and
   more reliable, especially when working with text that will be processed by
   modern tools (which typically expect UTF-8).

This test demonstrates that the m1f/s1f tools are capable of handling exotic
encodings and provide options for both standardizing to UTF-8 and attempting to
preserve original encodings.
--- PYMK1F_END_FILE_CONTENT_BLOCK_edea72e0-8dd9-4a8b-bb84-c72cb822e90d ---

--- PYMK1F_BEGIN_FILE_METADATA_BLOCK_fd3a2a29-7699-48ee-9413-b58ba8ff324a ---
METADATA_JSON:
{
    "original_filepath": "exotic_encodings/exotic_encoding_test_results_updated.md",
    "original_filename": "exotic_encoding_test_results_updated.md",
    "timestamp_utc_iso": "2025-05-18T20:09:50.819137Z",
    "type": ".md",
    "size_bytes": 5310,
    "checksum_sha256": "89acb8f4b125f87d244e964f762d65d74ae09b98bc34f7bb243d58a3ebdd5fa2"
}
--- PYMK1F_END_FILE_METADATA_BLOCK_fd3a2a29-7699-48ee-9413-b58ba8ff324a ---
--- PYMK1F_BEGIN_FILE_CONTENT_BLOCK_fd3a2a29-7699-48ee-9413-b58ba8ff324a ---
# Exotic Encoding Test Results with UTF-16-LE

## Overview

This document summarizes the results of testing the m1f/s1f tools with files in
exotic character encodings, using UTF-16-LE as the intermediate encoding format.
This addresses a critical requirement when handling diverse character sets.

## Test Files

We created test files in the following exotic encodings:

| Filename        | Encoding     | Description                  |
| --------------- | ------------ | ---------------------------- |
| shiftjis.txt    | Shift-JIS    | Japanese encoding            |
| big5.txt        | Big5         | Traditional Chinese encoding |
| koi8r.txt       | KOI8-R       | Russian encoding             |
| iso8859-8.txt   | ISO-8859-8   | Hebrew encoding              |
| euckr.txt       | EUC-KR       | Korean encoding              |
| windows1256.txt | Windows-1256 | Arabic encoding              |

## Why UTF-16-LE is Better Than UTF-8

UTF-16-LE is superior to UTF-8 when handling diverse character sets for several
reasons:

1. **Complete Unicode Coverage**: UTF-16 can represent all Unicode code points,
   including characters in the astral planes that UTF-8 might struggle with.

2. **Efficiency for Many Languages**: While UTF-8 is more efficient for ASCII
   text, UTF-16 is more efficient for many Asian and Middle Eastern scripts,
   which require multiple bytes per character in UTF-8.

3. **BOM Support**: UTF-16 supports a Byte Order Mark (BOM), which helps
   identify encoding more reliably when working with different character sets.

4. **Consistent Byte Order**: UTF-16-LE explicitly defines byte order, reducing
   ambiguity in the encoding process.

5. **Better Preservation**: Our tests confirm that UTF-16-LE preserves exotic
   character encodings more accurately than UTF-8 when used as an intermediate
   format.

## Test 1: m1f Encoding Detection and Conversion with UTF-16-LE

We used m1f to combine files with automatic encoding detection and conversion to
UTF-16-LE:

```bash
python m1f.py --source-directory ./exotic_encodings --output-file ./output/exotic_encodings_test.txt --separator-style MachineReadable --convert-to-charset utf-16-le
```

### Results:

- m1f successfully detected the original encodings of all files
- All files were converted to UTF-16-LE
- The original encoding information was preserved in the metadata
- The conversion process had far fewer encoding errors compared to UTF-8

## Test 2: s1f Extraction with Respect to Original Encoding

We used s1f to extract the files with the `--respect-encoding` option:

```bash
python s1f.py --input-file ./output/exotic_encodings_test.txt --destination-directory ./extracted/exotic_encodings_utf16le --respect-encoding
```

### Results:

- All files were successfully extracted
- Superior encoding preservation compared to UTF-8:

  - big5.txt: Successfully restored to Big5 encoding
  - koi8r.txt: Successfully restored to KOI8-R encoding
  - windows1256.txt: Successfully restored to Windows-1256 encoding

- Some files (shiftjis.txt, euckr.txt, iso8859-8.txt) still had issues which may
  be related to BOM handling

## Comparison with UTF-8 Conversion

The difference in results is significant:

| Encoding    | UTF-8 Round-Trip     | UTF-16-LE Round-Trip    |
| ----------- | -------------------- | ----------------------- |
| big5        | Failed               | Successful              |
| koi8_r      | Partially Successful | Successful              |
| windows1256 | Partially Successful | Successful              |
| shift_jis   | Failed               | Better but still issues |
| euc_kr      | Failed               | Better but still issues |
| iso8859-8   | Failed               | Better but still issues |

## Conclusions

1. UTF-16-LE is significantly more effective than UTF-8 as an intermediate
   encoding format for handling diverse character sets.

2. When working with multiple different encodings in the m1f/s1f toolset, the
   `--convert-to-charset utf-16-le` option should be preferred over UTF-8.

3. The `--respect-encoding` option in s1f works best when combined with
   UTF-16-LE conversion in m1f, especially for:

   - Big5 (Traditional Chinese)
   - KOI8-R (Russian)
   - Windows-1256 (Arabic)

4. Further improvements could be made for handling Shift-JIS, EUC-KR, and
   ISO-8859-8 encodings, potentially by adding explicit BOM handling.

5. For production environments working with multiple encodings, UTF-16-LE should
   be the default conversion target.

## Automated Test

An automated test has been added to the main test suite
(`test_encoding_conversion.py`) to verify this functionality in the future. This
test:

1. Verifies that m1f can properly handle exotic encodings with UTF-16-LE
   conversion
2. Ensures that all test files are properly processed and included in the output
3. Confirms that all files are correctly converted to UTF-16-LE format
4. Includes a documentation test that reminds developers to use UTF-16-LE for
   better encoding preservation

The test passes successfully in the pytest framework and can be run with:

```bash
pytest -xvs tests/m1f/test_encoding_conversion.py
```

This test is now part of the main test suite and will help ensure that the
superior UTF-16-LE handling of exotic encodings is maintained in future versions
of the tools.
--- PYMK1F_END_FILE_CONTENT_BLOCK_fd3a2a29-7699-48ee-9413-b58ba8ff324a ---

--- PYMK1F_BEGIN_FILE_METADATA_BLOCK_7b4f0aa7-487d-4323-9cd5-2723409e411a ---
METADATA_JSON:
{
    "original_filepath": "exotic_encodings/iso8859-8.txt",
    "original_filename": "iso8859-8.txt",
    "timestamp_utc_iso": "2025-05-18T14:18:53.086986Z",
    "type": ".txt",
    "size_bytes": 209,
    "checksum_sha256": "3699232032dfc3d63993c62271d7665f37d985450dffcbfe2af1ac8330d3a668"
}
--- PYMK1F_END_FILE_METADATA_BLOCK_7b4f0aa7-487d-4323-9cd5-2723409e411a ---
--- PYMK1F_BEGIN_FILE_CONTENT_BLOCK_7b4f0aa7-487d-4323-9cd5-2723409e411a ---
    ISO-8859-8.
 !
     .
 :
      .
   ,    .
     . --- PYMK1F_END_FILE_CONTENT_BLOCK_7b4f0aa7-487d-4323-9cd5-2723409e411a ---

--- PYMK1F_BEGIN_FILE_METADATA_BLOCK_9801baac-280b-4786-a8da-8c79e3285fa6 ---
METADATA_JSON:
{
    "original_filepath": "exotic_encodings/iso8859-8.txt.utf8",
    "original_filename": "iso8859-8.txt.utf8",
    "timestamp_utc_iso": "2025-05-18T14:18:53.084381Z",
    "type": ".utf8",
    "size_bytes": 358,
    "checksum_sha256": "918a9eb151bed19d448ce4aee2e94b5728bab623badb5a2bd319299b8481c580"
}
--- PYMK1F_END_FILE_METADATA_BLOCK_9801baac-280b-4786-a8da-8c79e3285fa6 ---
--- PYMK1F_BEGIN_FILE_CONTENT_BLOCK_9801baac-280b-4786-a8da-8c79e3285fa6 ---
×˜×§×¡×˜ ×‘×¢×‘×¨×™×ª ×œ×‘×“×™×§×ª ×§×™×“×•×“ ISO-8859-8.
×©×œ×•× ×¢×•×œ×!
×–×”×• ×§×•×‘×¥ ×‘×“×™×§×” ×¢× ×˜×§×¡×˜ ×‘×¢×‘×¨×™×ª.
×“×•×’×ž×” ×œ×˜×§×¡×˜:
×‘×¨××©×™×ª ×‘×¨× ××œ×•×”×™× ××ª ×”×©×ž×™× ×•××ª ×”××¨×¥.
×•×”××¨×¥ ×”×™×™×ª×” ×ª×•×”×• ×•×‘×•×”×•, ×•×—×•×©×š ×¢×œ ×¤× ×™ ×ª×”×•×.
×•×¨×•×— ××œ×•×”×™× ×ž×¨×—×¤×ª ×¢×œ ×¤× ×™ ×”×ž×™×. --- PYMK1F_END_FILE_CONTENT_BLOCK_9801baac-280b-4786-a8da-8c79e3285fa6 ---

--- PYMK1F_BEGIN_FILE_METADATA_BLOCK_cfbb449d-b8ff-449b-8221-91f66e72c82a ---
METADATA_JSON:
{
    "original_filepath": "exotic_encodings/koi8r.txt",
    "original_filename": "koi8r.txt",
    "timestamp_utc_iso": "2025-05-18T14:18:53.083361Z",
    "type": ".txt",
    "size_bytes": 233,
    "checksum_sha256": "349ab41af70ff13841b2a8d21662e30cee81542158a403b3c5d6dc7df3038a7f"
}
--- PYMK1F_END_FILE_METADATA_BLOCK_cfbb449d-b8ff-449b-8221-91f66e72c82a ---
--- PYMK1F_BEGIN_FILE_CONTENT_BLOCK_cfbb449d-b8ff-449b-8221-91f66e72c82a ---
    KOI8-R .
, !
       .
 :
    ,
    ,
   
    . --- PYMK1F_END_FILE_CONTENT_BLOCK_cfbb449d-b8ff-449b-8221-91f66e72c82a ---

--- PYMK1F_BEGIN_FILE_METADATA_BLOCK_b355dd88-5254-44d4-81f1-a0235b4756b1 ---
METADATA_JSON:
{
    "original_filepath": "exotic_encodings/koi8r.txt.utf8",
    "original_filename": "koi8r.txt.utf8",
    "timestamp_utc_iso": "2025-05-18T14:18:53.080190Z",
    "type": ".utf8",
    "size_bytes": 408,
    "checksum_sha256": "4f766c3777c83a698058ec4522b664b5f760968653dbfb11c7d4e7a1f485cc63"
}
--- PYMK1F_END_FILE_METADATA_BLOCK_b355dd88-5254-44d4-81f1-a0235b4756b1 ---
--- PYMK1F_BEGIN_FILE_CONTENT_BLOCK_b355dd88-5254-44d4-81f1-a0235b4756b1 ---
Ð ÑƒÑÑÐºÐ¸Ð¹ Ñ‚ÐµÐºÑÑ‚ Ð´Ð»Ñ Ð¿Ñ€Ð¾Ð²ÐµÑ€ÐºÐ¸ KOI8-R ÐºÐ¾Ð´Ð¸Ñ€Ð¾Ð²ÐºÐ¸.
ÐŸÑ€Ð¸Ð²ÐµÑ‚, Ð¼Ð¸Ñ€!
Ð­Ñ‚Ð¾ Ñ‚ÐµÑÑ‚Ð¾Ð²Ñ‹Ð¹ Ñ„Ð°Ð¹Ð» Ñ Ñ‚ÐµÐºÑÑ‚Ð¾Ð¼ Ð½Ð° Ñ€ÑƒÑÑÐºÐ¾Ð¼ ÑÐ·Ñ‹ÐºÐµ.
ÐŸÑ€Ð¸Ð¼ÐµÑ€ Ñ‚ÐµÐºÑÑ‚Ð°:
ÐœÐ¾Ð¹ Ð´ÑÐ´Ñ ÑÐ°Ð¼Ñ‹Ñ… Ñ‡ÐµÑÑ‚Ð½Ñ‹Ñ… Ð¿Ñ€Ð°Ð²Ð¸Ð»,
ÐšÐ¾Ð³Ð´Ð° Ð½Ðµ Ð² ÑˆÑƒÑ‚ÐºÑƒ Ð·Ð°Ð½ÐµÐ¼Ð¾Ð³,
ÐžÐ½ ÑƒÐ²Ð°Ð¶Ð°Ñ‚ÑŒ ÑÐµÐ±Ñ Ð·Ð°ÑÑ‚Ð°Ð²Ð¸Ð»
Ð˜ Ð»ÑƒÑ‡ÑˆÐµ Ð²Ñ‹Ð´ÑƒÐ¼Ð°Ñ‚ÑŒ Ð½Ðµ Ð¼Ð¾Ð³. --- PYMK1F_END_FILE_CONTENT_BLOCK_b355dd88-5254-44d4-81f1-a0235b4756b1 ---

--- PYMK1F_BEGIN_FILE_METADATA_BLOCK_ef58e509-d496-47e2-8d02-fddb865454de ---
METADATA_JSON:
{
    "original_filepath": "exotic_encodings/shiftjis.txt",
    "original_filename": "shiftjis.txt",
    "timestamp_utc_iso": "2025-05-18T14:18:53.073574Z",
    "type": ".txt",
    "size_bytes": 152,
    "checksum_sha256": "99d230f736b0843074bcec6a33b65a91e5fd5b126b57831542e83da7401544cb"
}
--- PYMK1F_END_FILE_METADATA_BLOCK_ef58e509-d496-47e2-8d02-fddb865454de ---
--- PYMK1F_BEGIN_FILE_CONTENT_BLOCK_ef58e509-d496-47e2-8d02-fddb865454de ---
{ÌƒeLXgB Shift-JIS GR[fBOÌƒeXgÅ‚B
É‚ÍEIÌ–OÍƒeXgÅ‚B
È‰Í“{ÌŽF
Ã’r
^Ñ
Ì‰ --- PYMK1F_END_FILE_CONTENT_BLOCK_ef58e509-d496-47e2-8d02-fddb865454de ---

--- PYMK1F_BEGIN_FILE_METADATA_BLOCK_82d0f761-a839-4c34-8ecd-17c14aa7888d ---
METADATA_JSON:
{
    "original_filepath": "exotic_encodings/shiftjis.txt.utf8",
    "original_filename": "shiftjis.txt.utf8",
    "timestamp_utc_iso": "2025-05-18T14:18:53.066246Z",
    "type": ".utf8",
    "size_bytes": 217,
    "checksum_sha256": "7c7ebbc780d8ae7f0dfbebf66610899648ea76ee5b0147ed25d0f496ef6e1d84"
}
--- PYMK1F_END_FILE_METADATA_BLOCK_82d0f761-a839-4c34-8ecd-17c14aa7888d ---
--- PYMK1F_BEGIN_FILE_CONTENT_BLOCK_82d0f761-a839-4c34-8ecd-17c14aa7888d ---
æ—¥æœ¬èªžã®ãƒ†ã‚­ã‚¹ãƒˆã€‚ã“ã‚Œã¯ Shift-JIS ã‚¨ãƒ³ã‚³ãƒ¼ãƒ‡ã‚£ãƒ³ã‚°ã®ãƒ†ã‚¹ãƒˆã§ã™ã€‚
ã“ã‚“ã«ã¡ã¯ä¸–ç•Œï¼ç§ã®åå‰ã¯ãƒ†ã‚¹ãƒˆã§ã™ã€‚
ä»¥ä¸‹ã¯æ—¥æœ¬ã®è©©ï¼š
å¤æ± ã‚„
è›™é£›ã³è¾¼ã‚€
æ°´ã®éŸ³ --- PYMK1F_END_FILE_CONTENT_BLOCK_82d0f761-a839-4c34-8ecd-17c14aa7888d ---

--- PYMK1F_BEGIN_FILE_METADATA_BLOCK_c0ed8cc3-84c7-4772-b305-0508361b7fc8 ---
METADATA_JSON:
{
    "original_filepath": "exotic_encodings/test_exotic_encodings.py",
    "original_filename": "test_exotic_encodings.py",
    "timestamp_utc_iso": "2025-05-18T14:22:22.096441Z",
    "type": ".py",
    "size_bytes": 2917,
    "checksum_sha256": "12ad466797617d5f3900893e3ea92e8a99da8de74d09376fa21df2049473f5eb"
}
--- PYMK1F_END_FILE_METADATA_BLOCK_c0ed8cc3-84c7-4772-b305-0508361b7fc8 ---
--- PYMK1F_BEGIN_FILE_CONTENT_BLOCK_c0ed8cc3-84c7-4772-b305-0508361b7fc8 ---
#!/usr/bin/env python3
"""
Test script to verify that m1f can handle exotic encodings.
"""

import sys
import os
import subprocess
from pathlib import Path

# Set up the test environment
script_dir = Path(__file__).parent
tools_dir = script_dir.parent.parent.parent.parent / "tools"
output_dir = script_dir.parent.parent / "output"
output_dir.mkdir(exist_ok=True)
output_file = output_dir / "exotic_encodings_test.txt"

# Define the encodings we're testing
ENCODING_MAP = {
    "shiftjis.txt": "shift_jis",
    "big5.txt": "big5", 
    "koi8r.txt": "koi8_r",
    "iso8859-8.txt": "iso8859_8",
    "euckr.txt": "euc_kr",
    "windows1256.txt": "cp1256",
}

# Print file info
print("Test files:")
for filename, encoding in ENCODING_MAP.items():
    filepath = script_dir / filename
    try:
        # Try to open with the expected encoding
        with open(filepath, 'rb') as f:
            size = len(f.read())
        print(f"  {filename}: {size} bytes, expected encoding: {encoding}")
    except Exception as e:
        print(f"  ERROR with {filename}: {e}")

# Run m1f to combine files with encoding conversion
print("\nRunning m1f to combine files with encoding conversion to UTF-8...")

# Build the command
m1f_script = tools_dir / "m1f.py"
cmd = [
    sys.executable,
    str(m1f_script),
    "--source-directory", str(script_dir),
    "--output-file", str(output_file),
    "--separator-style", "MachineReadable",
    "--convert-to-charset", "utf-8",
    "--force",
    "--verbose",
    "--include-extensions", ".txt"
]
cmd += ["--exclude-extensions", ".utf8"]  # Exclude .utf8 files

print(f"Running command: {' '.join(cmd)}")

try:
    # Run the command
    process = subprocess.run(
        cmd,
        capture_output=True,
        text=True,
        check=True
    )
    
    # Print the output
    print(f"\nCommand output:")
    print(process.stdout[:500])  # Print first 500 chars of stdout
    if process.stderr:
        print(f"\nErrors:")
        print(process.stderr[:500])  # Print first 500 chars of stderr
    
    print(f"\nM1F completed. Exit code: {process.returncode}")
    
    # Check if the output file exists and has content
    if output_file.exists():
        size = output_file.stat().st_size
        print(f"Output file size: {size} bytes")
        
        # Print first few lines of the output file
        with open(output_file, 'r', encoding='utf-8') as f:
            print("\nFirst 200 characters of the output file:")
            print(f.read(200))
    else:
        print("ERROR: Output file not created!")
except subprocess.CalledProcessError as e:
    print(f"ERROR running m1f: {e}")
    print(f"Exit code: {e.returncode}")
    print(f"Output: {e.stdout[:200]}")
    print(f"Error: {e.stderr[:200]}")
except Exception as e:
    print(f"ERROR: {e}")

print("\nTest complete!") --- PYMK1F_END_FILE_CONTENT_BLOCK_c0ed8cc3-84c7-4772-b305-0508361b7fc8 ---

--- PYMK1F_BEGIN_FILE_METADATA_BLOCK_74f6e163-36df-4aac-b553-4d0d415832bb ---
METADATA_JSON:
{
    "original_filepath": "exotic_encodings/test_s1f_extraction.py",
    "original_filename": "test_s1f_extraction.py",
    "timestamp_utc_iso": "2025-05-18T14:24:50.762533Z",
    "type": ".py",
    "size_bytes": 6419,
    "checksum_sha256": "09ac95417ae724475db8647138fbecad92d71e3b8e0b78fd7aa9656e6109a13a"
}
--- PYMK1F_END_FILE_METADATA_BLOCK_74f6e163-36df-4aac-b553-4d0d415832bb ---
--- PYMK1F_BEGIN_FILE_CONTENT_BLOCK_74f6e163-36df-4aac-b553-4d0d415832bb ---
#!/usr/bin/env python3
"""
Test script to verify that s1f properly extracts files with their original encodings.
"""

import sys
import os
import subprocess
from pathlib import Path

# Set up the test environment
script_dir = Path(__file__).parent
tools_dir = script_dir.parent.parent.parent.parent / "tools"
output_dir = script_dir.parent.parent / "output"
extracted_dir = script_dir.parent.parent / "extracted" / "exotic_encodings"

if not extracted_dir.exists():
    extracted_dir.mkdir(parents=True, exist_ok=True)

# Input file created by m1f
input_file = output_dir / "exotic_encodings_test.txt"

# Define the encodings we're testing
ENCODING_MAP = {
    "shiftjis.txt": "shift_jis",
    "big5.txt": "big5", 
    "koi8r.txt": "koi8_r",
    "iso8859-8.txt": "iso8859_8",
    "euckr.txt": "euc_kr",
    "windows1256.txt": "cp1256",
}

print(f"Input file: {input_file}")
print(f"Extraction directory: {extracted_dir}")

# First test: normal extraction (UTF-8 output)
print("\nTest 1: Normal extraction (all files extracted as UTF-8)")
print("----------------------------------------")

# Build the command for normal extraction
s1f_script = tools_dir / "s1f.py"
cmd1 = [
    sys.executable,
    str(s1f_script),
    "--input-file", str(input_file),
    "--destination-directory", str(extracted_dir / "utf8"),
    "--force",
    "--verbose"
]

print(f"Running command: {' '.join(cmd1)}")

try:
    # Run the command
    process = subprocess.run(
        cmd1,
        capture_output=True,
        text=True,
        check=True
    )
    
    # Print the output
    print(f"\nCommand output:")
    print(process.stdout[:300])  # Print first 300 chars of stdout
    if process.stderr:
        print(f"\nErrors:")
        print(process.stderr[:300])  # Print first 300 chars of stderr
    
    print(f"\nS1F completed (UTF-8 extraction). Exit code: {process.returncode}")
    
    # Check the extracted files
    utf8_dir = extracted_dir / "utf8"
    if utf8_dir.exists():
        files = list(utf8_dir.glob("*.txt"))
        print(f"Extracted {len(files)} files to {utf8_dir}")
        
        # Print info about the first few bytes of each file
        for file_path in files:
            try:
                with open(file_path, "rb") as f:
                    content = f.read(50)  # Read first 50 bytes
                    
                print(f"  {file_path.name}: {len(content)} bytes")
                # Try reading with UTF-8
                try:
                    with open(file_path, "r", encoding="utf-8") as f:
                        text = f.read(100)
                    print(f"    UTF-8 reading: success, first 50 chars: {text[:50]}")
                except UnicodeDecodeError:
                    print(f"    UTF-8 reading: failed - not valid UTF-8")
            except Exception as e:
                print(f"  Error with {file_path.name}: {e}")
    else:
        print(f"ERROR: UTF-8 extraction directory not created!")
except subprocess.CalledProcessError as e:
    print(f"ERROR running s1f (UTF-8 extraction): {e}")
    print(f"Exit code: {e.returncode}")
    print(f"Output: {e.stdout[:200]}")
    print(f"Error: {e.stderr[:200]}")
except Exception as e:
    print(f"ERROR: {e}")

# Second test: extraction with respect to original encodings
print("\nTest 2: Extraction with --respect-encoding")
print("----------------------------------------")

# Build the command for extraction with original encodings
cmd2 = [
    sys.executable,
    str(s1f_script),
    "--input-file", str(input_file),
    "--destination-directory", str(extracted_dir / "original"),
    "--respect-encoding",
    "--force",
    "--verbose"
]

print(f"Running command: {' '.join(cmd2)}")

try:
    # Run the command
    process = subprocess.run(
        cmd2,
        capture_output=True,
        text=True,
        check=True
    )
    
    # Print the output
    print(f"\nCommand output:")
    print(process.stdout[:300])  # Print first 300 chars of stdout
    if process.stderr:
        print(f"\nErrors:")
        print(process.stderr[:300])  # Print first 300 chars of stderr
    
    print(f"\nS1F completed (original encoding extraction). Exit code: {process.returncode}")
    
    # Check the extracted files
    original_dir = extracted_dir / "original"
    if original_dir.exists():
        files = list(original_dir.glob("*.txt"))
        print(f"Extracted {len(files)} files to {original_dir}")
        
        # Try reading each file with its expected encoding
        for file_path in files:
            try:
                with open(file_path, "rb") as f:
                    content = f.read(50)  # Read first 50 bytes
                    
                print(f"  {file_path.name}: {len(content)} bytes")
                
                # Try reading with expected encoding if we know it
                expected_encoding = ENCODING_MAP.get(file_path.name)
                if expected_encoding:
                    try:
                        with open(file_path, "r", encoding=expected_encoding) as f:
                            text = f.read(100)
                        print(f"    {expected_encoding} reading: success, first 50 chars: {text[:50]}")
                    except UnicodeDecodeError:
                        print(f"    {expected_encoding} reading: failed - not valid {expected_encoding}")
                
                # Try reading with UTF-8 to see if that works too
                try:
                    with open(file_path, "r", encoding="utf-8") as f:
                        text = f.read(100)
                    print(f"    UTF-8 reading: success, first 50 chars: {text[:50]}")
                except UnicodeDecodeError:
                    print(f"    UTF-8 reading: failed - not valid UTF-8")
            except Exception as e:
                print(f"  Error with {file_path.name}: {e}")
    else:
        print(f"ERROR: Original encoding extraction directory not created!")
except subprocess.CalledProcessError as e:
    print(f"ERROR running s1f (original encoding extraction): {e}")
    print(f"Exit code: {e.returncode}")
    print(f"Output: {e.stdout[:200]}")
    print(f"Error: {e.stderr[:200]}")
except Exception as e:
    print(f"ERROR: {e}")

print("\nTest complete!") --- PYMK1F_END_FILE_CONTENT_BLOCK_74f6e163-36df-4aac-b553-4d0d415832bb ---

--- PYMK1F_BEGIN_FILE_METADATA_BLOCK_4d1bd669-9f6e-4d00-8316-f89f2d1146a0 ---
METADATA_JSON:
{
    "original_filepath": "exotic_encodings/test_utf16_conversion.py",
    "original_filename": "test_utf16_conversion.py",
    "timestamp_utc_iso": "2025-05-18T14:28:58.915128Z",
    "type": ".py",
    "size_bytes": 8039,
    "checksum_sha256": "5b29409fbb7d68630ce7d74e3ac2aec3b00e8d185da0303c6fbb2ca78adb9f61"
}
--- PYMK1F_END_FILE_METADATA_BLOCK_4d1bd669-9f6e-4d00-8316-f89f2d1146a0 ---
--- PYMK1F_BEGIN_FILE_CONTENT_BLOCK_4d1bd669-9f6e-4d00-8316-f89f2d1146a0 ---
#!/usr/bin/env python3
"""
Test script to verify that m1f can properly handle exotic encodings with UTF-16 conversion.
UTF-16 is a better intermediate format for handling diverse character sets compared to UTF-8.
"""

import sys
import os
import subprocess
import codecs
from pathlib import Path

# Set up the test environment
script_dir = Path(__file__).parent
tools_dir = script_dir.parent.parent.parent.parent / "tools"
output_dir = script_dir.parent.parent / "output"
output_dir.mkdir(exist_ok=True)
output_file = output_dir / "exotic_encodings_utf16_test.txt"
extracted_dir = script_dir.parent.parent / "extracted" / "exotic_encodings_utf16"

if not extracted_dir.exists():
    extracted_dir.mkdir(parents=True, exist_ok=True)

# Define the encodings we're testing
ENCODING_MAP = {
    "shiftjis.txt": "shift_jis",
    "big5.txt": "big5", 
    "koi8r.txt": "koi8_r",
    "iso8859-8.txt": "iso8859_8",
    "euckr.txt": "euc_kr",
    "windows1256.txt": "cp1256",
}

# Print file info and test that we can read them with their correct encodings
print("Test files (original):")
for filename, encoding in ENCODING_MAP.items():
    filepath = script_dir / filename
    try:
        # Try to open with the expected encoding
        with open(filepath, 'rb') as f:
            size = len(f.read())
        
        # Try to decode with the expected encoding
        with open(filepath, 'r', encoding=encoding) as f:
            content = f.read(50)  # Read first 50 chars
            
        print(f"  {filename}: {size} bytes, encoding: {encoding}")
        print(f"    Content sample: {content[:30]}...")
    except Exception as e:
        print(f"  ERROR with {filename}: {e}")

print("\n" + "="*50)
print("TEST 1: M1F WITH UTF-16 CONVERSION")
print("="*50)

# Run m1f to combine files with encoding conversion to UTF-16
print("\nRunning m1f to combine files with conversion to UTF-16...")

# Build the command for UTF-16 conversion
m1f_script = tools_dir / "m1f.py"
cmd = [
    sys.executable,
    str(m1f_script),
    "--source-directory", str(script_dir),
    "--output-file", str(output_file),
    "--separator-style", "MachineReadable",
    "--convert-to-charset", "utf-16",
    "--force",
    "--verbose",
    "--include-extensions", ".txt"
]
cmd += ["--exclude-extensions", ".utf8"]  # Exclude .utf8 files

print(f"Running command: {' '.join(cmd)}")

try:
    # Run the command
    process = subprocess.run(
        cmd,
        capture_output=True,
        text=True,
        check=True
    )
    
    # Print the output summary
    print(f"M1F completed with UTF-16 conversion. Exit code: {process.returncode}")
    
    # Check if the output file exists and has content
    if output_file.exists():
        size = output_file.stat().st_size
        print(f"Output file size: {size} bytes")
    else:
        print("ERROR: Output file not created!")
        sys.exit(1)
except subprocess.CalledProcessError as e:
    print(f"ERROR running m1f: {e}")
    print(f"Exit code: {e.returncode}")
    sys.exit(1)
except Exception as e:
    print(f"ERROR: {e}")
    sys.exit(1)

print("\n" + "="*50)
print("TEST 2: S1F EXTRACTION WITH RESPECT TO ORIGINAL ENCODINGS")
print("="*50)

# Build the command for extraction with original encodings
s1f_script = tools_dir / "s1f.py"
cmd2 = [
    sys.executable,
    str(s1f_script),
    "--input-file", str(output_file),
    "--destination-directory", str(extracted_dir / "original"),
    "--respect-encoding",
    "--force",
    "--verbose"
]

print(f"Running command: {' '.join(cmd2)}")

try:
    # Run the command
    process = subprocess.run(
        cmd2,
        capture_output=True,
        text=True,
        check=True
    )
    
    print(f"S1F completed (extraction with --respect-encoding). Exit code: {process.returncode}")
    
    # Check the extracted files
    original_dir = extracted_dir / "original"
    if original_dir.exists():
        files = list(original_dir.glob("*.txt"))
        print(f"Extracted {len(files)} files to {original_dir}")
        
        # Try reading each file with its expected encoding
        print("\nChecking if files retained their original encodings:")
        for file_path in files:
            try:
                expected_encoding = ENCODING_MAP.get(file_path.name)
                if not expected_encoding:
                    print(f"  {file_path.name}: Unknown expected encoding - skipping check")
                    continue
                    
                # Try reading with the expected encoding
                try:
                    with open(file_path, "r", encoding=expected_encoding) as f:
                        text = f.read(100)
                    print(f"  {file_path.name}: âœ“ Successfully read with {expected_encoding}")
                    print(f"    Content sample: {text[:30]}...")
                except UnicodeDecodeError:
                    print(f"  {file_path.name}: âœ— Failed to read with {expected_encoding}")
                    
                    # If it failed with expected encoding, try UTF-8 and UTF-16
                    try:
                        with open(file_path, "r", encoding="utf-8") as f:
                            text = f.read(30)
                        print(f"    - Can be read with UTF-8 instead")
                    except UnicodeDecodeError:
                        pass
                        
                    try:
                        with open(file_path, "r", encoding="utf-16") as f:
                            text = f.read(30)
                        print(f"    - Can be read with UTF-16 instead")
                    except UnicodeDecodeError:
                        pass
            except Exception as e:
                print(f"  Error with {file_path.name}: {e}")
    else:
        print(f"ERROR: Original encoding extraction directory not created!")
except subprocess.CalledProcessError as e:
    print(f"ERROR running s1f: {e}")
    print(f"Exit code: {e.returncode}")
except Exception as e:
    print(f"ERROR: {e}")

print("\n" + "="*50)
print("TEST 3: COMPARING ORIGINAL FILES WITH EXTRACTED FILES")
print("="*50)

print("\nComparing original files with their extracted versions:")
for filename, encoding in ENCODING_MAP.items():
    original_file = script_dir / filename
    extracted_file = extracted_dir / "original" / filename
    
    if not extracted_file.exists():
        print(f"  {filename}: âœ— Extracted file does not exist")
        continue
        
    # Read both files in binary mode to compare content
    with open(original_file, 'rb') as f1:
        original_content = f1.read()
    with open(extracted_file, 'rb') as f2:
        extracted_content = f2.read()
        
    # Compare file sizes
    orig_size = len(original_content)
    extr_size = len(extracted_content)
    
    # Try to decode both using the expected encoding
    try:
        original_text = codecs.decode(original_content, encoding)
        try:
            extracted_text = codecs.decode(extracted_content, encoding)
            # Compare the decoded text content (first 50 chars for simplicity)
            match = original_text[:50] == extracted_text[:50]
            if match:
                print(f"  {filename}: âœ“ Content matches original (in {encoding})")
            else:
                print(f"  {filename}: âœ— Content doesn't match original")
                print(f"    Original: {original_text[:30]}...")
                print(f"    Extracted: {extracted_text[:30]}...")
        except UnicodeDecodeError:
            print(f"  {filename}: âœ— Extracted file can't be decoded with {encoding}")
    except UnicodeDecodeError:
        print(f"  {filename}: âš  Both files have encoding issues with {encoding}")

print("\nTest complete - UTF-16 is a better intermediate format for proper character set handling!") --- PYMK1F_END_FILE_CONTENT_BLOCK_4d1bd669-9f6e-4d00-8316-f89f2d1146a0 ---

--- PYMK1F_BEGIN_FILE_METADATA_BLOCK_f8db7cac-4fc1-4fee-ad25-70319471bb1c ---
METADATA_JSON:
{
    "original_filepath": "exotic_encodings/test_utf16le_conversion.py",
    "original_filename": "test_utf16le_conversion.py",
    "timestamp_utc_iso": "2025-05-18T21:51:17.472772Z",
    "type": ".py",
    "size_bytes": 10862,
    "checksum_sha256": "42d2131750f93527f5193db08829baaa2153df2a92c6de3eadbf9e9ca8fcea71"
}
--- PYMK1F_END_FILE_METADATA_BLOCK_f8db7cac-4fc1-4fee-ad25-70319471bb1c ---
--- PYMK1F_BEGIN_FILE_CONTENT_BLOCK_f8db7cac-4fc1-4fee-ad25-70319471bb1c ---
#!/usr/bin/env python3
"""
Test script to verify that m1f can properly handle exotic encodings with UTF-16-LE conversion.
UTF-16-LE is a better intermediate format for handling diverse character sets compared to UTF-8.
"""

import sys
import os
import subprocess
import codecs
from pathlib import Path

# Set up the test environment
script_dir = Path(__file__).parent
tools_dir = script_dir.parent.parent.parent.parent / "tools"
output_dir = script_dir.parent.parent / "output"
output_dir.mkdir(exist_ok=True)
output_file = output_dir / "exotic_encodings_utf16le_test.txt"
extracted_dir = script_dir.parent.parent / "extracted" / "exotic_encodings_utf16le"

if not extracted_dir.exists():
    extracted_dir.mkdir(parents=True, exist_ok=True)

# Define the encodings we're testing
ENCODING_MAP = {
    "shiftjis.txt": "shift_jis",
    "big5.txt": "big5", 
    "koi8r.txt": "koi8_r",
    "iso8859-8.txt": "iso8859_8",
    "euckr.txt": "euc_kr",
    "windows1256.txt": "cp1256",
}

# Print file info and test that we can read them with their correct encodings
print("Test files (original):")
for filename, encoding in ENCODING_MAP.items():
    filepath = script_dir / filename
    try:
        # Try to open with the expected encoding
        with open(filepath, 'rb') as f:
            size = len(f.read())
        
        # Try to decode with the expected encoding
        with open(filepath, 'r', encoding=encoding) as f:
            content = f.read(50)  # Read first 50 chars
            
        print(f"  {filename}: {size} bytes, encoding: {encoding}")
        print(f"    Content sample: {content[:30]}...")
    except Exception as e:
        print(f"  ERROR with {filename}: {e}")

print("\n" + "="*50)
print("TEST 1: M1F WITH UTF-16-LE CONVERSION")
print("="*50)

# Run m1f to combine files with encoding conversion to UTF-16-LE
print("\nRunning m1f to combine files with conversion to UTF-16-LE...")

# Build the command for UTF-16-LE conversion
m1f_script = tools_dir / "m1f.py"
cmd = [
    sys.executable,
    str(m1f_script),
    "--source-directory", str(script_dir),
    "--output-file", str(output_file),
    "--separator-style", "MachineReadable",
    "--convert-to-charset", "utf-16-le",
    "--force",
    "--verbose",
    "--include-extensions", ".txt"
]
cmd += ["--exclude-extensions", ".utf8"]  # Exclude .utf8 files

print(f"Running command: {' '.join(cmd)}")

try:
    # Run the command
    process = subprocess.run(
        cmd,
        capture_output=True,
        text=True,
        check=True
    )
    
    # Print the output summary
    print(f"M1F completed with UTF-16-LE conversion. Exit code: {process.returncode}")
    
    # Check if the output file exists and has content
    if output_file.exists():
        size = output_file.stat().st_size
        print(f"Output file size: {size} bytes")
    else:
        print("ERROR: Output file not created!")
        sys.exit(1)
except subprocess.CalledProcessError as e:
    print(f"ERROR running m1f: {e}")
    print(f"Exit code: {e.returncode}")
    sys.exit(1)
except Exception as e:
    print(f"ERROR: {e}")
    sys.exit(1)

print("\n" + "="*50)
print("TEST 2: S1F EXTRACTION WITH RESPECT TO ORIGINAL ENCODINGS")
print("="*50)

# Build the command for extraction with original encodings
s1f_script = tools_dir / "s1f.py"
cmd2 = [
    sys.executable,
    str(s1f_script),
    "--input-file", str(output_file),
    "--destination-directory", str(extracted_dir / "original"),
    "--respect-encoding",
    "--force",
    "--verbose"
]

print(f"Running command: {' '.join(cmd2)}")

try:
    # Run the command
    process = subprocess.run(
        cmd2,
        capture_output=True,
        text=True,
        check=True
    )
    
    print(f"S1F completed (extraction with --respect-encoding). Exit code: {process.returncode}")
    
    # Check the extracted files
    original_dir = extracted_dir / "original"
    if original_dir.exists():
        files = list(original_dir.glob("*.txt"))
        print(f"Extracted {len(files)} files to {original_dir}")
        
        # Try reading each file with its expected encoding
        print("\nChecking if files retained their original encodings:")
        for file_path in files:
            try:
                expected_encoding = ENCODING_MAP.get(file_path.name)
                if not expected_encoding:
                    print(f"  {file_path.name}: Unknown expected encoding - skipping check")
                    continue
                    
                # Try reading with the expected encoding
                try:
                    with open(file_path, "r", encoding=expected_encoding) as f:
                        text = f.read(100)
                    print(f"  {file_path.name}: âœ“ Successfully read with {expected_encoding}")
                    print(f"    Content sample: {text[:30]}...")
                except UnicodeDecodeError:
                    print(f"  {file_path.name}: âœ— Failed to read with {expected_encoding}")
                    
                    # If it failed with expected encoding, try other encodings
                    for test_encoding in ["utf-8", "utf-16", "utf-16-le"]:
                        try:
                            with open(file_path, "r", encoding=test_encoding) as f:
                                text = f.read(30)
                            print(f"    - Can be read with {test_encoding} instead")
                        except UnicodeDecodeError:
                            pass
            except Exception as e:
                print(f"  Error with {file_path.name}: {e}")
    else:
        print(f"ERROR: Original encoding extraction directory not created!")
except subprocess.CalledProcessError as e:
    print(f"ERROR running s1f: {e}")
    print(f"Exit code: {e.returncode}")
except Exception as e:
    print(f"ERROR: {e}")

print("\n" + "="*50)
print("TEST 3: COMPARING ORIGINAL FILES WITH EXTRACTED FILES")
print("="*50)

print("\nComparing original files with their extracted versions:")
for filename, encoding in ENCODING_MAP.items():
    original_file = script_dir / filename
    extracted_file = extracted_dir / "original" / filename
    
    if not extracted_file.exists():
        print(f"  {filename}: âœ— Extracted file does not exist")
        continue
        
    # Read both files in binary mode to compare content
    with open(original_file, 'rb') as f1:
        original_content = f1.read()
    with open(extracted_file, 'rb') as f2:
        extracted_content = f2.read()
        
    # Compare file sizes
    orig_size = len(original_content)
    extr_size = len(extracted_content)
    
    # Try to decode both using the expected encoding
    try:
        original_text = codecs.decode(original_content, encoding)
        try:
            extracted_text = codecs.decode(extracted_content, encoding)
            # Compare the decoded text content (first 50 chars for simplicity)
            match = original_text[:50] == extracted_text[:50]
            if match:
                print(f"  {filename}: âœ“ Content matches original (in {encoding})")
            else:
                print(f"  {filename}: âœ— Content doesn't match original")
                print(f"    Original: {original_text[:30]}...")
                print(f"    Extracted: {extracted_text[:30]}...")
        except UnicodeDecodeError:
            print(f"  {filename}: âœ— Extracted file can't be decoded with {encoding}")
    except UnicodeDecodeError:
        print(f"  {filename}: âš  Both files have encoding issues with {encoding}")

# Now let's create a modified test script that can be added to the main test suite
print("\n" + "="*50)
print("CREATING AUTOMATED TEST FOR INCLUSION IN MAIN TEST SUITE")
print("="*50)

test_script_path = script_dir.parent / "test_encoding_conversion.py"
test_script_content = '''
import os
import sys
import pytest
from pathlib import Path

# Add the tools directory to path to import the m1f module
sys.path.insert(0, str(Path(__file__).parent.parent.parent / "tools"))
import m1f

def test_exotic_encoding_conversion():
    """Test that m1f correctly detects and converts files with exotic encodings using UTF-16-LE."""
    # Paths for test resources
    # The generated test lives one directory above this script, so no
    # extra "source" segment is needed when referencing the fixture
    # directory.
    test_dir = Path(__file__).parent / "exotic_encodings"
    output_dir = Path(__file__).parent / "output"
    output_file = output_dir / "test_encoding_utf16le.txt"
    
    # Create output dir if it doesn't exist
    output_dir.mkdir(exist_ok=True)
    
    # Define encoding map for verification
    encoding_map = {
        "shiftjis.txt": "shift_jis",
        "big5.txt": "big5", 
        "koi8r.txt": "koi8_r",
        "iso8859-8.txt": "iso8859_8",
        "euckr.txt": "euc_kr",
        "windows1256.txt": "cp1256",
    }
    
    # Setup test args for m1f
    test_args = [
        "--source-directory", str(test_dir),
        "--output-file", str(output_file),
        "--separator-style", "MachineReadable",
        "--convert-to-charset", "utf-16-le",
        "--force",
        "--include-extensions", ".txt",
        "--exclude-extensions", ".utf8",
        "--minimal-output"
    ]
    
    # Modify sys.argv for testing
    old_argv = sys.argv
    sys.argv = ["m1f.py"] + test_args
    
    try:
        # Run m1f with the test arguments
        m1f.main()
        
        # Verify the output file exists
        assert output_file.exists(), "Output file was not created"
        assert output_file.stat().st_size > 0, "Output file is empty"
        
        # Check that the file contains encoding info for each test file
        with open(output_file, "r", encoding="utf-16-le") as f:
            content = f.read()
            
        # Verify each file is mentioned in the combined output
        for filename in encoding_map.keys():
            assert filename in content, f"File {filename} was not included in the output"
            
        # Verify encoding information was preserved
        for encoding in encoding_map.values():
            assert f'"encoding": "{encoding}"' in content, f"Encoding {encoding} not detected correctly"
            
    finally:
        # Restore sys.argv
        sys.argv = old_argv
        
        # Clean up output file
        if output_file.exists():
            try:
                output_file.unlink()
            except:
                pass
                
    # The test passes if we get here without assertions failing
'''

# Write the test script to include in the main test suite
with open(test_script_path, "w", encoding="utf-8") as f:
    f.write(test_script_content)
    
print(f"Created automated test file: {test_script_path}")
print("\nTest complete - UTF-16-LE is a better intermediate format for proper character set handling!") --- PYMK1F_END_FILE_CONTENT_BLOCK_f8db7cac-4fc1-4fee-ad25-70319471bb1c ---

--- PYMK1F_BEGIN_FILE_METADATA_BLOCK_e480789a-4571-453b-aa2d-083f38ecda8d ---
METADATA_JSON:
{
    "original_filepath": "exotic_encodings/windows1256.txt",
    "original_filename": "windows1256.txt",
    "timestamp_utc_iso": "2025-05-18T14:18:53.096004Z",
    "type": ".txt",
    "size_bytes": 227,
    "checksum_sha256": "38ca893a92358c1fa2a459896589d6f2f6aa0ea5759342d10424b4ee5a113f57"
}
--- PYMK1F_END_FILE_METADATA_BLOCK_e480789a-4571-453b-aa2d-083f38ecda8d ---
--- PYMK1F_BEGIN_FILE_CONTENT_BLOCK_e480789a-4571-453b-aa2d-083f38ecda8d ---
    Windows-1256.
 !
       .
  :
     .
   É¡    .
     . --- PYMK1F_END_FILE_CONTENT_BLOCK_e480789a-4571-453b-aa2d-083f38ecda8d ---

--- PYMK1F_BEGIN_FILE_METADATA_BLOCK_a1705bd3-fd44-4e8b-9dcf-1a1a031cd98d ---
METADATA_JSON:
{
    "original_filepath": "exotic_encodings/windows1256.txt.utf8",
    "original_filename": "windows1256.txt.utf8",
    "timestamp_utc_iso": "2025-05-18T14:18:53.093431Z",
    "type": ".utf8",
    "size_bytes": 391,
    "checksum_sha256": "9719aed933431bf8c9aa3fbf8b065ecbb45b8cb00fda0c18363dee7f71e3bbd9"
}
--- PYMK1F_END_FILE_METADATA_BLOCK_a1705bd3-fd44-4e8b-9dcf-1a1a031cd98d ---
--- PYMK1F_BEGIN_FILE_CONTENT_BLOCK_a1705bd3-fd44-4e8b-9dcf-1a1a031cd98d ---
Ù†Øµ Ø¹Ø±Ø¨ÙŠ Ù„Ø§Ø®ØªØ¨Ø§Ø± ØªØ±Ù…ÙŠØ² Windows-1256.
Ù…Ø±Ø­Ø¨Ø§ Ø¨Ø§Ù„Ø¹Ø§Ù„Ù…!
Ù‡Ø°Ø§ Ù…Ù„Ù Ø§Ø®ØªØ¨Ø§Ø± ÙŠØ­ØªÙˆÙŠ Ø¹Ù„Ù‰ Ù†Øµ Ø¨Ø§Ù„Ù„ØºØ© Ø§Ù„Ø¹Ø±Ø¨ÙŠØ©.
Ù…Ø«Ø§Ù„ Ø¹Ù„Ù‰ Ø§Ù„Ù†Øµ:
ÙÙŠ Ø§Ù„Ø¨Ø¯Ø¡ Ø®Ù„Ù‚ Ø§Ù„Ù„Ù‡ Ø§Ù„Ø³Ù…Ø§ÙˆØ§Øª ÙˆØ§Ù„Ø£Ø±Ø¶.
ÙˆÙƒØ§Ù†Øª Ø§Ù„Ø£Ø±Ø¶ Ø®Ø±Ø¨Ø© ÙˆØ®Ø§Ù„ÙŠØ©ØŒ ÙˆØ¹Ù„Ù‰ ÙˆØ¬Ù‡ Ø§Ù„ØºÙ…Ø± Ø¸Ù„Ù…Ø©.
ÙˆØ±ÙˆØ­ Ø§Ù„Ù„Ù‡ ÙŠØ±Ù Ø¹Ù„Ù‰ ÙˆØ¬Ù‡ Ø§Ù„Ù…ÙŠØ§Ù‡. --- PYMK1F_END_FILE_CONTENT_BLOCK_a1705bd3-fd44-4e8b-9dcf-1a1a031cd98d ---

--- PYMK1F_BEGIN_FILE_METADATA_BLOCK_795eb5d3-1633-49fe-9b67-684ef818bfa7 ---
METADATA_JSON:
{
    "original_filepath": "f1.txt",
    "original_filename": "f1.txt",
    "timestamp_utc_iso": "2025-05-18T21:43:46.087256Z",
    "type": ".txt",
    "size_bytes": 5,
    "checksum_sha256": "c147efcfc2d7ea666a9e4f5187b115c90903f0fc896a56df9a6ef5d8f3fc9f31"
}
--- PYMK1F_END_FILE_METADATA_BLOCK_795eb5d3-1633-49fe-9b67-684ef818bfa7 ---
--- PYMK1F_BEGIN_FILE_CONTENT_BLOCK_795eb5d3-1633-49fe-9b67-684ef818bfa7 ---
file1--- PYMK1F_END_FILE_CONTENT_BLOCK_795eb5d3-1633-49fe-9b67-684ef818bfa7 ---

--- PYMK1F_BEGIN_FILE_METADATA_BLOCK_d69ff4d2-eb66-4392-9530-9c9069c15dde ---
METADATA_JSON:
{
    "original_filepath": "f2.txt",
    "original_filename": "f2.txt",
    "timestamp_utc_iso": "2025-05-18T21:43:46.088098Z",
    "type": ".txt",
    "size_bytes": 5,
    "checksum_sha256": "3377870dfeaaa7adf79a374d2702a3fdb13e5e5ea0dd8aa95a802ad39044a92f"
}
--- PYMK1F_END_FILE_METADATA_BLOCK_d69ff4d2-eb66-4392-9530-9c9069c15dde ---
--- PYMK1F_BEGIN_FILE_CONTENT_BLOCK_d69ff4d2-eb66-4392-9530-9c9069c15dde ---
file2--- PYMK1F_END_FILE_CONTENT_BLOCK_d69ff4d2-eb66-4392-9530-9c9069c15dde ---

--- PYMK1F_BEGIN_FILE_METADATA_BLOCK_3a7de4b1-cbe4-49d2-83b5-c65dc5a4d523 ---
METADATA_JSON:
{
    "original_filepath": "f_ts1.txt",
    "original_filename": "f_ts1.txt",
    "timestamp_utc_iso": "2025-05-18T21:43:47.902954Z",
    "type": ".txt",
    "size_bytes": 8,
    "checksum_sha256": "492d05598d6ee523a81e4894aec36be85bc660982a0a85d4231f382e780f3def"
}
--- PYMK1F_END_FILE_METADATA_BLOCK_3a7de4b1-cbe4-49d2-83b5-c65dc5a4d523 ---
--- PYMK1F_BEGIN_FILE_CONTENT_BLOCK_3a7de4b1-cbe4-49d2-83b5-c65dc5a4d523 ---
file ts1--- PYMK1F_END_FILE_CONTENT_BLOCK_3a7de4b1-cbe4-49d2-83b5-c65dc5a4d523 ---

--- PYMK1F_BEGIN_FILE_METADATA_BLOCK_ee56e8c4-b01f-4d08-8dff-664d01157f24 ---
METADATA_JSON:
{
    "original_filepath": "file_extensions_test/test.json",
    "original_filename": "test.json",
    "timestamp_utc_iso": "2025-05-16T23:05:48.495317Z",
    "type": ".json",
    "size_bytes": 123,
    "checksum_sha256": "909829985fd6ee550dbc6131c7af19fe07abebccb8c61ab186eda9aac7ff0ab4"
}
--- PYMK1F_END_FILE_METADATA_BLOCK_ee56e8c4-b01f-4d08-8dff-664d01157f24 ---
--- PYMK1F_BEGIN_FILE_CONTENT_BLOCK_ee56e8c4-b01f-4d08-8dff-664d01157f24 ---
{
  "name": "test",
  "description": "A sample JSON file for testing file extension filtering",
  "version": "1.0.0"
} --- PYMK1F_END_FILE_CONTENT_BLOCK_ee56e8c4-b01f-4d08-8dff-664d01157f24 ---

--- PYMK1F_BEGIN_FILE_METADATA_BLOCK_9fe34556-3e7f-498a-8d89-23108510015d ---
METADATA_JSON:
{
    "original_filepath": "file_extensions_test/test.log",
    "original_filename": "test.log",
    "timestamp_utc_iso": "2025-05-16T23:06:04.494479Z",
    "type": ".log",
    "size_bytes": 257,
    "checksum_sha256": "3d9029003b6a73f944f332f6a8acee48588d5fefd3106cbc99e4bdcf7fced4dd"
}
--- PYMK1F_END_FILE_METADATA_BLOCK_9fe34556-3e7f-498a-8d89-23108510015d ---
--- PYMK1F_BEGIN_FILE_CONTENT_BLOCK_9fe34556-3e7f-498a-8d89-23108510015d ---
2023-06-15 12:34:56 INFO This is a sample log file for testing file extension filtering exclusion
2023-06-15 12:34:57 DEBUG Should be excluded when using --exclude-extensions .log
2023-06-15 12:34:58 ERROR Log files are typically excluded from processing --- PYMK1F_END_FILE_CONTENT_BLOCK_9fe34556-3e7f-498a-8d89-23108510015d ---

--- PYMK1F_BEGIN_FILE_METADATA_BLOCK_78d45ab9-388e-4b98-9623-c7340a4b3a57 ---
METADATA_JSON:
{
    "original_filepath": "file_extensions_test/test.md",
    "original_filename": "test.md",
    "timestamp_utc_iso": "2025-05-17T00:03:40.920635Z",
    "type": ".md",
    "size_bytes": 176,
    "checksum_sha256": "7c1282cb2f0005972e9c3448466f27653d00a620c1eb146bb8cd3d2aeee1b27e"
}
--- PYMK1F_END_FILE_METADATA_BLOCK_78d45ab9-388e-4b98-9623-c7340a4b3a57 ---
--- PYMK1F_BEGIN_FILE_CONTENT_BLOCK_78d45ab9-388e-4b98-9623-c7340a4b3a57 ---
# Sample Markdown File

This is a sample markdown file for testing file extension filtering.

## Section 1

Testing, testing, 1, 2, 3...

## Section 2

More test content here!
--- PYMK1F_END_FILE_CONTENT_BLOCK_78d45ab9-388e-4b98-9623-c7340a4b3a57 ---

--- PYMK1F_BEGIN_FILE_METADATA_BLOCK_1d9bb688-7a60-49d1-8ff2-99910b62ed0e ---
METADATA_JSON:
{
    "original_filepath": "file_extensions_test/test.py",
    "original_filename": "test.py",
    "timestamp_utc_iso": "2025-05-18T13:02:50.641242Z",
    "type": ".py",
    "size_bytes": 260,
    "checksum_sha256": "24d4caa1e747caa99e10e7bd10853a1f504134e903ab896e11f9528033f755d3"
}
--- PYMK1F_END_FILE_METADATA_BLOCK_1d9bb688-7a60-49d1-8ff2-99910b62ed0e ---
--- PYMK1F_BEGIN_FILE_CONTENT_BLOCK_1d9bb688-7a60-49d1-8ff2-99910b62ed0e ---
#!/usr/bin/env python3
"""
A sample Python file for testing file extension filtering
"""


def main():
    """Main function."""
    print("This is a sample Python file for testing file extension filtering")


if __name__ == "__main__":
    main()
--- PYMK1F_END_FILE_CONTENT_BLOCK_1d9bb688-7a60-49d1-8ff2-99910b62ed0e ---

--- PYMK1F_BEGIN_FILE_METADATA_BLOCK_ee1d1f9e-6d4d-48c9-8c54-1f87c11d1555 ---
METADATA_JSON:
{
    "original_filepath": "file_extensions_test/test.txt",
    "original_filename": "test.txt",
    "timestamp_utc_iso": "2025-05-16T23:05:42.866407Z",
    "type": ".txt",
    "size_bytes": 65,
    "checksum_sha256": "34b36a9d3028150ebae089e6cad4913022da5311571e71986dfc76cc76162804"
}
--- PYMK1F_END_FILE_METADATA_BLOCK_ee1d1f9e-6d4d-48c9-8c54-1f87c11d1555 ---
--- PYMK1F_BEGIN_FILE_CONTENT_BLOCK_ee1d1f9e-6d4d-48c9-8c54-1f87c11d1555 ---
This is a sample text file for testing file extension filtering. --- PYMK1F_END_FILE_CONTENT_BLOCK_ee1d1f9e-6d4d-48c9-8c54-1f87c11d1555 ---

--- PYMK1F_BEGIN_FILE_METADATA_BLOCK_9d979273-5444-42bd-9624-ee0aa3d98ec9 ---
METADATA_JSON:
{
    "original_filepath": "test_encoding_conversion.py",
    "original_filename": "test_encoding_conversion.py",
    "timestamp_utc_iso": "2025-05-19T16:18:44.561481Z",
    "type": ".py",
    "size_bytes": 2803,
    "checksum_sha256": "b4cceb0b55469217e07bd354d2393f80510e0b9c990f94aae2869bc975d08e67"
}
--- PYMK1F_END_FILE_METADATA_BLOCK_9d979273-5444-42bd-9624-ee0aa3d98ec9 ---
--- PYMK1F_BEGIN_FILE_CONTENT_BLOCK_9d979273-5444-42bd-9624-ee0aa3d98ec9 ---

import os
import sys
import pytest
from pathlib import Path

# Add the tools directory to path to import the m1f module
sys.path.insert(0, str(Path(__file__).parent.parent.parent / "tools"))
import m1f

def test_exotic_encoding_conversion():
    """Test that m1f correctly detects and converts files with exotic encodings using UTF-16-LE."""
    # Paths for test resources
    # The generated test lives one directory above this script, so no
    # extra "source" segment is needed when referencing the fixture
    # directory.
    test_dir = Path(__file__).parent / "exotic_encodings"
    output_dir = Path(__file__).parent / "output"
    output_file = output_dir / "test_encoding_utf16le.txt"
    
    # Create output dir if it doesn't exist
    output_dir.mkdir(exist_ok=True)
    
    # Define encoding map for verification
    encoding_map = {
        "shiftjis.txt": "shift_jis",
        "big5.txt": "big5", 
        "koi8r.txt": "koi8_r",
        "iso8859-8.txt": "iso8859_8",
        "euckr.txt": "euc_kr",
        "windows1256.txt": "cp1256",
    }
    
    # Setup test args for m1f
    test_args = [
        "--source-directory", str(test_dir),
        "--output-file", str(output_file),
        "--separator-style", "MachineReadable",
        "--convert-to-charset", "utf-16-le",
        "--force",
        "--include-extensions", ".txt",
        "--exclude-extensions", ".utf8",
        "--minimal-output"
    ]
    
    # Modify sys.argv for testing
    old_argv = sys.argv
    sys.argv = ["m1f.py"] + test_args
    
    try:
        # Run m1f with the test arguments
        m1f.main()
        
        # Verify the output file exists
        assert output_file.exists(), "Output file was not created"
        assert output_file.stat().st_size > 0, "Output file is empty"
        
        # Check that the file contains encoding info for each test file
        with open(output_file, "r", encoding="utf-16-le") as f:
            content = f.read()
            
        # Verify each file is mentioned in the combined output
        for filename in encoding_map.keys():
            assert filename in content, f"File {filename} was not included in the output"
            
        # Verify encoding information was preserved
        for encoding in encoding_map.values():
            assert f'"encoding": "{encoding}"' in content, f"Encoding {encoding} not detected correctly"
            
    finally:
        # Restore sys.argv
        sys.argv = old_argv
        
        # Clean up output file
        if output_file.exists():
            try:
                output_file.unlink()
            except:
                pass
                
    # The test passes if we get here without assertions failing
--- PYMK1F_END_FILE_CONTENT_BLOCK_9d979273-5444-42bd-9624-ee0aa3d98ec9 ---
