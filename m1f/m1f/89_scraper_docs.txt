======= 40_webscraper.md ======
# webscraper (Website Downloader)

A modern web scraping tool for downloading websites with multiple backend
options, async I/O, and intelligent crawling capabilities.

## Overview

The webscraper tool provides a robust solution for downloading websites for
offline viewing and analysis. Built with Python 3.10+ and modern async
architecture, it features pluggable scraper backends for different use cases.

**Primary Use Case**: Download online documentation to make it available to LLMs
(like Claude) for analysis and reference. The downloaded HTML files can be
converted to Markdown with html2md, then bundled into a single file with m1f for
optimal LLM context usage.

## Key Features

- **Multiple Scraper Backends**: Choose from BeautifulSoup (default), HTTrack,
  Scrapy, Playwright, or Selectolax
- **Async I/O**: High-performance concurrent downloading
- **Intelligent Crawling**: Automatically respects robots.txt, follows
  redirects, handles encoding
- **Duplicate Prevention**: Three-layer deduplication system:
  - Canonical URL checking (enabled by default)
  - Content-based deduplication (enabled by default)
  - GET parameter normalization (optional with `--ignore-get-params`)
- **Metadata Preservation**: Saves HTTP headers and metadata alongside HTML
  files
- **Domain Restriction**: Automatically restricts crawling to the starting
  domain
- **Subdirectory Restriction**: When URL contains a path, only scrapes within
  that subdirectory
- **Rate Limiting**: Configurable delays between requests
- **Progress Tracking**: Real-time download progress with file listing
- **Resume Support**: Interrupt and resume scraping sessions with SQLite
  tracking

## Quick Start

```bash
# Basic website download
m1f-scrape https://example.com -o ./downloaded_html

# Download with specific depth and page limits
m1f-scrape https://example.com -o ./html \
  --max-pages 50 \
  --max-depth 3

# Use different scraper backend
m1f-scrape https://example.com -o ./html --scraper httrack

# List downloaded files after completion (limited to 30 files for large sites)
m1f-scrape https://example.com -o ./html --list-files

# Save all scraped URLs to a file for later analysis
m1f-scrape https://example.com -o ./html --save-urls ./scraped_urls.txt

# Save list of all downloaded files to a file
m1f-scrape https://example.com -o ./html --save-files ./file_list.txt

# Resume interrupted scraping (with verbose mode to see progress)
m1f-scrape https://example.com -o ./html -v

# Force rescrape to update content (ignores cache)
m1f-scrape https://example.com -o ./html --force-rescrape

# Clear URLs and start fresh (keeps content checksums)
m1f-scrape https://example.com -o ./html --clear-urls
```

## Command Line Interface

```bash
m1f-scrape <url> -o <output> [options]
```

### Required Arguments

| Option         | Description                |
| -------------- | -------------------------- |
| `url`          | URL to start scraping from |
| `-o, --output` | Output directory           |

### Optional Arguments

| Option                  | Description                                                   | Default       |
| ----------------------- | ------------------------------------------------------------- | ------------- |
| `--scraper`             | Scraper backend to use (choices: httrack, beautifulsoup, bs4, | beautifulsoup |
|                         | selectolax, httpx, scrapy, playwright)                        |               |
| `--scraper-config`      | Path to scraper-specific config file (YAML/JSON)              | None          |
| `--max-depth`           | Maximum crawl depth                                           | 5             |
| `--max-pages`           | Maximum pages to crawl (-1 for unlimited)                     | 10000         |
| `--allowed-path`        | Restrict crawling to this path (legacy, single path)          | None          |
| `--allowed-paths`       | Restrict crawling to multiple paths (new, space-separated)    | None          |
| `--request-delay`       | Delay between requests in seconds (for Cloudflare protection) | 15.0          |
| `--concurrent-requests` | Number of concurrent requests (for Cloudflare protection)     | 2             |
| `--user-agent`          | Custom user agent string                                      | Mozilla/5.0   |
| `--ignore-get-params`   | Ignore GET parameters in URLs (e.g., ?tab=linux)              | False         |
| `--ignore-canonical`    | Ignore canonical URL tags (checking is enabled by default)    | False         |
| `--ignore-duplicates`   | Ignore duplicate content detection (enabled by default)       | False         |
| `--clear-urls`          | Clear all URLs from database and start fresh                  | False         |
| `--force-rescrape`      | Force rescraping of all URLs (ignores cached content)         | False         |
| `--list-files`          | List all downloaded files after completion (limited display)  | False         |
| `--save-urls`           | Save all scraped URLs to a file (one per line)                | None          |
| `--save-files`          | Save list of all downloaded files to a file (one per line)    | None          |
| `-v, --verbose`         | Enable verbose output (file listing limited to 30 files)      | False         |
| `-q, --quiet`           | Suppress all output except errors                             | False         |
| `--show-db-stats`       | Show scraping statistics from the database                    | False         |
| `--show-errors`         | Show URLs that had errors during scraping                     | False         |
| `--show-scraped-urls`   | List all scraped URLs from the database                       | False         |
| `--show-sessions`       | Show all scraping sessions with basic info                    | False         |
| `--show-sessions-detailed` | Show detailed information for all sessions                 | False         |
| `--clear-session`       | Clear a specific session by ID                                | None          |
| `--clear-last-session`  | Clear the most recent scraping session                        | False         |
| `--cleanup-sessions`    | Clean up orphaned sessions from crashes                       | False         |
| `--version`             | Show version information and exit                             | -             |

## Scraper Backends

### BeautifulSoup (default)

- **Best for**: General purpose scraping, simple websites
- **Features**: Fast HTML parsing, good encoding detection
- **Limitations**: No JavaScript support

```bash
m1f-scrape https://example.com -o ./html --scraper beautifulsoup
```

### HTTrack

- **Best for**: Complete website mirroring, preserving structure
- **Features**: External links handling, advanced mirroring options
- **Limitations**: Requires HTTrack to be installed separately

```bash
m1f-scrape https://example.com -o ./html --scraper httrack
```

### Scrapy

- **Best for**: Large-scale crawling, complex scraping rules
- **Features**: Advanced crawling settings, middleware support
- **Limitations**: More complex configuration

```bash
m1f-scrape https://example.com -o ./html --scraper scrapy
```

### Playwright

- **Best for**: JavaScript-heavy sites, SPAs
- **Features**: Full browser automation, JavaScript execution
- **Limitations**: Slower, requires more resources

```bash
m1f-scrape https://example.com -o ./html --scraper playwright
```

### Selectolax

- **Best for**: Speed-critical applications
- **Features**: Fastest HTML parsing, minimal overhead
- **Limitations**: Basic feature set

```bash
m1f-scrape https://example.com -o ./html --scraper selectolax
```

## Usage Examples

### Basic Website Download

```bash
# Download a simple website
m1f-scrape https://docs.example.com -o ./docs_html

# Download with verbose output
m1f-scrape https://docs.example.com -o ./docs_html -v
```

### Canonical URL Checking

By default, the scraper checks for canonical URLs to avoid downloading duplicate
content:

```bash
# Pages with different canonical URLs are automatically skipped
m1f-scrape https://example.com -o ./html

# Ignore canonical tags if you want all page versions
m1f-scrape https://example.com -o ./html --ignore-canonical
```

When enabled (default), the scraper:

- Checks the `<link rel="canonical">` tag on each page
- Skips pages where the canonical URL differs from the current URL
- Prevents downloading duplicate content (e.g., print versions, mobile versions)
- Logs skipped pages with their canonical URLs for transparency

This is especially useful for sites that have multiple URLs pointing to the same
content.

### Content Deduplication

By default, the scraper detects and skips pages with duplicate content based on
text-only checksums:

```bash
# Content deduplication is enabled by default
m1f-scrape https://example.com -o ./html

# Disable content deduplication if needed
m1f-scrape https://example.com -o ./html --ignore-duplicates
```

This feature:

- Enabled by default to avoid downloading duplicate content
- Extracts plain text from HTML (removes all tags, scripts, styles)
- Calculates SHA-256 checksum of the normalized text
- Skips pages with identical text content
- Useful for sites with multiple URLs serving the same content
- Works together with canonical URL checking for thorough deduplication

The scraper now has three levels of duplicate prevention, applied in this order:

1. **GET parameter normalization** (default: disabled) - Use
   `--ignore-get-params` to enable
2. **Canonical URL checking** (default: enabled) - Respects
   `<link rel="canonical">`
3. **Content deduplication** (default: enabled) - Compares text content

**Important**: All deduplication data is stored in the SQLite database
(`scrape_tracker.db`), which means:

- Content checksums persist across resume operations
- Canonical URL information is saved for each page
- The deduplication works correctly even when resuming interrupted scrapes
- Memory-efficient: checksums are queried from database, not loaded into memory
- Scales to large websites without excessive memory usage

### Subdirectory Restriction

When you specify a URL with a path, the scraper automatically restricts crawling
to that subdirectory:

```bash
# Only scrape pages under /docs subdirectory
m1f-scrape https://example.com/docs -o ./docs_only

# Only scrape API documentation pages
m1f-scrape https://api.example.com/v2/reference -o ./api_docs

# This will NOT scrape /products, /blog, etc. - only /tutorials/*
m1f-scrape https://learn.example.com/tutorials -o ./tutorials_only
```

### Advanced Path Control with --allowed-path and --allowed-paths

Sometimes you need to start from a specific page but allow crawling in different directories. 

#### Single Path (Legacy)
Use `--allowed-path` to override the automatic path restriction with a single path:

```bash
# Start from product page but allow crawling all products
m1f-scrape https://docs.example.com/products/widget.html -o ./products \
  --allowed-path /products/

# Start from a deep nested page but allow broader documentation crawling
m1f-scrape https://docs.example.com/v2/api/users/create.html -o ./api_docs \
  --allowed-path /v2/api/

# Start from main docs page but restrict to specific section
m1f-scrape https://docs.example.com/index.html -o ./guides \
  --allowed-path /guides/
```

#### Multiple Paths (New)
Use `--allowed-paths` to allow crawling in multiple directories:

```bash
# Scrape both documentation and API reference sections
m1f-scrape https://example.com -o ./docs \
  --allowed-paths /docs/ /api/ /reference/

# Start from main page but only scrape specific sections
m1f-scrape https://docs.example.com/index.html -o ./selected_docs \
  --allowed-paths /tutorials/ /how-to/ /faq/

# Combine multiple documentation areas
m1f-scrape https://framework.com/docs -o ./framework_docs \
  --allowed-paths /docs/core/ /docs/plugins/ /docs/api/
```

**Note**: You cannot use both `--allowed-path` and `--allowed-paths` in the same command. The `--allowed-path` option is maintained for backward compatibility.

The start URL is always scraped regardless of path restrictions, making it perfect for
documentation sites where the index page links to content in different directories.

### Controlled Crawling

```bash
# Limit crawl depth for shallow scraping
m1f-scrape https://blog.example.com -o ./blog \
  --max-depth 2 \
  --max-pages 20

# Unlimited scraping (use with caution!)
m1f-scrape https://docs.example.com -o ./docs \
  --max-pages -1 \
  --request-delay 2.0

# Slow crawling to be respectful
m1f-scrape https://example.com -o ./html \
  --request-delay 2.0 \
  --concurrent-requests 2

# Start from specific page but allow broader crawling area (single path)
m1f-scrape https://docs.example.com/api/index.html -o ./api_docs \
  --allowed-path /api/ \
  --max-pages 100

# Scrape multiple sections with the new --allowed-paths option
m1f-scrape https://docs.example.com -o ./docs \
  --allowed-paths /api/ /guides/ /reference/ \
  --max-pages 200
```

### Custom Configuration

```bash
# Use custom user agent
m1f-scrape https://example.com -o ./html \
  --user-agent "MyBot/1.0 (Compatible)"

# Use scraper-specific configuration
m1f-scrape https://example.com -o ./html \
  --scraper scrapy \
  --scraper-config ./scrapy-settings.yaml
```

## Session Management

m1f-scrape tracks each scraping run as a session with full statistics and state management.

### Session Tracking

Every scraping run creates a session with:
- Unique session ID
- Start/end timestamps  
- Configuration parameters used
- Success/failure statistics
- Session status (running, completed, interrupted, failed)

### View Sessions

```bash
# Show all sessions with basic info
m1f-scrape --show-sessions -o ./html

# Show detailed session information
m1f-scrape --show-sessions-detailed -o ./html

# Example output:
ID  | Status    | Started             | Pages | Success | Failed | URL
----------------------------------------------------------------------------------------------------
3   | completed | 2025-08-03 14:23:12 | 142   | 140     | 2      | https://docs.example.com
2   | interrupted| 2025-08-03 13:45:00 | 45    | 45      | 0      | https://api.example.com/v2
1   | completed | 2025-08-03 12:00:00 | 250   | 248     | 2      | https://example.com
```

### Clean Up Sessions

```bash
# Clear the most recent session (database only, asks about files)
m1f-scrape --clear-last-session -o ./html

# Clear session and automatically delete files (no prompt)
m1f-scrape --clear-last-session --delete-files -o ./html

# Clear a specific session by ID (asks about files)
m1f-scrape --clear-session 2 -o ./html

# Clear session 2 and delete files without confirmation
m1f-scrape --clear-session 2 --delete-files -o ./html

# Clean up orphaned sessions (from crashes)
m1f-scrape --cleanup-sessions -o ./html
```

#### File Deletion Behavior

When clearing sessions, the scraper will:
1. **Always delete** database entries (URLs, checksums, session records)
2. **Optionally delete** downloaded HTML files and metadata files
3. **Ask for confirmation** by default when files would be deleted
4. **Skip confirmation** if `--delete-files` flag is provided

This allows you to:
- Keep downloaded files while cleaning the database
- Fully clean up both database and files
- Automate cleanup in scripts with `--delete-files`

### Automatic Cleanup

The scraper automatically:
- Detects sessions left in 'running' state from crashes
- Marks sessions as 'interrupted' if no URLs have been scraped for >1 hour
- Preserves statistics for interrupted sessions
- Does NOT interrupt long-running active sessions (they can run for many hours)

### Session Recovery

If a process is killed (kill -9, system crash, etc.), the session will be left in 'running' state. On the next run:

1. **Automatic cleanup**: Sessions older than 1 hour are automatically marked as interrupted
2. **Manual cleanup**: Use `--cleanup-sessions` to manually review and clean up
3. **Resume capability**: The scraping can still resume from where it left off

```bash
# After a crash, cleanup and resume
m1f-scrape --cleanup-sessions -o ./html
m1f-scrape https://example.com -o ./html  # Resumes from last position
```

## Scraping Summary and Statistics

After each scraping session, m1f-scrape displays a comprehensive summary with:

- **Session ID**: Unique identifier for this scraping run
- **Success metrics**: Number of successfully scraped pages
- **Error count**: Number of failed page downloads
- **Success rate**: Percentage of successful downloads
- **Time statistics**: Total duration and average time per page
- **File counts**: Number of HTML files saved

Example output:
```
============================================================
Scraping Summary (Session #3)
============================================================
✓ Successfully scraped 142 pages
⚠ Failed to scrape 3 pages
Total URLs processed: 145
Success rate: 97.9%
Total duration: 435.2 seconds
Average time per page: 3.00 seconds
Output directory: ./html/example.com
HTML files saved in this session: 142

Session ID: #3
To clear this session: m1f-scrape --clear-session 3 -o ./html
```

## Output Structure

Downloaded files are organized to mirror the website structure:

```
output_directory/
├── scrape_tracker.db         # SQLite database for resume functionality
├── example.com/
│   ├── index.html
│   ├── index.meta.json
│   ├── about/
│   │   ├── index.html
│   │   └── index.meta.json
│   ├── blog/
│   │   ├── post1/
│   │   │   ├── index.html
│   │   │   └── index.meta.json
│   │   └── post2/
│   │       ├── index.html
│   │       └── index.meta.json
│   └── contact/
│       ├── index.html
│       └── index.meta.json
```

### Metadata Files

Each HTML file has an accompanying `.meta.json` file containing:

```json
{
  "url": "https://example.com/about/",
  "title": "About Us - Example",
  "encoding": "utf-8",
  "status_code": 200,
  "headers": {
    "Content-Type": "text/html; charset=utf-8",
    "Last-Modified": "2024-01-15T10:30:00Z"
  },
  "metadata": {
    "description": "Learn more about Example company",
    "og:title": "About Us",
    "canonical": "https://example.com/about/"
  }
}
```

## Integration with m1f Workflow

webscraper is designed as the first step in a workflow to provide documentation
to LLMs:

```bash
# Step 1: Download documentation website
m1f-scrape https://docs.example.com -o ./html_files

# Step 2: Analyze HTML structure
m1f-html2md analyze ./html_files/*.html --suggest-selectors

# Step 3: Convert to Markdown
m1f-html2md convert ./html_files -o ./markdown \
  --content-selector "main.content" \
  --ignore-selectors "nav" ".sidebar"

# Step 4: Bundle for LLM consumption
m1f -s ./markdown -o ./docs_bundle.txt \
  --remove-scraped-metadata

# Now docs_bundle.txt contains all documentation in a single file
# that can be provided to Claude or other LLMs for analysis
```

### Complete Documentation Download Example

```bash
# Download React documentation for LLM analysis
m1f-scrape https://react.dev/learn -o ./react_docs \
  --max-pages 100 \
  --max-depth 3

# Convert to clean Markdown
m1f-html2md convert ./react_docs -o ./react_md \
  --content-selector "article" \
  --ignore-selectors "nav" "footer" ".sidebar"

# Create single file for LLM
m1f -s ./react_md -o ./react_documentation.txt

# Now you can provide react_documentation.txt to Claude:
# "Here is the React documentation: <contents of react_documentation.txt>"
```

### Real-World Examples

The m1f project includes two complete documentation scraper examples:

#### Claude Code Documentation
Located in `examples/claude_code_doc/`:
- Scrapes ~31 pages from docs.anthropic.com/claude-code
- Includes optimized HTML extraction config (saves 5-8 minutes)
- Creates clean Markdown bundle for LLM consumption
- See the [README](../../examples/claude_code_doc/README.md) for usage

#### Tailscale Documentation  
Located in `examples/tailscale_doc/`:
- Scrapes ~422 pages from tailscale.com/kb
- Creates 11 thematic bundles (2.4MB total) organized by topic
- Includes parallel processing and skip-download options
- See the [README](../../examples/tailscale_doc/README.md) for details

Both examples demonstrate best practices for:
- Respectful scraping with delays
- Optimized HTML-to-Markdown conversion
- Bundle organization for LLM usage
- Configuration reuse to save time

## Resume Functionality

The scraper supports interrupting and resuming downloads, making it ideal for
large websites or unreliable connections.

### How It Works

- **SQLite Database**: Creates `scrape_tracker.db` in the output directory to
  track:
  - URL of each scraped page
  - HTTP status code and target filename
  - Timestamp and error messages (if any)
- **Progress Display**: Shows real-time progress in verbose mode:
  ```
  Processing: https://example.com/page1 (page 1)
  Processing: https://example.com/page2 (page 2)
  ```
- **Graceful Interruption**: Press Ctrl+C to interrupt cleanly:
  ```
  Press Ctrl+C to interrupt and resume later
  ^C
  ⚠️  Scraping interrupted by user
  Run the same command again to resume where you left off
  ```

### Resume Example

```bash
# Start scraping with verbose mode
m1f-scrape https://docs.example.com -o ./docs --max-pages 100 -v

# Interrupt with Ctrl+C when needed
# Resume by running the exact same command:
m1f-scrape https://docs.example.com -o ./docs --max-pages 100 -v

# You'll see:
# Resuming crawl - found 25 previously scraped URLs
# Populating queue from previously scraped pages...
# Found 187 URLs to visit after analyzing scraped pages
# Processing: https://docs.example.com/new-page (page 26)
```

### Overriding Resume Behavior

You can override the resume functionality using the new flags:

```bash
# Force rescraping all pages even if already in database
m1f-scrape https://docs.example.com -o ./docs --force-rescrape

# Clear all URLs and start from the beginning
m1f-scrape https://docs.example.com -o ./docs --clear-urls

# Both together for a complete fresh start
m1f-scrape https://docs.example.com -o ./docs --clear-urls --force-rescrape
```

### Database Inspection

```bash
# Show scraping statistics
m1f-scrape -o docs/ --show-db-stats

# View all scraped URLs with status codes
m1f-scrape -o docs/ --show-scraped-urls

# Check for errors
m1f-scrape -o docs/ --show-errors

# Combine multiple queries
m1f-scrape -o docs/ --show-db-stats --show-errors
```

## Force Rescraping and Clearing URLs

The scraper provides two options for managing cached content and URLs in the database:

### --clear-urls: Start Fresh

The `--clear-urls` option removes all URL tracking from the database while preserving content checksums:

```bash
# Clear all URLs and start a fresh scrape
m1f-scrape https://docs.example.com -o ./docs --clear-urls

# This will:
# - Remove all URL records from the database
# - Keep content checksums for deduplication
# - Start scraping from the beginning
```

**Use cases:**
- Website structure has changed significantly
- Want to re-crawl all pages without resetting content deduplication
- Need to update navigation paths while avoiding duplicate content

### --force-rescrape: Ignore Cached Content

The `--force-rescrape` option forces the scraper to re-download all pages, ignoring the cache:

```bash
# Force rescraping all pages (ignores cache)
m1f-scrape https://docs.example.com -o ./docs --force-rescrape

# Combine with clear-urls for complete reset
m1f-scrape https://docs.example.com -o ./docs --clear-urls --force-rescrape
```

**Use cases:**
- Content has been updated on the website
- Need fresh copies of all pages
- Want to override resume functionality temporarily

### Interaction with Content Checksums

Content checksums are preserved even when using `--clear-urls`, which means:

```bash
# First scrape - downloads all content
m1f-scrape https://example.com -o ./html

# Clear URLs but keep checksums
m1f-scrape https://example.com -o ./html --clear-urls

# Second scrape - URLs are re-crawled but duplicate content is still detected
# Pages with identical text content will be skipped based on checksums
```

To completely reset everything including checksums:

```bash
# Option 1: Delete the database file
rm ./html/scrape_tracker.db
m1f-scrape https://example.com -o ./html

# Option 2: Use both flags together
m1f-scrape https://example.com -o ./html --clear-urls --force-rescrape
```

### Examples of Force Rescraping Scenarios

#### Scenario 1: Documentation Update
```bash
# Initial scrape of documentation
m1f-scrape https://docs.framework.com -o ./docs_v1

# Framework releases new version with updated docs
# Force rescrape to get all updated content
m1f-scrape https://docs.framework.com -o ./docs_v2 --force-rescrape
```

#### Scenario 2: Partial Scrape Recovery
```bash
# Scrape was interrupted or had errors
m1f-scrape https://large-site.com -o ./site --max-pages 1000

# Clear URLs and try again with different settings
m1f-scrape https://large-site.com -o ./site \
  --clear-urls \
  --max-pages 500 \
  --request-delay 30
```

#### Scenario 3: Testing Different Scraper Backends
```bash
# Try with BeautifulSoup first
m1f-scrape https://complex-site.com -o ./test --max-pages 10

# Site has JavaScript - clear and try with Playwright
m1f-scrape https://complex-site.com -o ./test \
  --clear-urls \
  --scraper playwright \
  --max-pages 10
```

## Best Practices

1. **Respect robots.txt**: The tool automatically respects robots.txt files
2. **Use appropriate delays**: Set `--request-delay` to avoid overwhelming
   servers (default: 15 seconds)
3. **Limit concurrent requests**: Use `--concurrent-requests` responsibly
   (default: 2 connections)
4. **Test with small crawls**: Start with `--max-pages 10` to test your settings
5. **Check output**: Use `--list-files` to verify what was downloaded (limited to 30 files for large sites)
6. **Save URLs for analysis**: Use `--save-urls` to keep a record of all scraped URLs
7. **Track downloaded files**: Use `--save-files` to maintain a list of all downloaded files
8. **Use verbose mode**: Add `-v` flag to see progress and resume information
9. **Keep commands consistent**: Use the exact same command to resume a session
10. **Monitor statistics**: Check the summary statistics to verify scraping efficiency

## Dealing with Cloudflare Protection

Many websites use Cloudflare or similar services to protect against bots. The
scraper now includes conservative defaults to help avoid detection:

### Default Conservative Settings

- **Request delay**: 15 seconds between requests
- **Concurrent requests**: 2 simultaneous connections
- **HTTrack backend**: Limited to 0.5 connections/second max
- **Bandwidth limiting**: 100KB/s for HTTrack backend
- **Robots.txt**: Always respected (cannot be disabled)

### For Heavy Cloudflare Protection

For heavily protected sites, manually set very conservative values:

```bash
m1f-scrape https://protected-site.com -o ./output \
  --request-delay 30 \
  --concurrent-requests 1 \
  --max-pages 50 \
  --scraper httrack
```

### Cloudflare Avoidance Tips

1. **Start conservative**: Begin with 30-60 second delays
2. **Use realistic user agents**: The default is a current Chrome browser
3. **Limit scope**: Download only what you need with `--max-pages`
4. **Single connection**: Use `--concurrent-requests 1` for sensitive sites
5. **Respect robots.txt**: Always enabled by default
6. **Add randomness**: Consider adding random delays in custom scripts

### When Cloudflare Still Blocks

If conservative settings don't work:

1. **Try Playwright backend**: Uses real browser automation

   ```bash
   m1f-scrape https://site.com -o ./output --scraper playwright
   ```

2. **Manual download**: Some sites require manual browsing
3. **API access**: Check if the site offers an API
4. **Contact site owner**: Request permission or access

## Troubleshooting

### No files downloaded

- Check if the website blocks automated access
- Try a different scraper backend
- Verify the URL is accessible

### Incomplete downloads

- Increase `--max-depth` if pages are deeply nested
- Increase `--max-pages` if hitting the limit
- Check for JavaScript-rendered content (use Playwright)

### Encoding issues

- The tool automatically detects encoding
- Check `.meta.json` files for encoding information
- Use html2md with proper encoding settings for conversion

## See Also

- [html2md Documentation](../03_html2md/30_html2md.md) - For converting
  downloaded HTML to Markdown
- [m1f Documentation](../01_m1f/00_m1f.md) - For bundling converted content for
  LLMs

======= 41_html2md_scraper_backends.md ======
# Web Scraper Backends

The HTML2MD tool supports multiple web scraping backends, each optimized for
different use cases. Choose the right backend based on your specific needs for
optimal results.

## Overview

The HTML2MD scraper backend system provides flexibility to choose the most
appropriate tool for your web scraping needs:

- **Static websites**: BeautifulSoup4 (default) - Fast and lightweight
- **Complete mirroring**: HTTrack - Professional website copying
- **JavaScript-heavy sites**: Playwright (coming soon)
- **Large-scale scraping**: Scrapy (coming soon)
- **Performance-critical**: httpx + selectolax (coming soon)

## Available Backends

### BeautifulSoup4 (Default)

BeautifulSoup4 is the default backend, ideal for scraping static HTML websites.

**Pros:**

- Easy to use and lightweight
- Fast for simple websites
- Good encoding detection
- Excellent HTML parsing capabilities

**Cons:**

- No JavaScript support
- Basic crawling capabilities
- Single-threaded by default

**Usage:**

```bash
# Default backend (no need to specify)
m1f-scrape https://example.com -o output/

# Explicitly specify BeautifulSoup
m1f-scrape https://example.com -o output/ --scraper beautifulsoup

# With custom options
m1f-scrape https://example.com -o output/ \
  --scraper beautifulsoup \
  --max-depth 3 \
  --max-pages 100 \
  --request-delay 1.0
```

### HTTrack

HTTrack is a professional website copier that creates complete offline mirrors.

**Pros:**

- Complete website mirroring
- Preserves directory structure
- Handles complex websites well
- Resume interrupted downloads
- Automatic robots.txt compliance

**Cons:**

- Requires system installation
- Less flexible for custom parsing
- Larger resource footprint

**Installation:**

```bash
# Ubuntu/Debian
sudo apt-get install httrack

# macOS
brew install httrack

# Windows
# Download from https://www.httrack.com/
```

**Usage:**

```bash
m1f-scrape https://example.com -o output/ --scraper httrack

# With HTTrack-specific options
m1f-scrape https://example.com -o output/ \
  --scraper httrack \
  --max-depth 5 \
  --concurrent-requests 8
```

## Configuration Options

### Command Line Options

Common options for all scrapers:

```bash
--scraper BACKEND           # Choose scraper backend (beautifulsoup, bs4, httrack,
                           # selectolax, httpx, scrapy, playwright)
--max-depth N               # Maximum crawl depth (default: 5)
--max-pages N               # Maximum pages to crawl (default: 10000, -1 for unlimited)
--allowed-path PATH         # Restrict crawling to this path (overrides automatic restriction)
--request-delay SECONDS     # Delay between requests (default: 15.0)
--concurrent-requests N     # Number of concurrent requests (default: 2)
--user-agent STRING         # Custom user agent
--scraper-config PATH       # Path to scraper-specific config file (YAML/JSON)
--list-files                # List all downloaded files after completion
-v, --verbose               # Enable verbose output
-q, --quiet                 # Suppress all output except errors
--version                   # Show version information
```

Note: robots.txt is always respected and cannot be disabled.

### Configuration File

You can specify scraper-specific settings in a YAML or JSON configuration file:

```yaml
# beautifulsoup-config.yaml
parser: "html.parser" # Options: "html.parser", "lxml", "html5lib"
features: "lxml"
encoding: "auto" # Or specific encoding like "utf-8"
```

```yaml
# httrack-config.yaml
mirror_options:
  - "--assume-insecure" # For HTTPS issues
  - "--robots=3" # Strict robots.txt compliance
extra_filters:
  - "+*.css"
  - "+*.js"
  - "-*.zip"
```

Use with:

```bash
m1f-scrape https://example.com -o output/ \
  --scraper beautifulsoup \
  --scraper-config beautifulsoup-config.yaml
```

### Backend-Specific Configuration

Each backend can have specific configuration options:

#### BeautifulSoup Configuration

Create a `beautifulsoup.yaml`:

```yaml
scraper_config:
  parser: "lxml" # Options: "html.parser", "lxml", "html5lib"
  features: "lxml"
  encoding: "auto" # Or specific encoding like "utf-8"
```

#### HTTrack Configuration

Create a `httrack.yaml`:

```yaml
scraper_config:
  mirror_options:
    - "--assume-insecure" # For HTTPS issues
    - "--robots=3" # Strict robots.txt compliance
  extra_filters:
    - "+*.css"
    - "+*.js"
    - "-*.zip"
```

## Use Cases and Recommendations

### Static Documentation Sites

For sites with mostly static HTML content:

```bash
m1f-scrape https://docs.example.com -o docs/ \
  --scraper beautifulsoup \
  --max-depth 10 \
  --request-delay 0.2
```

### Complete Website Backup

For creating a complete offline mirror:

```bash
m1f-scrape https://example.com -o backup/ \
  --scraper httrack \
  --max-pages 10000
```

### Rate-Limited APIs

For sites with strict rate limits:

```bash
m1f-scrape https://api.example.com/docs -o api-docs/ \
  --scraper beautifulsoup \
  --request-delay 2.0 \
  --concurrent-requests 1
```

## Troubleshooting

### BeautifulSoup Issues

**Encoding Problems:**

```bash
# Create a config file with UTF-8 encoding
echo 'encoding: utf-8' > bs-config.yaml
m1f-scrape https://example.com -o output/ \
  --scraper beautifulsoup \
  --scraper-config bs-config.yaml
```

**Parser Issues:**

```bash
# Create a config file with different parser
echo 'parser: html5lib' > bs-config.yaml
m1f-scrape https://example.com -o output/ \
  --scraper beautifulsoup \
  --scraper-config bs-config.yaml
```

### HTTrack Issues

**SSL Certificate Problems:**

```bash
# Create a config file to ignore SSL errors (use with caution)
echo 'mirror_options: ["--assume-insecure"]' > httrack-config.yaml
m1f-scrape https://example.com -o output/ \
  --scraper httrack \
  --scraper-config httrack-config.yaml
```

**Incomplete Downloads:** HTTrack creates a cache that allows resuming. Check
the `.httrack` directory in your output folder.

## Performance Comparison

| Backend       | Speed     | Memory Usage | JavaScript | Accuracy  |
| ------------- | --------- | ------------ | ---------- | --------- |
| BeautifulSoup | Fast      | Low          | No         | High      |
| HTTrack       | Medium    | Medium       | No         | Very High |
| Selectolax    | Fastest   | Very Low     | No         | Medium    |
| Scrapy        | Very Fast | Low-Medium   | No         | High      |
| Playwright    | Slow      | High         | Yes        | Very High |

## Additional Backends

### Selectolax (httpx + selectolax)

The fastest HTML parsing solution using httpx for networking and selectolax for
parsing.

**Pros:**

- Blazing fast performance (C-based parser)
- Minimal memory footprint
- Excellent for large-scale simple scraping
- Modern async HTTP/2 support

**Cons:**

- No JavaScript support
- Limited parsing features compared to BeautifulSoup
- Less mature ecosystem

**Installation:**

```bash
pip install httpx selectolax
```

**Usage:**

```bash
# Basic usage
m1f-scrape https://example.com -o output/ --scraper selectolax

# With custom configuration
m1f-scrape https://example.com -o output/ \
  --scraper selectolax \
  --concurrent-requests 20 \
  --request-delay 0.1

# Using httpx alias
m1f-scrape https://example.com -o output/ --scraper httpx
```

### Scrapy

Industrial-strength web scraping framework with advanced features.

**Pros:**

- Battle-tested in production
- Built-in retry logic and error handling
- Auto-throttle based on server response
- Extensive middleware system
- Distributed crawling support
- Advanced caching and queuing

**Cons:**

- Steeper learning curve
- Heavier than simple scrapers
- Twisted-based (different async model)

**Installation:**

```bash
pip install scrapy
```

**Usage:**

```bash
# Basic usage
m1f-scrape https://example.com -o output/ --scraper scrapy

# With auto-throttle and caching
m1f-scrape https://example.com -o output/ \
  --scraper scrapy \
  --scraper-config scrapy.yaml

# Large-scale crawling
m1f-scrape https://example.com -o output/ \
  --scraper scrapy \
  --max-pages 10000 \
  --concurrent-requests 16
```

### Playwright

Browser automation for JavaScript-heavy websites and SPAs.

**Pros:**

- Full JavaScript execution
- Handles SPAs and dynamic content
- Multiple browser engines (Chromium, Firefox, WebKit)
- Screenshot and PDF generation
- Mobile device emulation
- Network interception

**Cons:**

- High resource usage
- Slower than HTML-only scrapers
- Requires browser installation

**Installation:**

```bash
pip install playwright
playwright install  # Install browser binaries
```

**Usage:**

```bash
# Basic usage
m1f-scrape https://example.com -o output/ --scraper playwright

# With custom browser settings
m1f-scrape https://example.com -o output/ \
  --scraper playwright \
  --scraper-config playwright.yaml

# For SPA with wait conditions
m1f-scrape https://spa-example.com -o output/ \
  --scraper playwright \
  --request-delay 2.0 \
  --concurrent-requests 2
```

## API Usage

You can also use the scraper backends programmatically:

```python
import asyncio
from tools.html2md.scrapers import create_scraper, ScraperConfig

async def scrape_example():
    # Configure scraper
    config = ScraperConfig(
        max_depth=5,
        max_pages=100,
        request_delay=0.5
    )

    # Create scraper instance
    scraper = create_scraper('beautifulsoup', config)

    # Scrape single page
    async with scraper:
        page = await scraper.scrape_url('https://example.com')
        print(f"Title: {page.title}")
        print(f"Content length: {len(page.content)}")

    # Scrape entire site
    async with scraper:
        async for page in scraper.scrape_site('https://example.com'):
            print(f"Scraped: {page.url}")

# Run the example
asyncio.run(scrape_example())
```

## Contributing

To add a new scraper backend:

1. Create a new file in `tools/html2md/scrapers/`
2. Inherit from `WebScraperBase`
3. Implement required methods: `scrape_url()` and `scrape_site()`
4. Register in `SCRAPER_REGISTRY` in `__init__.py`
5. Add tests in `tests/html2md/test_scrapers.py`
6. Update this documentation

See the BeautifulSoup implementation for a complete example.

======= 42_security.md ======
# m1f-scrape Security Documentation

## Overview

m1f-scrape implements multiple layers of security to protect against common web scraping vulnerabilities and ensure safe operation. This document covers all security features, potential risks, and best practices.

## Security Features

### 1. SSRF (Server-Side Request Forgery) Protection ✅

**Implementation**: Blocks requests to private IP addresses and cloud metadata endpoints.

**Location**: `tools/scrape_tool/scrapers/base.py:133-179`

**Protected Against**:
- Private networks (10.0.0.0/8, 172.16.0.0/12, 192.168.0.0/16)
- Localhost (127.0.0.0/8, ::1)
- Link-local addresses (169.254.0.0/16)
- Cloud metadata endpoints (169.254.169.254)
- Multicast addresses

**Configuration**: 
```yaml
check_ssrf: true  # Enable/disable SSRF protection (default: true)
```

**CLI**: `--disable-ssrf-check` to disable (use with caution)

### 2. Path Traversal Protection ✅

**Implementation**: Aggressive sanitization of file paths and names.

**Location**: `tools/scrape_tool/crawlers.py:1085-1162`

**Security Measures**:
- Removes `..` and `./` patterns from paths
- Sanitizes filenames to alphanumeric + `._-` characters
- Validates resolved paths stay within output directory
- Blocks dangerous file extensions (.exe, .dll, .bat, .cmd, .sh, etc.)

**Example Protection**:
```python
# Dangerous paths are blocked:
../../../etc/passwd  → Blocked
../../sensitive.txt  → Blocked
file/../../../etc    → Blocked

# Safe sanitized output:
my-file_name.jpg     → my-file_name.jpg
dangerous...file     → dangerous_file
.hidden..file        → hidden_file
```

### 3. File Type Validation ✅

**Implementation**: Magic number validation and content type checking.

**Location**: `tools/scrape_tool/file_validator.py`

**Features**:
- Magic number checking for 30+ file types
- Content-Type header validation
- File structure validation (images with Pillow, PDFs with PyPDF2)
- Detection of mismatched file extensions

**Dangerous Content Types Blocked**:
- application/x-executable
- application/x-msdownload
- application/x-msdos-program
- application/x-sh
- application/x-shellscript
- application/x-httpd-php
- application/x-httpd-cgi

### 4. HTML Content Security ⚠️

**Implementation**: HTML validation and malicious pattern detection.

**Location**: `tools/scrape_tool/file_validator.py:260-370`

**Security Checks**:
- Detection of eval() in scripts
- JavaScript URL detection in iframes/objects
- Inline binary detection (data: URLs)
- External resource tracking
- Malicious event handler detection

**Current Limitations**:
- ⚠️ **No HTML Sanitization**: Dangerous content is detected but not removed
- ⚠️ **Scripts Preserved**: JavaScript code remains in scraped content
- ⚠️ **Event Handlers Preserved**: onclick, onload, etc. are not stripped

### 5. Content Deduplication ✅

**Implementation**: SHA-256 based duplicate detection.

**Benefits**:
- Prevents duplicate downloads
- Saves bandwidth and storage
- Database-backed for memory efficiency

### 6. robots.txt Compliance ✅

**Implementation**: Automatic robots.txt checking with caching.

**Features**:
- Respects crawl delays
- Honors disallow rules
- Caches robots.txt per domain

## Security Risks and Mitigations

### High Risk Issues ⚠️

#### 1. XSS (Cross-Site Scripting) Vulnerability
**Risk**: Scraped HTML contains unsanitized JavaScript that could execute if rendered.

**Current State**: Scripts and event handlers are preserved in scraped content.

**Mitigation Needed**:
```python
# Recommended: Add HTML sanitization
from bleach import clean

allowed_tags = ['p', 'h1', 'h2', 'h3', 'h4', 'h5', 'h6', 
               'strong', 'em', 'ul', 'ol', 'li', 'a', 'img']
allowed_attributes = {'a': ['href'], 'img': ['src', 'alt']}

sanitized = clean(html_content, tags=allowed_tags, 
                 attributes=allowed_attributes, strip=True)
```

#### 2. Malicious JavaScript Preservation
**Risk**: Downloaded pages may contain malicious scripts that remain active.

**Mitigation Needed**:
```python
# Remove all script tags
for script in soup.find_all("script"):
    script.decompose()

# Remove event handlers
for tag in soup.find_all(True):
    for attr in list(tag.attrs.keys()):
        if attr.startswith('on'):
            del tag[attr]
```

### Medium Risk Issues ⚠️

#### 3. Inline Binary Data
**Risk**: Data URLs can embed malicious content bypassing file type restrictions.

**Current State**: Detected but not removed.

**Mitigation Needed**:
```python
# Remove or validate data URLs
for tag in soup.find_all(True):
    for attr in ['src', 'href', 'data']:
        if tag.get(attr) and tag[attr].startswith('data:'):
            # Validate MIME type or remove
            tag[attr] = ''
```

#### 4. External Resource References
**Risk**: External scripts and resources could be compromised.

**Current State**: Tracked but preserved.

## Configuration for Security

### Recommended Secure Configuration

```yaml
# .m1f-scrape-config.yml
crawler:
  check_ssrf: true           # Block private IPs
  check_canonical: true      # Prevent duplicates
  check_content_duplicates: true
  respect_robots_txt: true   # Always respect robots.txt
  verify_ssl: true          # Verify SSL certificates
  download_assets: false    # Don't download binaries by default
  download_external_assets: false  # Block external CDN assets
  
  # Conservative rate limiting
  request_delay: 5.0
  concurrent_requests: 2
  timeout: 30
  
  # File size limits
  max_asset_size: 10485760  # 10MB max for assets
  
  # Restricted asset types (safer subset)
  asset_types:
    - .jpg
    - .jpeg
    - .png
    - .gif
    - .css
    - .pdf
```

### High-Security Configuration

```yaml
# For maximum security (may break functionality)
crawler:
  check_ssrf: true
  download_assets: false    # No binary downloads
  download_external_assets: false
  allowed_domains:          # Whitelist specific domains
    - example.com
    - docs.example.com
  excluded_paths:           # Block sensitive paths
    - /admin/
    - /api/
    - /private/
  max_pages: 100           # Limit scope
  max_depth: 3             # Shallow crawling only
```

## BeautifulSoup Security Features

### What BeautifulSoup Provides:
- **Safe HTML Parsing**: Handles malformed HTML without code execution
- **Entity Decoding**: Properly decodes HTML entities
- **No Code Execution**: Parser doesn't execute JavaScript

### What BeautifulSoup Does NOT Provide:
- **No Sanitization**: All dangerous content is preserved
- **No Script Filtering**: JavaScript remains intact
- **No URL Validation**: Doesn't validate href/src attributes
- **No Event Handler Removal**: onclick, onload preserved
- **No Security Features**: It's a parser, not a security tool

## Security Best Practices

### 1. Always Validate Output
```bash
# Check for suspicious patterns in downloaded content
grep -r "eval(" ./downloaded_html/
grep -r "javascript:" ./downloaded_html/
grep -r "onclick=" ./downloaded_html/
```

### 2. Use Sandboxed Environments
- Run m1f-scrape in Docker containers
- Use virtual machines for untrusted sites
- Limit file system permissions

### 3. Post-Process for Safety
```python
# Example post-processing script
from pathlib import Path
from bs4 import BeautifulSoup

for html_file in Path("./downloaded_html").glob("**/*.html"):
    soup = BeautifulSoup(html_file.read_text(), 'html.parser')
    
    # Remove dangerous elements
    for script in soup.find_all("script"):
        script.decompose()
    
    for tag in soup.find_all(True):
        # Remove event handlers
        for attr in list(tag.attrs.keys()):
            if attr.startswith('on'):
                del tag[attr]
        
        # Remove javascript: URLs
        if tag.get('href') and 'javascript:' in tag['href']:
            tag['href'] = '#'
    
    html_file.write_text(str(soup))
```

### 4. Monitor and Audit
- Review logs for blocked requests
- Check for SSRF attempts
- Monitor file sizes and counts
- Audit downloaded content regularly

## Malware and Virus Scanning

### Post-Download Scanning

After downloading content with m1f-scrape, it's recommended to scan the files for malware and viruses, especially when scraping untrusted sites.

### 1. ClamAV (Open Source, Cross-Platform)

**Installation**:
```bash
# Ubuntu/Debian
sudo apt-get install clamav clamav-daemon
sudo freshclam  # Update virus definitions

# macOS
brew install clamav
freshclam

# Windows
# Download from https://www.clamav.net/downloads
```

**Scanning Downloaded Content**:
```bash
# Update virus definitions first
sudo freshclam

# Scan entire download directory
clamscan -r ./downloaded_html/

# Scan with automatic removal of infected files
clamscan -r --remove ./downloaded_html/

# Scan with detailed output and log
clamscan -r -v --log=/tmp/scan.log ./downloaded_html/

# Move infected files to quarantine
clamscan -r --move=/tmp/quarantine ./downloaded_html/
```

### 2. VirusTotal Integration (API-Based)

**Using vt-py (Python)**:
```python
# Install: pip install vt-py

import vt
import hashlib
from pathlib import Path

client = vt.Client("YOUR_API_KEY")

def scan_file(file_path):
    # Calculate file hash
    with open(file_path, "rb") as f:
        file_hash = hashlib.sha256(f.read()).hexdigest()
    
    # Check if file is already known
    try:
        file_report = client.get_object(f"/files/{file_hash}")
        stats = file_report.last_analysis_stats
        if stats["malicious"] > 0:
            print(f"⚠️ MALICIOUS: {file_path}")
            print(f"   Detections: {stats['malicious']}/{sum(stats.values())}")
        else:
            print(f"✅ Clean: {file_path}")
    except:
        # File not in database, upload for scanning
        with open(file_path, "rb") as f:
            analysis = client.scan_file(f)
        print(f"📤 Uploaded for analysis: {file_path}")
        print(f"   Check: https://www.virustotal.com/gui/file/{file_hash}")

# Scan all downloaded files
for file in Path("./downloaded_html").rglob("*"):
    if file.is_file():
        scan_file(file)

client.close()
```

### 3. YARA Rules (Pattern-Based Detection)

**Installation**:
```bash
# Ubuntu/Debian
sudo apt-get install yara

# macOS
brew install yara

# Python integration
pip install yara-python
```

**Create Custom Rules** (`web_threats.yar`):
```yara
rule Suspicious_JavaScript_Eval
{
    strings:
        $eval1 = "eval(" nocase
        $eval2 = "eval (" nocase
        $eval3 = ".eval(" nocase
        $obfuscated = /eval\s*\(\s*unescape/
        
    condition:
        any of them
}

rule Crypto_Miner_Scripts
{
    strings:
        $coinhive = "coinhive" nocase
        $cryptoloot = "cryptoloot" nocase
        $webminer = "webminer" nocase
        $miner1 = "CryptoNoter"
        $miner2 = "JSMiner"
        
    condition:
        any of them
}

rule Phishing_Patterns
{
    strings:
        $phish1 = "verify your account" nocase
        $phish2 = "suspended account" nocase
        $phish3 = "click here immediately" nocase
        $form = /<form[^>]+action\s*=\s*["'][^"']+phishing/
        
    condition:
        2 of them
}

rule Malicious_Iframe
{
    strings:
        $iframe1 = /<iframe[^>]+style\s*=\s*["']display\s*:\s*none/
        $iframe2 = /<iframe[^>]+width\s*=\s*["']0/
        $iframe3 = /<iframe[^>]+height\s*=\s*["']0/
        $suspicious_src = /<iframe[^>]+src\s*=\s*["']https?:\/\/[0-9]{1,3}\.[0-9]{1,3}/
        
    condition:
        any of them
}
```

**Scan with YARA**:
```bash
# Scan with rules file
yara web_threats.yar -r ./downloaded_html/

# Python script for detailed scanning
```

```python
import yara
from pathlib import Path

# Compile rules
rules = yara.compile(filepath='web_threats.yar')

# Scan files
for file_path in Path("./downloaded_html").rglob("*.html"):
    matches = rules.match(str(file_path))
    if matches:
        print(f"⚠️ Threats detected in {file_path}:")
        for match in matches:
            print(f"   - {match.rule}: {match.strings}")
```

### 4. Automated Scanning Pipeline

**Create a Post-Download Scanner** (`scan_downloads.sh`):
```bash
#!/bin/bash

DOWNLOAD_DIR="$1"
QUARANTINE_DIR="/tmp/quarantine"
LOG_FILE="/tmp/scan_$(date +%Y%m%d_%H%M%S).log"

echo "🔍 Starting security scan of $DOWNLOAD_DIR" | tee -a "$LOG_FILE"

# 1. ClamAV Scan
echo "Running ClamAV scan..." | tee -a "$LOG_FILE"
clamscan -r --move="$QUARANTINE_DIR/clamav" "$DOWNLOAD_DIR" >> "$LOG_FILE" 2>&1

# 2. YARA Scan
echo "Running YARA scan..." | tee -a "$LOG_FILE"
yara web_threats.yar -r "$DOWNLOAD_DIR" >> "$LOG_FILE" 2>&1

# 3. Check for suspicious patterns
echo "Checking for suspicious patterns..." | tee -a "$LOG_FILE"
grep -r "eval(" "$DOWNLOAD_DIR" >> "$LOG_FILE" 2>&1
grep -r "document.write" "$DOWNLOAD_DIR" >> "$LOG_FILE" 2>&1
grep -r "unescape(" "$DOWNLOAD_DIR" >> "$LOG_FILE" 2>&1

# 4. File type validation
echo "Validating file types..." | tee -a "$LOG_FILE"
find "$DOWNLOAD_DIR" -type f -exec file {} \; | grep -v "HTML\|text" >> "$LOG_FILE" 2>&1

echo "✅ Scan complete. Results saved to $LOG_FILE"

# Check if quarantine has files
if [ -d "$QUARANTINE_DIR" ] && [ "$(ls -A $QUARANTINE_DIR)" ]; then
    echo "⚠️ WARNING: Suspicious files quarantined in $QUARANTINE_DIR"
    exit 1
fi
```

### 5. Integration with m1f-scrape

**Automated Post-Download Scanning**:
```bash
# Create wrapper script
cat > scrape_and_scan.sh << 'EOF'
#!/bin/bash

# Run m1f-scrape
m1f-scrape "$@"

# Get output directory from arguments
OUTPUT_DIR=""
for i in "$@"; do
    if [[ "$prev" == "-o" ]]; then
        OUTPUT_DIR="$i"
        break
    fi
    prev="$i"
done

if [[ -n "$OUTPUT_DIR" ]]; then
    echo "🔍 Scanning downloaded content..."
    
    # Run ClamAV
    clamscan -r "$OUTPUT_DIR"
    
    # Run custom checks
    python3 check_security.py "$OUTPUT_DIR"
    
    echo "✅ Security scan complete"
fi
EOF

chmod +x scrape_and_scan.sh
```

### 6. Docker-Based Scanning (Isolated Environment)

**Dockerfile for Secure Scanning**:
```dockerfile
FROM ubuntu:22.04

RUN apt-get update && apt-get install -y \
    clamav \
    clamav-daemon \
    yara \
    python3 \
    python3-pip

RUN pip3 install vt-py yara-python

# Update ClamAV database
RUN freshclam

WORKDIR /scan

COPY scan_downloads.sh /usr/local/bin/
COPY web_threats.yar /etc/yara/

ENTRYPOINT ["scan_downloads.sh"]
```

**Usage**:
```bash
# Build scanner image
docker build -t web-scanner .

# Scan downloaded content in isolated container
docker run --rm -v $(pwd)/downloaded_html:/scan:ro web-scanner
```

### 7. Commercial Solutions

For enterprise environments, consider:

1. **Sophos CLI Scanner**
   ```bash
   savscan -all -archive -mime ./downloaded_html/
   ```

2. **ESET Command Line Scanner**
   ```bash
   /opt/eset/esets/sbin/esets_scan --clean-mode=delete ./downloaded_html/
   ```

3. **Kaspersky Endpoint Security**
   ```bash
   kavshell scan ./downloaded_html/ --action=disinfect
   ```

### Best Practices for Malware Scanning

1. **Always Update Definitions**
   ```bash
   # Before each scan
   sudo freshclam  # ClamAV
   yara-rules-update  # YARA
   ```

2. **Scan Immediately After Download**
   - Integrate scanning into your download workflow
   - Quarantine suspicious files before processing

3. **Use Multiple Scanners**
   - Different engines detect different threats
   - Combine signature-based and heuristic detection

4. **Monitor System Resources**
   ```bash
   # Watch for unusual activity during/after downloads
   htop
   netstat -tulpn
   ```

5. **Regular Scheduled Scans**
   ```cron
   # Add to crontab
   0 2 * * * /usr/local/bin/scan_downloads.sh /var/www/scraped/
   ```

### Security Checklist

Before scraping untrusted sites:

- [ ] Enable all security checks (SSRF, SSL verification)
- [ ] Set reasonable limits (max_pages, max_depth, timeouts)
- [ ] Configure allowed_domains if possible
- [ ] Disable asset downloads unless needed
- [ ] Use a sandboxed environment
- [ ] Plan post-processing sanitization
- [ ] Review robots.txt compliance
- [ ] Test with small page limits first
- [ ] Monitor resource usage
- [ ] Have incident response plan

## Reporting Security Issues

If you discover a security vulnerability in m1f-scrape:

1. **Do NOT** create a public GitHub issue
2. Contact the maintainers privately
3. Provide detailed reproduction steps
4. Allow time for a fix before disclosure

## Future Security Enhancements

Planned improvements:

1. **Built-in HTML Sanitization**
   - Integration with bleach library
   - Configurable sanitization levels
   - Preserve structure while removing dangerous content

2. **Content Security Policy Analysis**
   - Extract and analyze CSP headers
   - Warn about weak policies
   - Generate CSP recommendations

3. **Enhanced Binary Validation**
   - Deeper file format validation
   - Virus scanning integration
   - Sandboxed preview generation

4. **Security Scoring**
   - Rate scraped content for risk level
   - Provide security reports
   - Automated remediation options

## Conclusion

m1f-scrape provides robust security features for safe web scraping, but scraped content should always be treated as potentially dangerous. The tool detects but doesn't sanitize malicious content by default. Always post-process scraped content before use in production environments, especially if it will be rendered in browsers or processed by other systems.

For maximum security, combine m1f-scrape's built-in protections with post-processing sanitization and sandboxed environments.

======= scrapers/README.md ======
# HTML2MD Web Scrapers

This module provides a pluggable architecture for web scraping backends in the
HTML2MD tool.

## Architecture

The scraper system is built around:

- `WebScraperBase`: Abstract base class defining the scraper interface
- `ScraperConfig`: Configuration dataclass for all scrapers
- `create_scraper()`: Factory function to instantiate scrapers
- `SCRAPER_REGISTRY`: Registry of available backends

## Available Scrapers

### BeautifulSoup (`beautifulsoup`, `bs4`)

- **Purpose**: General-purpose web scraping for static sites
- **Features**: Async support, encoding detection, metadata extraction
- **Best for**: Most websites without JavaScript requirements

### HTTrack (`httrack`)

- **Purpose**: Complete website mirroring
- **Features**: Professional mirroring, preserves structure
- **Best for**: Creating offline copies of entire websites
- **Requires**: System installation of HTTrack

## Usage

```python
from tools.html2md.scrapers import create_scraper, ScraperConfig

# Configure scraper
config = ScraperConfig(
    max_depth=5,
    max_pages=100,
    request_delay=0.5,
    user_agent="Mozilla/5.0 ..."
)

# Create scraper instance
scraper = create_scraper('beautifulsoup', config)

# Use scraper
async with scraper:
    # Scrape single page
    page = await scraper.scrape_url('https://example.com')

    # Scrape entire site
    async for page in scraper.scrape_site('https://example.com'):
        print(f"Scraped: {page.url}")
```

## Adding New Scrapers

To add a new scraper backend:

1. Create a new file in this directory (e.g., `playwright.py`)
2. Create a class inheriting from `WebScraperBase`
3. Implement required methods:
   - `scrape_url()`: Scrape a single URL
   - `scrape_site()`: Scrape an entire website
4. Register in `__init__.py`:

   ```python
   from .playwright import PlaywrightScraper

   SCRAPER_REGISTRY['playwright'] = PlaywrightScraper
   ```

## Configuration

All scrapers share common configuration options through `ScraperConfig`:

- `max_depth`: Maximum crawl depth
- `max_pages`: Maximum pages to scrape
- `allowed_domains`: List of allowed domains
- `exclude_patterns`: URL patterns to exclude
- `request_delay`: Delay between requests
- `concurrent_requests`: Number of concurrent requests
- `user_agent`: User agent string
- `timeout`: Request timeout in seconds

Backend-specific options can be added as needed in the scraper implementation.
