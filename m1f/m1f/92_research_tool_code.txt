======= __init__.py ======
"""
m1f-research: AI-powered research extension for m1f

This module provides functionality to research any topic by:
- Using LLMs to find relevant URLs
- Scraping web content
- Converting HTML to Markdown
- Creating organized bundles from research findings
"""

from .cli import EnhancedResearchCommand, main
from .llm_interface import (
    LLMProvider,
    ClaudeProvider,
    ClaudeCodeProvider,
    GeminiProvider,
    CLIProvider,
    get_provider,
)
from .config import (
    ResearchConfig,
    LLMConfig,
    ScrapingConfig,
    OutputConfig,
    AnalysisConfig,
)
from .orchestrator import EnhancedResearchOrchestrator
from .models import ResearchResult, ScrapedContent, AnalyzedContent, ResearchSource
from .scraper import SmartScraper
from .content_filter import ContentFilter
from .analyzer import ContentAnalyzer
from .bundle_creator import SmartBundleCreator
from .readme_generator import ReadmeGenerator
from .analysis_templates import TEMPLATES, get_template
from .job_manager import JobManager
from .research_db import ResearchDatabase, JobDatabase, ResearchJob
from .url_manager import URLManager
from .smart_scraper import EnhancedSmartScraper

try:
    from _version import __version__, __version_info__
except ImportError:
    # Fallback for when running as a script
    __version__ = "3.9.0"
    __version_info__ = (3, 9, 0)
__all__ = [
    # Version
    "__version__",
    "__version_info__",
    # CLI
    "EnhancedResearchCommand",
    "main",
    # LLM
    "LLMProvider",
    "ClaudeProvider",
    "GeminiProvider",
    "CLIProvider",
    "get_provider",
    # Config
    "ResearchConfig",
    "LLMConfig",
    "ScrapingConfig",
    "OutputConfig",
    "AnalysisConfig",
    # Core
    "EnhancedResearchOrchestrator",
    "SmartScraper",
    "EnhancedSmartScraper",
    "ContentFilter",
    "ContentAnalyzer",
    "SmartBundleCreator",
    "ReadmeGenerator",
    # Job Management
    "JobManager",
    "ResearchDatabase",
    "JobDatabase",
    "ResearchJob",
    "URLManager",
    # Models
    "ResearchResult",
    "ScrapedContent",
    "AnalyzedContent",
    "ResearchSource",
    # Templates
    "TEMPLATES",
    "get_template",
]

======= __main__.py ======
# Copyright 2025 Franz und Franz GmbH
# SPDX-License-Identifier: Apache-2.0

"""
Entry point for running m1f-research as a module
"""
from .cli import main

if __name__ == "__main__":
    main()

======= analysis_generator.py ======
# Copyright 2025 Franz und Franz GmbH
#
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#     http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.

"""
Analysis generation for research bundles
"""

import logging
from pathlib import Path
from typing import Dict, Any, Optional, List
from datetime import datetime
from dataclasses import dataclass
import json

logger = logging.getLogger(__name__)


@dataclass
class AnalysisResult:
    """Result of analysis generation"""

    query: str
    analysis_type: str
    content: str
    metadata: Dict[str, Any]
    generated_at: datetime
    error: Optional[str] = None


class AnalysisGenerator:
    """Generates analysis documents from research bundles"""

    def __init__(self, llm_provider=None, output_dir: Optional[Path] = None):
        """
        Initialize the analysis generator

        Args:
            llm_provider: LLM provider instance for analysis
            output_dir: Directory to save analysis files
        """
        self.llm_provider = llm_provider
        self.output_dir = output_dir

    async def generate_analysis(
        self,
        bundle_path: Path,
        query: str,
        analysis_type: str = "summary",
        context: Optional[str] = None,
    ) -> AnalysisResult:
        """
        Generate analysis from a research bundle

        Args:
            bundle_path: Path to the research bundle file
            query: Original research query
            analysis_type: Type of analysis (summary, comparative, technical, trend)
            context: Optional additional context for analysis

        Returns:
            AnalysisResult object
        """
        logger.info(f"Generating {analysis_type} analysis for: {query}")

        # Read bundle content
        try:
            with open(bundle_path, "r", encoding="utf-8") as f:
                bundle_content = f.read()
        except Exception as e:
            logger.error(f"Error reading bundle: {e}")
            return AnalysisResult(
                query=query,
                analysis_type=analysis_type,
                content="",
                metadata={},
                generated_at=datetime.now(),
                error=f"Failed to read bundle: {e}",
            )

        # Generate analysis based on type
        if not self.llm_provider:
            logger.warning("No LLM provider configured, generating basic analysis")
            analysis_content = self._generate_basic_analysis(
                bundle_content, query, analysis_type
            )
        else:
            analysis_content = await self._generate_llm_analysis(
                bundle_content, query, analysis_type, context
            )

        # Create result
        result = AnalysisResult(
            query=query,
            analysis_type=analysis_type,
            content=analysis_content,
            metadata={
                "bundle_path": str(bundle_path),
                "bundle_size": len(bundle_content),
                "analysis_type": analysis_type,
                "has_context": bool(context),
            },
            generated_at=datetime.now(),
        )

        # Save analysis if output directory specified
        if self.output_dir:
            await self._save_analysis(result)

        return result

    async def _generate_llm_analysis(
        self,
        bundle_content: str,
        query: str,
        analysis_type: str,
        context: Optional[str] = None,
    ) -> str:
        """Generate analysis using LLM"""
        try:
            # Create analysis prompt based on type
            prompt = self._create_analysis_prompt(
                bundle_content, query, analysis_type, context
            )

            # Call LLM
            response = await self.llm_provider.query(prompt)

            if response.error:
                logger.error(f"LLM error during analysis: {response.error}")
                return self._generate_basic_analysis(
                    bundle_content, query, analysis_type
                )

            return response.content

        except Exception as e:
            logger.error(f"Error generating LLM analysis: {e}")
            return self._generate_basic_analysis(bundle_content, query, analysis_type)

    def _create_analysis_prompt(
        self,
        bundle_content: str,
        query: str,
        analysis_type: str,
        context: Optional[str] = None,
    ) -> str:
        """Create analysis prompt based on type"""

        # Truncate bundle if too long for context
        max_bundle_chars = 50000  # Adjust based on model limits
        if len(bundle_content) > max_bundle_chars:
            bundle_content = (
                bundle_content[:max_bundle_chars]
                + "\n\n[... content truncated for analysis ...]"
            )

        context_str = f"\n\nAdditional context: {context}" if context else ""

        if analysis_type == "summary":
            prompt = f"""Analyze the following research bundle about: "{query}"{context_str}

Create a comprehensive summary analysis that includes:
1. Executive Summary (2-3 paragraphs)
2. Key Findings (bullet points)
3. Main Themes and Patterns
4. Notable Sources and References
5. Conclusions and Insights
6. Potential Areas for Further Research

Format the output as a well-structured markdown document.

Research Bundle Content:
```
{bundle_content}
```

Generate the analysis:"""

        elif analysis_type == "comparative":
            prompt = f"""Analyze the following research bundle about: "{query}"{context_str}

Create a comparative analysis that:
1. Identifies different perspectives or approaches found
2. Compares and contrasts key viewpoints
3. Highlights agreements and disagreements
4. Evaluates the strength of different arguments
5. Synthesizes a balanced view

Format the output as a well-structured markdown document.

Research Bundle Content:
```
{bundle_content}
```

Generate the comparative analysis:"""

        elif analysis_type == "technical":
            prompt = f"""Analyze the following research bundle about: "{query}"{context_str}

Create a technical analysis that includes:
1. Technical concepts and terminology explained
2. Implementation details and specifications
3. Best practices and recommendations
4. Technical challenges and solutions
5. Code examples or technical diagrams (if applicable)
6. Performance considerations

Format the output as a well-structured markdown document.

Research Bundle Content:
```
{bundle_content}
```

Generate the technical analysis:"""

        elif analysis_type == "trend":
            prompt = f"""Analyze the following research bundle about: "{query}"{context_str}

Create a trend analysis that identifies:
1. Current state and recent developments
2. Historical context and evolution
3. Emerging trends and patterns
4. Future predictions and projections
5. Key drivers and influencing factors
6. Potential disruptions or changes

Format the output as a well-structured markdown document.

Research Bundle Content:
```
{bundle_content}
```

Generate the trend analysis:"""

        else:
            # Default to summary
            prompt = self._create_analysis_prompt(
                bundle_content, query, "summary", context
            )

        return prompt

    def _generate_basic_analysis(
        self, bundle_content: str, query: str, analysis_type: str
    ) -> str:
        """Generate basic analysis without LLM"""
        # Extract basic statistics
        lines = bundle_content.split("\n")
        word_count = len(bundle_content.split())
        url_count = bundle_content.count("http://") + bundle_content.count("https://")

        # Find section markers
        sections = []
        for line in lines:
            if line.startswith("# ") or line.startswith("## "):
                sections.append(line.strip("#").strip())

        # Create basic analysis
        analysis = f"""# Research Analysis: {query}

**Analysis Type:** {analysis_type.title()}  
**Generated:** {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}

## Summary Statistics

- Total content size: {word_count:,} words
- Number of sources: {url_count}
- Major sections: {len(sections)}

## Content Overview

This research bundle contains information gathered about "{query}". The content has been compiled from multiple sources and organized for analysis.

## Key Sections Found

"""

        for section in sections[:10]:  # List first 10 sections
            analysis += f"- {section}\n"

        if len(sections) > 10:
            analysis += f"- ... and {len(sections) - 10} more sections\n"

        analysis += """

## Notes

This is a basic analysis generated without AI assistance. For more detailed insights, configure an LLM provider in your research settings.

---

*This analysis provides a structural overview of the research bundle. Review the full bundle content for detailed information.*
"""

        return analysis

    async def _save_analysis(self, result: AnalysisResult) -> Path:
        """Save analysis to file"""
        if not self.output_dir:
            raise ValueError("No output directory specified")

        # Create filename
        timestamp = result.generated_at.strftime("%Y%m%d_%H%M%S")
        filename = f"RESEARCH_ANALYSIS_{result.analysis_type}_{timestamp}.md"
        filepath = self.output_dir / filename

        # Write content
        try:
            with open(filepath, "w", encoding="utf-8") as f:
                # Write header
                f.write(f"<!-- Research Analysis -->\n")
                f.write(f"<!-- Query: {result.query} -->\n")
                f.write(f"<!-- Type: {result.analysis_type} -->\n")
                f.write(f"<!-- Generated: {result.generated_at.isoformat()} -->\n")
                f.write(f"<!-- Metadata: {json.dumps(result.metadata)} -->\n\n")

                # Write content
                f.write(result.content)

            logger.info(f"Analysis saved to: {filepath}")
            return filepath

        except Exception as e:
            logger.error(f"Error saving analysis: {e}")
            raise

    async def generate_multi_analysis(
        self,
        bundle_path: Path,
        query: str,
        analysis_types: List[str],
        context: Optional[str] = None,
    ) -> Dict[str, AnalysisResult]:
        """
        Generate multiple types of analysis for a bundle

        Args:
            bundle_path: Path to research bundle
            query: Original research query
            analysis_types: List of analysis types to generate
            context: Optional context

        Returns:
            Dictionary of analysis_type -> AnalysisResult
        """
        results = {}

        for analysis_type in analysis_types:
            logger.info(f"Generating {analysis_type} analysis...")
            result = await self.generate_analysis(
                bundle_path, query, analysis_type, context
            )
            results[analysis_type] = result

        return results

    def combine_analyses(
        self, analyses: Dict[str, AnalysisResult], output_path: Optional[Path] = None
    ) -> str:
        """
        Combine multiple analyses into a single document

        Args:
            analyses: Dictionary of analysis results
            output_path: Optional path to save combined analysis

        Returns:
            Combined analysis content
        """
        combined = f"""# Combined Research Analysis

**Generated:** {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}

## Table of Contents

"""

        # Add TOC
        for analysis_type in analyses:
            combined += (
                f"- [{analysis_type.title()} Analysis](#{analysis_type}-analysis)\n"
            )

        combined += "\n---\n\n"

        # Add each analysis
        for analysis_type, result in analyses.items():
            combined += f"## {analysis_type.title()} Analysis\n\n"
            combined += result.content
            combined += "\n\n---\n\n"

        # Save if path provided
        if output_path:
            try:
                with open(output_path, "w", encoding="utf-8") as f:
                    f.write(combined)
                logger.info(f"Combined analysis saved to: {output_path}")
            except Exception as e:
                logger.error(f"Error saving combined analysis: {e}")

        return combined

======= analysis_templates.py ======
# Copyright 2025 Franz und Franz GmbH
#
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#     http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.

"""
Analysis templates for different research types
"""
from typing import Dict, Any, List
from dataclasses import dataclass


@dataclass
class AnalysisTemplate:
    """Template for content analysis"""

    name: str
    description: str
    focus_areas: List[str]
    evaluation_criteria: Dict[str, float]  # criterion -> weight
    prompt_paths: Dict[str, str]  # analysis type -> prompt file path
    content_preferences: Dict[str, Any]


# Technical analysis template
TECHNICAL_TEMPLATE = AnalysisTemplate(
    name="technical",
    description="For implementation details and code examples",
    focus_areas=[
        "implementation_details",
        "code_examples",
        "performance_considerations",
        "best_practices",
        "common_pitfalls",
    ],
    evaluation_criteria={
        "code_quality": 0.3,
        "practical_examples": 0.3,
        "depth_of_explanation": 0.2,
        "accuracy": 0.2,
    },
    prompt_paths={
        "relevance": "analysis/technical_relevance.md",
        "key_points": "analysis/technical_key_points.md",
    },
    content_preferences={
        "prefer_code_examples": True,
        "min_code_ratio": 0.2,
        "preferred_content_types": ["tutorial", "documentation", "code"],
        "relevance_boost_keywords": [
            "implementation",
            "example",
            "code",
            "performance",
            "optimization",
            "pattern",
            "practice",
            "tutorial",
        ],
    },
)


# Academic analysis template
ACADEMIC_TEMPLATE = AnalysisTemplate(
    name="academic",
    description="For theoretical understanding and research papers",
    focus_areas=[
        "theoretical_foundations",
        "research_methodology",
        "citations_references",
        "empirical_evidence",
        "future_directions",
    ],
    evaluation_criteria={
        "theoretical_depth": 0.3,
        "citations_quality": 0.2,
        "methodology_rigor": 0.2,
        "novelty": 0.15,
        "clarity": 0.15,
    },
    prompt_paths={
        "relevance": "analysis/academic_relevance.md",
        "key_points": "analysis/academic_key_points.md",
    },
    content_preferences={
        "prefer_code_examples": False,
        "min_citation_count": 5,
        "preferred_content_types": ["research", "paper", "study", "analysis"],
        "relevance_boost_keywords": [
            "research",
            "study",
            "theory",
            "framework",
            "methodology",
            "findings",
            "conclusion",
            "hypothesis",
            "evidence",
        ],
    },
)


# Tutorial analysis template
TUTORIAL_TEMPLATE = AnalysisTemplate(
    name="tutorial",
    description="For step-by-step guides and learning resources",
    focus_areas=[
        "learning_progression",
        "clear_instructions",
        "practical_exercises",
        "prerequisite_coverage",
        "common_mistakes",
    ],
    evaluation_criteria={
        "clarity": 0.3,
        "completeness": 0.25,
        "practical_examples": 0.25,
        "learning_curve": 0.2,
    },
    prompt_paths={
        "relevance": "analysis/tutorial_relevance.md",
        "key_points": "analysis/tutorial_key_points.md",
    },
    content_preferences={
        "prefer_code_examples": True,
        "prefer_numbered_steps": True,
        "preferred_content_types": ["tutorial", "guide", "howto", "walkthrough"],
        "relevance_boost_keywords": [
            "step-by-step",
            "tutorial",
            "guide",
            "learn",
            "example",
            "exercise",
            "practice",
            "beginner",
            "getting started",
        ],
    },
)


# Reference analysis template
REFERENCE_TEMPLATE = AnalysisTemplate(
    name="reference",
    description="For API documentation and reference materials",
    focus_areas=[
        "api_completeness",
        "parameter_documentation",
        "return_value_specs",
        "usage_examples",
        "error_handling",
    ],
    evaluation_criteria={
        "completeness": 0.3,
        "accuracy": 0.3,
        "examples": 0.2,
        "organization": 0.2,
    },
    prompt_paths={
        "relevance": "analysis/reference_relevance.md",
        "key_points": "analysis/reference_key_points.md",
    },
    content_preferences={
        "prefer_code_examples": True,
        "prefer_structured_data": True,
        "preferred_content_types": ["documentation", "reference", "api", "spec"],
        "relevance_boost_keywords": [
            "api",
            "reference",
            "documentation",
            "parameters",
            "returns",
            "method",
            "function",
            "class",
            "interface",
            "specification",
        ],
    },
)


# General analysis template (default)
GENERAL_TEMPLATE = AnalysisTemplate(
    name="general",
    description="Balanced analysis for any topic",
    focus_areas=[
        "main_concepts",
        "practical_applications",
        "examples_illustrations",
        "pros_and_cons",
        "related_topics",
    ],
    evaluation_criteria={
        "relevance": 0.3,
        "clarity": 0.25,
        "depth": 0.25,
        "practicality": 0.2,
    },
    prompt_paths={
        "relevance": "analysis/general_relevance.md",
        "key_points": "analysis/general_key_points.md",
    },
    content_preferences={
        "prefer_code_examples": False,
        "balanced_content": True,
        "preferred_content_types": None,  # No preference
        "relevance_boost_keywords": [],
    },
)


# Template registry
TEMPLATES = {
    "technical": TECHNICAL_TEMPLATE,
    "academic": ACADEMIC_TEMPLATE,
    "tutorial": TUTORIAL_TEMPLATE,
    "reference": REFERENCE_TEMPLATE,
    "general": GENERAL_TEMPLATE,
}


def get_template(name: str) -> AnalysisTemplate:
    """Get analysis template by name"""
    return TEMPLATES.get(name, GENERAL_TEMPLATE)


def apply_template_scoring(
    template: AnalysisTemplate, analysis_results: Dict[str, Any]
) -> float:
    """Apply template-specific scoring weights to analysis results"""
    weighted_score = 0.0
    total_weight = 0.0

    # Map analysis results to template criteria
    criteria_scores = {
        "relevance": analysis_results.get("relevance_score", 5.0),
        "clarity": estimate_clarity_score(analysis_results),
        "completeness": estimate_completeness_score(analysis_results),
        "accuracy": analysis_results.get("technical_accuracy", 7.0),
        "practical_examples": estimate_example_score(analysis_results),
        "depth": estimate_depth_score(analysis_results),
    }

    # Apply template weights
    for criterion, weight in template.evaluation_criteria.items():
        if criterion in criteria_scores:
            weighted_score += criteria_scores[criterion] * weight
            total_weight += weight

    # Normalize to 0-10 scale
    return (weighted_score / total_weight) if total_weight > 0 else 5.0


def estimate_clarity_score(analysis: Dict[str, Any]) -> float:
    """Estimate clarity based on analysis metadata"""
    # Simple heuristic based on summary quality
    summary = analysis.get("summary", "")
    if len(summary) > 50 and len(summary) < 500:
        return 8.0
    return 6.0


def estimate_completeness_score(analysis: Dict[str, Any]) -> float:
    """Estimate completeness based on key points"""
    key_points = analysis.get("key_points", [])
    if len(key_points) >= 5:
        return 9.0
    elif len(key_points) >= 3:
        return 7.0
    return 5.0


def estimate_example_score(analysis: Dict[str, Any]) -> float:
    """Estimate quality of examples"""
    content_type = analysis.get("content_type", "")
    if content_type in ["tutorial", "code"]:
        return 8.0
    elif "example" in str(analysis.get("topics", [])).lower():
        return 7.0
    return 5.0


def estimate_depth_score(analysis: Dict[str, Any]) -> float:
    """Estimate depth of coverage"""
    # Based on technical level and key points
    level = analysis.get("technical_level", "intermediate")
    key_points = len(analysis.get("key_points", []))

    if level == "advanced" and key_points >= 4:
        return 9.0
    elif level == "intermediate" and key_points >= 3:
        return 7.0
    return 5.0

======= analyzer.py ======
# Copyright 2025 Franz und Franz GmbH
#
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#     http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.

"""
Content analysis using LLMs for m1f-research
"""
import asyncio
import logging
from typing import List, Dict, Any, Optional
from datetime import datetime
import re
import json

from .models import ScrapedContent, AnalyzedContent
from .llm_interface import LLMProvider
from .config import AnalysisConfig
from .analysis_templates import get_template, apply_template_scoring
from .prompt_utils import get_analysis_prompt, get_synthesis_prompt

logger = logging.getLogger(__name__)


class ContentAnalyzer:
    """
    LLM-powered content analysis with:
    - Relevance scoring (0-10)
    - Key points extraction
    - Content summarization
    - Content type detection
    - Topic extraction
    """

    def __init__(
        self,
        llm_provider: LLMProvider,
        config: AnalysisConfig,
        template_name: str = "general",
    ):
        self.llm = llm_provider
        self.config = config
        self.template = get_template(template_name)

    async def analyze_content(
        self,
        content_list: List[ScrapedContent],
        research_query: str,
        batch_size: int = 5,
    ) -> List[AnalyzedContent]:
        """
        Analyze scraped content for relevance and insights

        Args:
            content_list: List of scraped content to analyze
            research_query: Original research query for context
            batch_size: Number of items to analyze concurrently

        Returns:
            List of analyzed content with scores and insights
        """
        analyzed = []

        # Process in batches to avoid overwhelming the LLM
        for i in range(0, len(content_list), batch_size):
            batch = content_list[i : i + batch_size]

            # Analyze batch concurrently
            tasks = [
                self._analyze_single_content(item, research_query) for item in batch
            ]

            batch_results = await asyncio.gather(*tasks, return_exceptions=True)

            # Process results
            for item, result in zip(batch, batch_results):
                if isinstance(result, Exception):
                    logger.error(f"Analysis failed for {item.url}: {result}")
                    # Create fallback analyzed content
                    analyzed.append(self._create_fallback_analysis(item))
                else:
                    analyzed.append(result)

        return analyzed

    async def _analyze_single_content(
        self, content: ScrapedContent, research_query: str
    ) -> AnalyzedContent:
        """Analyze a single piece of content"""
        try:
            # Prepare content for analysis (truncate if needed)
            content_for_analysis = self._prepare_content(content.content)

            # Get comprehensive analysis from LLM
            analysis = await self._get_llm_analysis(
                content_for_analysis, research_query, content.url
            )

            # Create analyzed content
            return AnalyzedContent(
                url=content.url,
                title=content.title,
                content=content.content,
                relevance_score=analysis.get("relevance_score", 5.0),
                key_points=analysis.get("key_points", []),
                summary=analysis.get("summary", ""),
                content_type=analysis.get("content_type"),
                analysis_metadata=analysis,
            )

        except Exception as e:
            logger.error(f"Error analyzing {content.url}: {e}")
            return self._create_fallback_analysis(content)

    async def _get_llm_analysis(
        self, content: str, research_query: str, url: str
    ) -> Dict[str, Any]:
        """Get comprehensive analysis from LLM"""
        # Get template-specific or default analysis prompt
        prompt = get_analysis_prompt(
            template_name=self.template.name,
            prompt_type="relevance",
            query=research_query,
            url=url,
            content=content,
        )

        # Get analysis from LLM
        response = await self.llm.query(prompt)

        if response.error:
            raise Exception(f"LLM error: {response.error}")

        # Parse JSON response
        try:
            # Extract JSON from response
            json_str = self._extract_json(response.content)
            analysis = json.loads(json_str)

            # Validate and normalize the analysis
            analysis = self._validate_analysis(analysis)

            # Apply template-based scoring adjustments
            if self.template.name != "general":
                original_score = analysis["relevance_score"]
                template_adjusted_score = apply_template_scoring(
                    self.template, analysis
                )
                # Blend original and template scores
                analysis["relevance_score"] = (
                    original_score * 0.6 + template_adjusted_score * 0.4
                )
                analysis["template_score"] = template_adjusted_score

            return analysis

        except json.JSONDecodeError as e:
            logger.error(f"Failed to parse LLM response as JSON: {e}")
            # Try to extract what we can
            return self._extract_partial_analysis(response.content)

    def _prepare_content(self, content: str, max_length: int = 3000) -> str:
        """Prepare content for LLM analysis"""
        # Clean up content
        content = content.strip()

        # Remove excessive whitespace
        content = re.sub(r"\n{3,}", "\n\n", content)
        content = re.sub(r" {2,}", " ", content)

        # Truncate if too long
        if len(content) > max_length:
            # Try to truncate at a reasonable boundary
            truncated = content[:max_length]

            # Find last complete sentence
            last_period = truncated.rfind(".")
            if last_period > max_length * 0.8:
                content = truncated[: last_period + 1]
            else:
                content = truncated + "..."

        return content

    def _extract_json(self, text: str) -> str:
        """Extract JSON from LLM response"""
        # Remove markdown code blocks if present
        if "```json" in text:
            match = re.search(r"```json\s*(.*?)\s*```", text, re.DOTALL)
            if match:
                return match.group(1)

        # Try to find JSON object
        match = re.search(r"\{.*\}", text, re.DOTALL)
        if match:
            return match.group(0)

        return text

    def _validate_analysis(self, analysis: Dict[str, Any]) -> Dict[str, Any]:
        """Validate and normalize analysis results"""
        # Ensure required fields
        validated = {
            "relevance_score": float(analysis.get("relevance_score", 5.0)),
            "summary": str(analysis.get("summary", "")),
            "key_points": list(analysis.get("key_points", [])),
            "content_type": analysis.get("content_type", "unknown"),
            "topics": list(analysis.get("topics", [])),
            "technical_level": analysis.get("technical_level", "intermediate"),
            "strengths": analysis.get("strengths", ""),
            "limitations": analysis.get("limitations", ""),
        }

        # Clamp relevance score
        validated["relevance_score"] = max(0.0, min(10.0, validated["relevance_score"]))

        # Ensure key_points is a list of strings
        validated["key_points"] = [str(point) for point in validated["key_points"][:5]]

        # Validate content type
        valid_types = [
            "tutorial",
            "documentation",
            "blog",
            "discussion",
            "code",
            "reference",
            "news",
            "technical",
            "academic",
            "unknown",
        ]
        if validated["content_type"] not in valid_types:
            validated["content_type"] = "unknown"

        # Preserve any additional fields from the original analysis
        for key, value in analysis.items():
            if key not in validated:
                validated[key] = value

        return validated

    def _extract_partial_analysis(self, text: str) -> Dict[str, Any]:
        """Try to extract partial analysis from non-JSON response"""
        analysis = {
            "relevance_score": 5.0,
            "summary": "",
            "key_points": [],
            "content_type": "unknown",
        }

        # Try to extract relevance score
        score_match = re.search(r"relevance.*?(\d+(?:\.\d+)?)", text, re.IGNORECASE)
        if score_match:
            try:
                analysis["relevance_score"] = float(score_match.group(1))
            except:
                pass

        # Try to extract summary
        summary_match = re.search(r"summary[:\s]+(.*?)(?:\n|$)", text, re.IGNORECASE)
        if summary_match:
            analysis["summary"] = summary_match.group(1).strip()

        # Try to extract bullet points as key points
        bullets = re.findall(r"[-â€¢*]\s+(.+?)(?:\n|$)", text)
        if bullets:
            analysis["key_points"] = bullets[:5]

        return analysis

    def _create_fallback_analysis(self, content: ScrapedContent) -> AnalyzedContent:
        """Create fallback analysis when LLM analysis fails"""
        # Basic heuristic analysis
        word_count = len(content.content.split())
        has_code = bool(re.search(r"```|`[^`]+`", content.content))

        # Estimate relevance based on title
        relevance = 5.0

        # Extract first paragraph as summary
        paragraphs = content.content.split("\n\n")
        summary = paragraphs[0][:200] + "..." if paragraphs else "No summary available"

        return AnalyzedContent(
            url=content.url,
            title=content.title,
            content=content.content,
            relevance_score=relevance,
            key_points=[],
            summary=summary,
            content_type="code" if has_code else "unknown",
            analysis_metadata={"fallback": True, "word_count": word_count},
        )

    async def extract_topics(
        self, analyzed_content: List[AnalyzedContent]
    ) -> Dict[str, List[str]]:
        """Extract and group topics from analyzed content"""
        all_topics = []

        for item in analyzed_content:
            topics = item.analysis_metadata.get("topics", [])
            all_topics.extend(topics)

        # Count topic frequency
        from collections import Counter

        topic_counts = Counter(all_topics)

        # Group by frequency
        grouped = {
            "primary": [t for t, c in topic_counts.items() if c >= 3],
            "secondary": [t for t, c in topic_counts.items() if c == 2],
            "mentioned": [t for t, c in topic_counts.items() if c == 1],
        }

        return grouped

    async def generate_synthesis(
        self, analyzed_content: List[AnalyzedContent], research_query: str
    ) -> str:
        """Generate a synthesis of all analyzed content"""
        if not analyzed_content:
            return "No content available for synthesis."

        # Prepare content summaries
        summaries = []
        for item in analyzed_content[:10]:  # Limit to top 10
            summaries.append(
                f"- {item.title} (Relevance: {item.relevance_score}): {item.summary}"
            )

        prompt = get_synthesis_prompt(
            query=research_query, summaries=chr(10).join(summaries)
        )

        response = await self.llm.query(prompt)

        if response.error:
            return "Unable to generate synthesis."

        return response.content

======= bundle_creator.py ======
# Copyright 2025 Franz und Franz GmbH
#
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#     http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.

"""
Smart bundle creation with intelligent content organization for m1f-research
"""
import re
import logging
from pathlib import Path
from typing import List, Dict, Any, Optional, Tuple
from datetime import datetime
import json
from collections import defaultdict, Counter

from m1f.file_operations import (
    safe_open,
)

from .models import AnalyzedContent
from .config import OutputConfig, ResearchConfig
from .llm_interface import LLMProvider
from .readme_generator import ReadmeGenerator
from .prompt_utils import get_subtopic_grouping_prompt, get_topic_summary_prompt

logger = logging.getLogger(__name__)


class SmartBundleCreator:
    """
    Intelligent bundle creation with:
    - Subtopic grouping and organization
    - Hierarchical content structuring
    - Smart navigation generation
    - Cross-reference linking
    - Summary synthesis per topic
    """

    def __init__(
        self,
        llm_provider: Optional[LLMProvider] = None,
        config: Optional[OutputConfig] = None,
        research_config: Optional[ResearchConfig] = None,
    ):
        self.llm = llm_provider
        self.config = config or OutputConfig()
        self.research_config = research_config

    async def create_bundle(
        self,
        content_list: List[AnalyzedContent],
        research_query: str,
        output_dir: Path,
        synthesis: Optional[str] = None,
    ) -> Path:
        """
        Create an intelligently organized research bundle

        Args:
            content_list: List of analyzed content to include
            research_query: Original research query
            output_dir: Directory to save bundle
            synthesis: Optional pre-generated synthesis

        Returns:
            Path to the created bundle file
        """
        # Group content by subtopics
        topic_groups = await self._group_by_subtopics(content_list, research_query)

        # Generate bundle structure
        bundle_content = await self._generate_bundle_content(
            topic_groups, research_query, synthesis
        )

        # Write bundle file
        bundle_path = output_dir / f"{self.config.bundle_prefix}-bundle.md"
        with safe_open(bundle_path, "w", encoding="utf-8") as f:
            if f:
                f.write(bundle_content)

        # Create supplementary files if enabled
        if self.config.create_index:
            await self._create_index_file(topic_groups, output_dir)

        if self.config.include_metadata:
            await self._create_metadata_file(content_list, research_query, output_dir)

        # Generate README if we have research config
        if self.research_config:
            readme_gen = ReadmeGenerator(self.research_config)
            await readme_gen.generate_readme(
                content_list=content_list,
                research_query=research_query,
                output_dir=output_dir,
                topic_groups=topic_groups,
                synthesis=synthesis,
            )

            # Also generate citations file
            await readme_gen.generate_citation_file(
                content_list, research_query, output_dir
            )

        logger.info(f"Created smart bundle at: {bundle_path}")
        return bundle_path

    async def _group_by_subtopics(
        self, content_list: List[AnalyzedContent], research_query: str
    ) -> Dict[str, List[AnalyzedContent]]:
        """Group content by subtopics using content analysis"""
        if not self.llm:
            # Fallback to simple grouping by content type
            return self._simple_grouping(content_list)

        # Extract topics from all content
        all_topics = []
        for item in content_list:
            topics = item.analysis_metadata.get("topics", [])
            all_topics.extend([(topic, item) for topic in topics])

        # If we have topics from analysis, use them
        if all_topics:
            return self._group_by_extracted_topics(all_topics, content_list)

        # Otherwise, use LLM to identify subtopics
        return await self._llm_group_by_subtopics(content_list, research_query)

    def _simple_grouping(
        self, content_list: List[AnalyzedContent]
    ) -> Dict[str, List[AnalyzedContent]]:
        """Simple grouping by content type"""
        groups = defaultdict(list)

        for item in content_list:
            content_type = item.content_type or "general"
            groups[content_type].append(item)

        # Sort items within each group by relevance
        for group in groups.values():
            group.sort(key=lambda x: x.relevance_score, reverse=True)

        return dict(groups)

    def _group_by_extracted_topics(
        self,
        topic_items: List[Tuple[str, AnalyzedContent]],
        all_content: List[AnalyzedContent],
    ) -> Dict[str, List[AnalyzedContent]]:
        """Group content by extracted topics"""
        # Count topic frequencies
        topic_counts = Counter(topic for topic, _ in topic_items)

        # Get top topics (those appearing in multiple documents)
        top_topics = [
            topic for topic, count in topic_counts.most_common(10) if count >= 2
        ]

        # Create groups
        groups = defaultdict(list)
        assigned = set()

        # Assign content to most relevant topic
        for item in all_content:
            item_topics = item.analysis_metadata.get("topics", [])

            # Find best matching top topic
            best_topic = None
            for topic in top_topics:
                if topic in item_topics:
                    best_topic = topic
                    break

            if best_topic:
                groups[best_topic].append(item)
                assigned.add(item.url)

        # Add ungrouped items to "Other" category
        other_items = [item for item in all_content if item.url not in assigned]
        if other_items:
            groups["Other Resources"].extend(other_items)

        # Sort items within each group by relevance
        for group in groups.values():
            group.sort(key=lambda x: x.relevance_score, reverse=True)

        return dict(groups)

    async def _llm_group_by_subtopics(
        self, content_list: List[AnalyzedContent], research_query: str
    ) -> Dict[str, List[AnalyzedContent]]:
        """Use LLM to identify and group by subtopics"""
        # Prepare content summaries for LLM
        summaries = []
        for i, item in enumerate(content_list):
            summaries.append(f"{i}. {item.title}: {item.summary[:100]}...")

        prompt = get_subtopic_grouping_prompt(
            query=research_query, summaries=chr(10).join(summaries)
        )

        try:
            response = await self.llm.query(prompt)

            if response.error:
                logger.error(f"LLM error during grouping: {response.error}")
                return self._simple_grouping(content_list)

            # Parse JSON response
            import re

            json_match = re.search(r"\{.*\}", response.content, re.DOTALL)
            if json_match:
                grouping_data = json.loads(json_match.group(0))

                # Create groups based on LLM response
                groups = {}
                used_indices = set()

                for subtopic in grouping_data.get("subtopics", []):
                    name = subtopic["name"]
                    indices = subtopic.get("item_indices", [])

                    groups[name] = []
                    for idx in indices:
                        if 0 <= idx < len(content_list) and idx not in used_indices:
                            groups[name].append(content_list[idx])
                            used_indices.add(idx)

                # Add any ungrouped items
                ungrouped = [
                    item for i, item in enumerate(content_list) if i not in used_indices
                ]
                if ungrouped:
                    groups["Other Resources"] = ungrouped

                # Sort items within each group
                for group in groups.values():
                    group.sort(key=lambda x: x.relevance_score, reverse=True)

                return groups

        except Exception as e:
            logger.error(f"Error in LLM grouping: {e}")

        # Fallback to simple grouping
        return self._simple_grouping(content_list)

    async def _generate_bundle_content(
        self,
        topic_groups: Dict[str, List[AnalyzedContent]],
        research_query: str,
        synthesis: Optional[str] = None,
    ) -> str:
        """Generate the bundle content with smart organization"""
        lines = []

        # Header
        lines.append(f"# Research: {research_query}")
        lines.append(f"\nGenerated on: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")

        # Statistics
        total_sources = sum(len(items) for items in topic_groups.values())
        lines.append(f"Total sources: {total_sources}")
        lines.append(f"Topics covered: {len(topic_groups)}")
        lines.append("\n---\n")

        # Executive Summary
        if self.config.create_summary:
            lines.append("## Executive Summary\n")

            if synthesis:
                lines.append(synthesis)
                lines.append("\n")

            # Topic overview
            lines.append("### Topics Covered:\n")
            for topic, items in topic_groups.items():
                avg_relevance = sum(item.relevance_score for item in items) / len(items)
                lines.append(
                    f"- **{topic}** ({len(items)} sources, avg relevance: {avg_relevance:.1f}/10)"
                )
            lines.append("\n---\n")

        # Table of Contents
        if self.config.create_index:
            lines.append("## Table of Contents\n")

            # Topic-based navigation
            for i, (topic, items) in enumerate(topic_groups.items(), 1):
                topic_anchor = self._create_anchor(topic)
                lines.append(f"{i}. [{topic}](#{topic_anchor}) ({len(items)} sources)")

                # Show top items under each topic
                if len(items) > 0:
                    for j, item in enumerate(items[:3], 1):  # Show top 3
                        item_anchor = self._create_anchor(f"{topic}-{j}-{item.title}")
                        lines.append(f"   - [{item.title[:60]}...](#{item_anchor})")
                    if len(items) > 3:
                        lines.append(f"   - ...and {len(items) - 3} more")

            lines.append("\n---\n")

        # Content sections by topic
        for topic_idx, (topic, items) in enumerate(topic_groups.items(), 1):
            topic_anchor = self._create_anchor(topic)
            lines.append(f"## {topic_idx}. {topic}\n")

            # Topic summary if we have LLM
            if self.llm and len(items) > 2:
                topic_summary = await self._generate_topic_summary(topic, items)
                if topic_summary:
                    lines.append(f"*{topic_summary}*\n")

            # Items in this topic
            for item_idx, item in enumerate(items, 1):
                item_anchor = self._create_anchor(f"{topic}-{item_idx}-{item.title}")
                lines.append(f"### {topic_idx}.{item_idx}. {item.title}\n")

                # Metadata
                lines.append(f"**Source:** {item.url}")
                lines.append(f"**Relevance:** {item.relevance_score}/10")
                if item.content_type:
                    lines.append(f"**Type:** {item.content_type}")
                lines.append("")

                # Key points
                if item.key_points:
                    lines.append("**Key Points:**")
                    for point in item.key_points:
                        lines.append(f"- {point}")
                    lines.append("")

                # Summary
                if item.summary:
                    lines.append("**Summary:**")
                    lines.append(item.summary)
                    lines.append("")

                # Content
                lines.append("**Content:**")
                lines.append(item.content)
                lines.append("\n---\n")

        # Cross-references section
        if self.config.create_index:
            cross_refs = self._identify_cross_references(topic_groups)
            if cross_refs:
                lines.append("## Cross-References\n")
                lines.append("Topics that appear across multiple sources:\n")
                for term, locations in cross_refs.items():
                    if len(locations) > 1:
                        lines.append(
                            f"- **{term}**: appears in {len(locations)} sources"
                        )
                lines.append("\n")

        return "\n".join(lines)

    async def _generate_topic_summary(
        self, topic: str, items: List[AnalyzedContent]
    ) -> Optional[str]:
        """Generate a summary for a specific topic"""
        if not self.llm or not items:
            return None

        # Prepare item summaries
        summaries = [f"- {item.title}: {item.summary[:100]}..." for item in items[:5]]

        prompt = get_topic_summary_prompt(
            topic=topic, summaries=chr(10).join(summaries)
        )

        try:
            response = await self.llm.query(prompt)
            if not response.error:
                return response.content.strip()
        except Exception as e:
            logger.error(f"Error generating topic summary: {e}")

        return None

    def _create_anchor(self, text: str) -> str:
        """Create a valid markdown anchor from text"""
        # Remove special characters and convert to lowercase
        anchor = re.sub(r"[^\w\s-]", "", text.lower())
        # Replace spaces with hyphens
        anchor = re.sub(r"\s+", "-", anchor)
        # Remove duplicate hyphens
        anchor = re.sub(r"-+", "-", anchor)
        # Trim hyphens from ends
        return anchor.strip("-")

    def _identify_cross_references(
        self, topic_groups: Dict[str, List[AnalyzedContent]]
    ) -> Dict[str, List[str]]:
        """Identify terms that appear across multiple topics"""
        term_locations = defaultdict(set)

        # Extract key terms from each topic group
        for topic, items in topic_groups.items():
            for item in items:
                # Extract from key points
                for point in item.key_points:
                    # Simple term extraction (in production, use NLP)
                    terms = re.findall(r"\b[A-Z][a-z]+(?:\s+[A-Z][a-z]+)*\b", point)
                    for term in terms:
                        if len(term) > 3:  # Skip short terms
                            term_locations[term].add(topic)

        # Filter to terms appearing in multiple topics
        cross_refs = {
            term: list(locations)
            for term, locations in term_locations.items()
            if len(locations) > 1
        }

        return cross_refs

    async def _create_index_file(
        self, topic_groups: Dict[str, List[AnalyzedContent]], output_dir: Path
    ):
        """Create a separate index file for navigation"""
        index_path = output_dir / "index.md"

        lines = []
        lines.append("# Research Index\n")
        lines.append("## By Topic\n")

        for topic, items in topic_groups.items():
            lines.append(f"### {topic}")
            for item in items:
                lines.append(
                    f"- [{item.title}]({item.url}) (Relevance: {item.relevance_score}/10)"
                )
            lines.append("")

        lines.append("## By Relevance\n")
        all_items = []
        for items in topic_groups.values():
            all_items.extend(items)
        all_items.sort(key=lambda x: x.relevance_score, reverse=True)

        for item in all_items[:20]:  # Top 20
            lines.append(f"- {item.relevance_score}/10: [{item.title}]({item.url})")

        with safe_open(index_path, "w", encoding="utf-8") as f:
            if f:
                f.write("\n".join(lines))

    async def _create_metadata_file(
        self, content_list: List[AnalyzedContent], research_query: str, output_dir: Path
    ):
        """Create metadata JSON file"""
        metadata = {
            "query": research_query,
            "generated_at": datetime.now().isoformat(),
            "statistics": {
                "total_sources": len(content_list),
                "average_relevance": (
                    sum(item.relevance_score for item in content_list)
                    / len(content_list)
                    if content_list
                    else 0
                ),
                "content_types": dict(
                    Counter(item.content_type or "unknown" for item in content_list)
                ),
            },
            "sources": [
                {
                    "url": item.url,
                    "title": item.title,
                    "relevance_score": item.relevance_score,
                    "content_type": item.content_type,
                    "key_points": item.key_points,
                    "topics": item.analysis_metadata.get("topics", []),
                }
                for item in content_list
            ],
        }

        metadata_path = output_dir / "metadata.json"
        with safe_open(metadata_path, "w", encoding="utf-8") as f:
            if f:
                f.write(json.dumps(metadata, indent=2))

======= cli.py ======
# Copyright 2025 Franz und Franz GmbH
#
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#     http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.

"""
Enhanced CLI interface for m1f-research with improved UX
"""

import argparse
import sys
import os
from pathlib import Path
from typing import Optional, List, Dict, Any
import asyncio
import logging
from datetime import datetime

from m1f.file_operations import safe_exists

from .config import ResearchConfig
from .orchestrator import EnhancedResearchOrchestrator
from .output import OutputFormatter, ProgressTracker

# Use unified colorama module
try:
    from shared.colors import Colors, ColoredHelpFormatter, COLORAMA_AVAILABLE, info
except ImportError:
    # Fallback to local implementation
    from .output import Colors, COLORAMA_AVAILABLE

    def info(msg):
        print(msg)

    class ColoredHelpFormatter(argparse.RawDescriptionHelpFormatter):
        """Fallback help formatter with colors if available."""

        def _format_action_invocation(self, action: argparse.Action) -> str:
            """Format action with colors."""
            parts = super()._format_action_invocation(action)

            if COLORAMA_AVAILABLE:
                # Color the option names
                parts = parts.replace("-", f"{Colors.CYAN}-")
                parts = f"{parts}{Colors.RESET}"

            return parts

        def _format_usage(
            self, usage: str, actions, groups, prefix: Optional[str]
        ) -> str:
            """Format usage line with colors."""
            result = super()._format_usage(usage, actions, groups, prefix)

            if COLORAMA_AVAILABLE and result:
                # Highlight the program name
                prog_name = self._prog
                colored_prog = f"{Colors.GREEN}{prog_name}{Colors.RESET}"
                result = result.replace(prog_name, colored_prog, 1)

            return result


# Import version
try:
    from _version import __version__
except ImportError:
    __version__ = "3.8.0"


class EnhancedResearchCommand:
    """Enhanced CLI with better user experience"""

    def __init__(self):
        self.parser = self._create_parser()
        self.formatter: Optional[OutputFormatter] = None

    def _create_parser(self) -> argparse.ArgumentParser:
        """Create enhanced argument parser"""
        parser = argparse.ArgumentParser(
            prog="m1f-research",
            description="AI-powered research tool with advanced job management",
            formatter_class=ColoredHelpFormatter,
            epilog=f"""
{Colors.BOLD}Examples:{Colors.RESET}
  {Colors.CYAN}# Start new research{Colors.RESET}
  m1f-research "microservices best practices"
  
  {Colors.CYAN}# List jobs with filters{Colors.RESET}
  m1f-research --list-jobs --search "python" --limit 10
  
  {Colors.CYAN}# Resume with progress tracking{Colors.RESET}
  m1f-research --resume abc123 --verbose
  
  {Colors.CYAN}# JSON output for automation{Colors.RESET}
  m1f-research --list-jobs --format json | jq '.[] | select(.status=="completed")'
  
  {Colors.CYAN}# Clean up old data{Colors.RESET}
  m1f-research --clean-raw abc123
  
  {Colors.CYAN}# Delete a specific job{Colors.RESET}
  m1f-research --delete abc123
  
  {Colors.CYAN}# Delete multiple jobs{Colors.RESET}
  m1f-research --delete-bulk --status-filter failed
  m1f-research --delete-bulk --date 2025-01 --yes

{Colors.BOLD}For more help:{Colors.RESET}
  m1f-research --help-examples    # More usage examples
  m1f-research --help-filters     # Filtering guide
  m1f-research --help-providers   # LLM provider setup
""",
        )

        # Main query
        parser.add_argument(
            "query", nargs="?", help="Research query (required for new jobs)"
        )

        # Output format
        output_group = parser.add_argument_group("output options")
        output_group.add_argument(
            "--format",
            choices=["text", "json"],
            default="text",
            help="Output format (default: text)",
        )

        output_group.add_argument(
            "--quiet", "-q", action="store_true", help="Suppress non-error output"
        )

        output_group.add_argument(
            "--verbose",
            "-v",
            action="count",
            default=0,
            help="Increase verbosity (-vv for debug)",
        )

        output_group.add_argument(
            "--no-color", action="store_true", help="Disable colored output"
        )

        # Help extensions
        help_group = parser.add_argument_group("extended help")
        help_group.add_argument(
            "--help-examples", action="store_true", help="Show extended examples"
        )

        help_group.add_argument(
            "--help-filters", action="store_true", help="Show filtering guide"
        )

        help_group.add_argument(
            "--help-providers",
            action="store_true",
            help="Show LLM provider setup guide",
        )

        # Job management
        job_group = parser.add_argument_group("job management")
        job_group.add_argument(
            "--resume", metavar="JOB_ID", help="Resume an existing research job"
        )

        job_group.add_argument(
            "--list-jobs", action="store_true", help="List all research jobs"
        )

        job_group.add_argument(
            "--status", metavar="JOB_ID", help="Show detailed job status"
        )

        job_group.add_argument(
            "--watch", metavar="JOB_ID", help="Watch job progress in real-time"
        )

        job_group.add_argument(
            "--urls-file", type=Path, help="File containing URLs to add (one per line)"
        )

        # List filters
        filter_group = parser.add_argument_group("filtering options")
        filter_group.add_argument("--limit", type=int, help="Limit number of results")

        filter_group.add_argument(
            "--offset", type=int, default=0, help="Offset for pagination"
        )

        filter_group.add_argument("--date", help="Filter by date (Y-M-D, Y-M, or Y)")

        filter_group.add_argument("--search", help="Search jobs by query term")

        filter_group.add_argument(
            "--status-filter",
            choices=["active", "completed", "failed"],
            help="Filter by job status",
        )

        # Data management
        data_group = parser.add_argument_group("data management")
        data_group.add_argument(
            "--clean-raw", metavar="JOB_ID", help="Clean raw HTML data for a job"
        )

        data_group.add_argument(
            "--clean-all-raw",
            action="store_true",
            help="Clean raw HTML data for all jobs",
        )

        data_group.add_argument(
            "--export", metavar="JOB_ID", help="Export job data to JSON"
        )

        data_group.add_argument(
            "--delete", metavar="JOB_ID", help="Delete a research job completely"
        )

        data_group.add_argument(
            "--delete-bulk",
            action="store_true",
            help="Delete multiple jobs based on filters",
        )

        # Research options
        research_group = parser.add_argument_group("research options")
        research_group.add_argument(
            "--urls",
            type=int,
            default=20,
            help="Number of URLs to search for (default: 20)",
        )

        research_group.add_argument(
            "--scrape",
            type=int,
            default=10,
            help="Maximum URLs to scrape (default: 10)",
        )

        research_group.add_argument(
            "--provider",
            "-p",
            choices=["claude", "claude-code", "gemini", "gemini-cli"],
            default="claude",
            help="LLM provider to use",
        )

        research_group.add_argument("--model", "-m", help="Specific model to use")

        research_group.add_argument(
            "--template",
            "-t",
            choices=["general", "technical", "academic", "tutorial", "reference"],
            default="general",
            help="Analysis template",
        )

        # Query control options
        query_group = parser.add_argument_group("query options")
        query_group.add_argument(
            "--max-queries",
            type=int,
            default=5,
            help="Maximum number of query variations (1 = original only, default: 5)",
        )

        query_group.add_argument(
            "--custom-queries",
            nargs="+",
            help="Provide custom query variations (overrides auto-expansion)",
        )

        query_group.add_argument(
            "--interactive-queries",
            action="store_true",
            help="Interactively enter custom query variations",
        )

        # Behavior options
        behavior_group = parser.add_argument_group("behavior options")
        behavior_group.add_argument(
            "--output",
            "-o",
            type=Path,
            default=Path("./research-data"),
            help="Output directory",
        )

        behavior_group.add_argument(
            "--name", "-n", help="Custom name for research bundle"
        )

        behavior_group.add_argument(
            "--config", "-c", type=Path, help="Configuration file path"
        )

        behavior_group.add_argument(
            "--interactive", "-i", action="store_true", help="Start in interactive mode"
        )

        behavior_group.add_argument(
            "--no-filter", action="store_true", help="Disable content filtering"
        )

        behavior_group.add_argument(
            "--no-analysis", action="store_true", help="Skip AI analysis"
        )

        behavior_group.add_argument(
            "--concurrent", type=int, default=5, help="Max concurrent operations"
        )

        behavior_group.add_argument(
            "--dry-run", action="store_true", help="Preview without executing"
        )

        behavior_group.add_argument(
            "--yes", "-y", action="store_true", help="Answer yes to all prompts"
        )

        # Version
        parser.add_argument(
            "--version", action="version", version=f"%(prog)s {__version__}"
        )

        return parser

    def _validate_args(self, args) -> Optional[str]:
        """Validate arguments and return error message if invalid"""
        # Check for conflicting options
        if args.resume and args.query:
            return "Cannot specify both query and --resume"

        # Check required args for operations
        if not any(
            [
                args.query,
                args.resume,
                args.list_jobs,
                args.status,
                args.clean_raw,
                args.clean_all_raw,
                args.export,
                args.watch,
                args.delete,
                args.delete_bulk,
                args.help_examples,
                args.help_filters,
                args.help_providers,
                args.interactive,
            ]
        ):
            # Default to interactive mode when no operation specified
            args.interactive = True

        # Validate URLs file if provided
        if args.urls_file and not safe_exists(args.urls_file):
            return f"URLs file not found: {args.urls_file}"

        # Validate numeric ranges
        if args.urls < 0:
            return "--urls must be non-negative"

        if args.scrape < 0:
            return "--scrape must be non-negative"

        if args.concurrent < 1:
            return "--concurrent must be at least 1"

        if args.limit and args.limit < 1:
            return "--limit must be positive"

        if args.offset < 0:
            return "--offset must be non-negative"

        # Validate date format
        if args.date:
            if not self._validate_date_format(args.date):
                return f"Invalid date format: {args.date}. Use Y-M-D, Y-M, or Y"

        return None

    def _validate_date_format(self, date_str: str) -> bool:
        """Validate date format"""
        try:
            if len(date_str) == 10:  # Y-M-D
                datetime.strptime(date_str, "%Y-%m-%d")
            elif len(date_str) == 7:  # Y-M
                datetime.strptime(date_str, "%Y-%m")
            elif len(date_str) == 4:  # Y
                datetime.strptime(date_str, "%Y")
            else:
                return False
            return True
        except ValueError:
            return False

    async def run(self, args=None):
        """Run the CLI with enhanced output"""
        args = self.parser.parse_args(args)

        # Handle extended help
        if args.help_examples:
            self._show_examples()
            return 0

        if args.help_filters:
            self._show_filters_guide()
            return 0

        if args.help_providers:
            self._show_providers_guide()
            return 0

        # Setup formatter
        if args.no_color:
            Colors.disable()

        self.formatter = OutputFormatter(
            format=args.format, verbose=args.verbose, quiet=args.quiet
        )

        # Validate arguments
        error = self._validate_args(args)
        if error:
            self.formatter.error(error)
            return 1

        # Setup logging
        self._setup_logging(args)

        try:
            # Route to appropriate handler
            if args.list_jobs:
                return await self._list_jobs(args)
            elif args.status:
                return await self._show_status(args)
            elif args.watch:
                return await self._watch_job(args)
            elif args.clean_raw:
                return await self._clean_raw(args)
            elif args.clean_all_raw:
                return await self._clean_all_raw(args)
            elif args.export:
                return await self._export_job(args)
            elif args.delete:
                return await self._delete_job(args)
            elif args.delete_bulk:
                return await self._delete_bulk(args)
            elif args.interactive:
                return await self._interactive_mode(args)
            else:
                return await self._run_research(args)

        except KeyboardInterrupt:
            self.formatter.warning("Interrupted by user")
            return 130
        except Exception as e:
            self.formatter.error(str(e))
            if args.verbose > 0:
                import traceback

                traceback.print_exc()
            return 1
        finally:
            self.formatter.cleanup()

    def _setup_logging(self, args):
        """Setup logging based on verbosity"""
        if args.format == "json":
            # Suppress all logging in JSON mode
            logging.disable(logging.CRITICAL)
        else:
            level = logging.WARNING
            if args.verbose == 1:
                level = logging.INFO
            elif args.verbose >= 2:
                level = logging.DEBUG

            logging.basicConfig(
                level=level,
                format="%(asctime)s - %(name)s - %(levelname)s - %(message)s",
            )

    def _show_examples(self):
        """Show extended examples"""
        examples = """
# Research Workflows

## Basic Research
m1f-research "python async programming"

## Research with Custom Settings
m1f-research "react hooks" \\
  --urls 50 \\
  --scrape 25 \\
  --template technical \\
  --output ~/research

## Using Manual URLs
# Create URL list
cat > urls.txt << EOF
https://docs.python.org/3/library/asyncio.html
https://realpython.com/async-io-python/
EOF

# Use in research
m1f-research "python async" --urls-file urls.txt

## Query Control

# Use only the original query (no expansion)
m1f-research "Python dictionaries" --max-queries 1

# Limit expansion to 3 variations
m1f-research "machine learning" --max-queries 3

# Provide custom query variations
m1f-research "python" --custom-queries "Python tutorials" "Python best practices" "Python examples"

# Interactive query input
m1f-research "python" --interactive-queries
# Then enter queries line by line:
# 1> Python list comprehensions
# 2> Python dictionary methods
# 3> Python string formatting
# 4> [press Enter to finish]

## Job Management

# List recent jobs
m1f-research --list-jobs --limit 10

# Find specific research
m1f-research --list-jobs --search "react" --date 2025-07

# Resume interrupted job
m1f-research --resume abc123

# Add more URLs to existing job
m1f-research --resume abc123 --urls-file more-urls.txt

## Automation

# Export job data
m1f-research --export abc123 > job-data.json

# List completed jobs as JSON
m1f-research --list-jobs --status-filter completed --format json

# Batch processing
for topic in "react hooks" "vue composition" "angular signals"; do
  m1f-research "$topic" --quiet
done

## Data Management

# Delete specific job
m1f-research --delete abc123

# Delete all failed jobs
m1f-research --delete-bulk --status-filter failed

# Delete jobs from specific month
m1f-research --delete-bulk --date 2025-01

# Delete jobs matching search term (with confirmation)
m1f-research --delete-bulk --search "test"

# Force delete without confirmation
m1f-research --delete abc123 --yes

## Interactive Research
m1f-research --interactive
"""
        info(examples)

    def _show_filters_guide(self):
        """Show filtering guide"""
        guide = """
# Filtering Guide

## Date Filtering

Filter jobs by creation date:

  # Specific day
  m1f-research --list-jobs --date 2025-07-23
  
  # Specific month
  m1f-research --list-jobs --date 2025-07
  
  # Specific year
  m1f-research --list-jobs --date 2025

## Search Filtering

Find jobs by query content:

  # Simple search
  m1f-research --list-jobs --search "python"
  
  # Case-insensitive
  m1f-research --list-jobs --search "REACT"
  
  # Partial matches
  m1f-research --list-jobs --search "async"

## Status Filtering

Filter by job status:

  # Only completed jobs
  m1f-research --list-jobs --status-filter completed
  
  # Only failed jobs
  m1f-research --list-jobs --status-filter failed
  
  # Active jobs
  m1f-research --list-jobs --status-filter active

## Pagination

Handle large result sets:

  # First page (10 items)
  m1f-research --list-jobs --limit 10
  
  # Second page
  m1f-research --list-jobs --limit 10 --offset 10
  
  # Large page
  m1f-research --list-jobs --limit 50

## Combined Filters

Combine multiple filters:

  # Python jobs from July 2025
  m1f-research --list-jobs \\
    --search "python" \\
    --date 2025-07 \\
    --limit 20
  
  # Completed React jobs
  m1f-research --list-jobs \\
    --search "react" \\
    --status-filter completed \\
    --limit 10
"""
        info(guide)

    def _show_providers_guide(self):
        """Show providers setup guide"""
        guide = """
# LLM Provider Setup Guide

## Claude (Anthropic)

1. Get API key from https://console.anthropic.com/
2. Set environment variable:
   export ANTHROPIC_API_KEY="your-key-here"
3. Use in research:
   m1f-research "topic" --provider claude --model claude-3-opus-20240229

## Gemini (Google)

1. Get API key from https://makersuite.google.com/app/apikey
2. Set environment variable:
   export GOOGLE_API_KEY="your-key-here"
3. Use in research:
   m1f-research "topic" --provider gemini --model gemini-1.5-pro

## OpenAI

1. Get API key from https://platform.openai.com/api-keys
2. Set environment variable:
   export OPENAI_API_KEY="your-key-here"
3. Use in research:
   m1f-research "topic" --provider openai --model gpt-4

## CLI Providers

For enhanced integration:

# Claude Code SDK (uses proper Claude Code SDK integration)
m1f-research "topic" --provider claude-cli

# Gemini CLI (requires gemini-cli installed) 
m1f-research "topic" --provider gemini-cli

## Configuration File

Set default provider in .m1f.config.yml:

```yaml
research:
  llm:
    provider: claude
    model: claude-3-opus-20240229
```
"""
        info(guide)

    async def _run_research(self, args):
        """Run research with progress tracking"""
        config = self._create_config(args)
        orchestrator = EnhancedResearchOrchestrator(config)

        # Show research plan
        self.formatter.header(
            f"ðŸ” Research: {args.query or 'Resuming job'}",
            f"Provider: {config.llm.provider} | URLs: {config.scraping.search_limit} | Scrape: {config.scraping.scrape_limit}",
        )

        # Add progress callback
        def progress_callback(phase: str, current: int, total: int):
            if phase == "searching":
                self.formatter.info(f"Searching for URLs... ({current}/{total})")
            elif phase == "scraping":
                self.formatter.progress(current, total, "Scraping URLs")
            elif phase == "analyzing":
                self.formatter.progress(current, total, "Analyzing content")

        # Set callback if orchestrator supports it
        if hasattr(orchestrator, "set_progress_callback"):
            orchestrator.set_progress_callback(progress_callback)

        # Run research
        result = await orchestrator.research(
            query=args.query, job_id=args.resume, urls_file=args.urls_file
        )

        # Show results
        self.formatter.success("Research completed!")

        if self.formatter.format == "json":
            self.formatter._json_buffer.append(
                {
                    "type": "result",
                    "job_id": result.job_id,
                    "output_dir": str(result.output_dir),
                    "urls_found": result.urls_found,
                    "pages_scraped": len(result.scraped_content),
                    "pages_analyzed": len(result.analyzed_content),
                    "bundle_created": result.bundle_created,
                }
            )
        else:
            self.formatter.info(f"Job ID: {result.job_id}")
            self.formatter.info(f"Output: {result.output_dir}")
            self.formatter.list_item(f"URLs found: {result.urls_found}")
            self.formatter.list_item(f"Pages scraped: {len(result.scraped_content)}")
            self.formatter.list_item(f"Pages analyzed: {len(result.analyzed_content)}")

            if result.bundle_created:
                bundle_path = result.output_dir / "research_bundle.md"
                self.formatter.success(f"Research bundle: {bundle_path}")

        return 0

    async def _list_jobs(self, args):
        """List jobs with enhanced formatting"""
        from .job_manager import JobManager

        job_manager = JobManager(args.output)

        # Get total count
        total_count = job_manager.count_jobs(
            status=args.status_filter, date_filter=args.date, search_term=args.search
        )

        # Get jobs
        jobs = job_manager.list_jobs(
            status=args.status_filter,
            limit=args.limit,
            offset=args.offset,
            date_filter=args.date,
            search_term=args.search,
        )

        if not jobs:
            self.formatter.info("No jobs found matching criteria")
            return 0

        # Format for display
        if self.formatter.format == "json":
            self.formatter._json_buffer = jobs
        else:
            # Build filter description
            filters = []
            if args.search:
                filters.append(f"search: '{args.search}'")
            if args.date:
                filters.append(f"date: {args.date}")
            if args.status_filter:
                filters.append(f"status: {args.status_filter}")

            filter_str = f" (filtered by {', '.join(filters)})" if filters else ""

            # Show header
            if args.limit:
                page = (args.offset // args.limit) + 1 if args.limit else 1
                total_pages = (
                    (total_count + args.limit - 1) // args.limit if args.limit else 1
                )
                self.formatter.header(
                    f"ðŸ“‹ Research Jobs - Page {page}/{total_pages}",
                    f"Showing {len(jobs)} of {total_count}{filter_str}",
                )
            else:
                self.formatter.header(
                    f"ðŸ“‹ Research Jobs ({total_count} total{filter_str})"
                )

            # Prepare table data
            headers = ["ID", "Status", "Query", "Created", "Stats"]
            rows = []

            for job in jobs:
                stats = job["stats"]
                stats_str = f"{stats['scraped_urls']}/{stats['total_urls']}"
                if stats["analyzed_urls"]:
                    stats_str += f" ({stats['analyzed_urls']})"

                # Format status with color
                status = job["status"]
                if status == "completed":
                    status_display = f"{Colors.GREEN}{status}{Colors.RESET}"
                elif status == "active":
                    status_display = f"{Colors.YELLOW}{status}{Colors.RESET}"
                else:
                    status_display = f"{Colors.RED}{status}{Colors.RESET}"

                rows.append(
                    [
                        job["job_id"][:8],
                        status_display,
                        job["query"][:40],
                        job["created_at"][:16],
                        stats_str,
                    ]
                )

            self.formatter.table(headers, rows, highlight_search=args.search)

            # Pagination hints
            if args.limit and total_count > args.limit:
                self.formatter.info("")
                if args.offset + args.limit < total_count:
                    next_offset = args.offset + args.limit
                    self.formatter.info(f"Next page: --offset {next_offset}")
                if args.offset > 0:
                    prev_offset = max(0, args.offset - args.limit)
                    self.formatter.info(f"Previous page: --offset {prev_offset}")

        return 0

    async def _show_status(self, args):
        """Show job status with enhanced formatting"""
        from .job_manager import JobManager

        job_manager = JobManager(args.output)

        job = job_manager.get_job(args.status)
        if not job:
            self.formatter.error(f"Job not found: {args.status}")
            return 1

        info = job_manager.get_job_info(job)

        if self.formatter.format == "json":
            self.formatter._json_buffer.append(info)
        else:
            self.formatter.job_status(info)

        return 0

    async def _clean_raw(self, args):
        """Clean raw data with confirmation"""
        from .job_manager import JobManager

        job_manager = JobManager(args.output)

        if not args.yes:
            if not self.formatter.confirm(f"Clean raw data for job {args.clean_raw}?"):
                self.formatter.info("Cancelled")
                return 0

        self.formatter.info(f"Cleaning raw data for job {args.clean_raw}...")

        stats = await job_manager.cleanup_job_raw_data(args.clean_raw)

        if "error" in stats:
            self.formatter.error(stats["error"])
            return 1

        self.formatter.success(
            f"Cleaned {stats.get('html_files_deleted', 0)} files, "
            f"freed {stats.get('space_freed_mb', 0)} MB"
        )

        return 0

    async def _clean_all_raw(self, args):
        """Clean all raw data with confirmation"""
        from .job_manager import JobManager

        job_manager = JobManager(args.output)

        if not args.yes:
            if not self.formatter.confirm(
                "âš ï¸  This will delete ALL raw HTML data. Continue?", default=False
            ):
                self.formatter.info("Cancelled")
                return 0

        self.formatter.info("Cleaning raw data for all jobs...")

        # Show progress
        all_jobs = job_manager.list_jobs()
        progress = ProgressTracker(self.formatter, len(all_jobs), "Cleaning jobs")

        stats = {"jobs_cleaned": 0, "files_deleted": 0, "space_freed_mb": 0}

        for i, job_info in enumerate(all_jobs):
            try:
                job_stats = await job_manager.cleanup_job_raw_data(job_info["job_id"])
                if "error" not in job_stats:
                    stats["jobs_cleaned"] += 1
                    stats["files_deleted"] += job_stats.get("html_files_deleted", 0)
                    stats["space_freed_mb"] += job_stats.get("space_freed_mb", 0)
            except Exception as e:
                self.formatter.debug(f"Error cleaning {job_info['job_id']}: {e}")

            progress.update()

        progress.complete("Cleanup complete")

        self.formatter.success(
            f"Cleaned {stats['jobs_cleaned']} jobs, "
            f"{stats['files_deleted']} files, "
            f"freed {stats['space_freed_mb']:.1f} MB"
        )

        return 0

    async def _export_job(self, args):
        """Export job data to JSON"""
        from .job_manager import JobManager

        job_manager = JobManager(args.output)

        job = job_manager.get_job(args.export)
        if not job:
            self.formatter.error(f"Job not found: {args.export}")
            return 1

        info = job_manager.get_job_info(job)

        # Add content if available
        job_db = job_manager.get_job_database(job)
        content = job_db.get_content_for_bundle()
        info["content"] = content

        # Output as JSON
        import json

        print(json.dumps(info, indent=2, default=str))

        return 0

    async def _watch_job(self, args):
        """Watch job progress in real-time"""
        from .job_manager import JobManager
        import time

        job_manager = JobManager(args.output)

        self.formatter.info(f"Watching job {args.watch}... (Ctrl+C to stop)")

        last_stats = None
        while True:
            try:
                job = job_manager.get_job(args.watch)
                if not job:
                    self.formatter.error(f"Job not found: {args.watch}")
                    return 1

                info = job_manager.get_job_info(job)
                stats = info["stats"]

                # Check if stats changed
                if stats != last_stats:
                    # Clear screen and show status
                    os.system("clear" if os.name == "posix" else "cls")
                    self.formatter.job_status(info)
                    last_stats = stats

                # Check if job is complete
                if info["status"] in ["completed", "failed"]:
                    break

                # Wait before next check
                await asyncio.sleep(2)

            except KeyboardInterrupt:
                break

        return 0

    async def _delete_job(self, args):
        """Delete a specific job with confirmation"""
        from .job_manager import JobManager

        job_manager = JobManager(args.output)

        # Get job details for confirmation
        job = job_manager.get_job(args.delete)
        if not job:
            self.formatter.error(f"Job not found: {args.delete}")
            return 1

        # Show job details and confirm
        if not args.yes:
            self.formatter.header(f"Job to delete: {args.delete}")
            self.formatter.info(f"Query: {job.query}")
            self.formatter.info(f"Status: {job.status}")
            self.formatter.info(f"Created: {job.created_at}")
            self.formatter.info(f"Output: {job.output_dir}")

            if not self.formatter.confirm(
                f"\nâš ï¸  Delete job {args.delete} and all its data?", default=False
            ):
                self.formatter.info("Cancelled")
                return 0

        # Delete the job
        self.formatter.info(f"Deleting job {args.delete}...")

        result = await job_manager.delete_job(args.delete)

        if result.get("error"):
            self.formatter.error(result["error"])
            return 1

        if result.get("deleted"):
            self.formatter.success(
                f"Successfully deleted job {args.delete} ({result.get('query', 'Unknown query')})"
            )
            if result.get("errors"):
                for error in result["errors"]:
                    self.formatter.warning(f"Warning: {error}")
        else:
            self.formatter.error(f"Failed to delete job {args.delete}")
            if result.get("errors"):
                for error in result["errors"]:
                    self.formatter.error(f"Error: {error}")
            return 1

        return 0

    async def _delete_bulk(self, args):
        """Delete multiple jobs with confirmation"""
        from .job_manager import JobManager

        job_manager = JobManager(args.output)

        # Get jobs that match the filters
        jobs_to_delete = job_manager.list_jobs(
            status=args.status_filter, date_filter=args.date, search_term=args.search
        )

        if not jobs_to_delete:
            self.formatter.info("No jobs found matching the criteria")
            return 0

        # Show jobs and confirm
        if not args.yes:
            self.formatter.header(f"Jobs to delete ({len(jobs_to_delete)} total)")

            # Show first 10 jobs as preview
            preview_count = min(10, len(jobs_to_delete))
            for i, job in enumerate(jobs_to_delete[:preview_count], 1):
                self.formatter.info(
                    f"{i}. [{job['job_id']}] {job['query'][:50]} ({job['status']})"
                )

            if len(jobs_to_delete) > preview_count:
                self.formatter.info(
                    f"... and {len(jobs_to_delete) - preview_count} more"
                )

            # Build filter description
            filters = []
            if args.search:
                filters.append(f"search='{args.search}'")
            if args.date:
                filters.append(f"date={args.date}")
            if args.status_filter:
                filters.append(f"status={args.status_filter}")

            filter_desc = f" with filters: {', '.join(filters)}" if filters else ""

            if not self.formatter.confirm(
                f"\nâš ï¸  Delete {len(jobs_to_delete)} jobs{filter_desc}?", default=False
            ):
                self.formatter.info("Cancelled")
                return 0

        # Delete the jobs with progress tracking
        self.formatter.info(f"Deleting {len(jobs_to_delete)} jobs...")

        # Show progress
        progress = ProgressTracker(self.formatter, len(jobs_to_delete), "Deleting jobs")

        # Perform deletion
        result = await job_manager.delete_jobs(
            status=args.status_filter, date_filter=args.date, search_term=args.search
        )

        progress.complete("Deletion complete")

        # Show results
        if result["successfully_deleted"] > 0:
            self.formatter.success(
                f"Successfully deleted {result['successfully_deleted']} jobs"
            )

        if result["failed_deletions"] > 0:
            self.formatter.error(f"Failed to delete {result['failed_deletions']} jobs")
            for failed_job in result["failed_jobs"]:
                self.formatter.error(
                    f"  - {failed_job['job_id']}: {', '.join(failed_job.get('errors', ['Unknown error']))}"
                )

        if result.get("errors"):
            for error in result["errors"]:
                self.formatter.warning(f"Warning: {error}")

        return 0 if result["failed_deletions"] == 0 else 1

    async def _interactive_mode(self, args):
        """Run in interactive mode"""
        self.formatter.header("ðŸ” m1f-research Interactive Mode")
        self.formatter.info("Type 'help' for commands, 'exit' to quit\n")

        while True:
            try:
                command = input(f"{Colors.CYAN}research> {Colors.RESET}").strip()

                if not command:
                    continue

                if command.lower() in ["exit", "quit"]:
                    break

                if command.lower() == "help":
                    self._show_interactive_help()
                    continue

                # Parse command
                parts = command.split()
                if parts[0] == "research":
                    # New research
                    query = " ".join(parts[1:])
                    # Create new args with the query, avoiding duplicate 'query' key
                    research_args = vars(args).copy()
                    research_args["query"] = query
                    research_args["resume"] = None
                    research_args["urls_file"] = None
                    await self._run_research(argparse.Namespace(**research_args))
                elif parts[0] == "list":
                    # List jobs
                    await self._list_jobs(args)
                elif parts[0] == "status" and len(parts) > 1:
                    # Show status
                    args.status = parts[1]
                    await self._show_status(args)
                elif parts[0] == "resume" and len(parts) > 1:
                    # Resume job
                    args.resume = parts[1]
                    args.query = None
                    await self._run_research(args)
                else:
                    self.formatter.warning(f"Unknown command: {command}")
                    self.formatter.info("Type 'help' for available commands")

            except KeyboardInterrupt:
                info("")  # New line after ^C
                continue
            except EOFError:
                break

        self.formatter.info("\nðŸ‘‹ Goodbye!")
        return 0

    def _show_interactive_help(self):
        """Show interactive mode help"""
        help_text = """
Available commands:
  research <query>     Start new research
  list                 List all jobs
  status <job_id>      Show job status
  resume <job_id>      Resume a job
  help                 Show this help
  exit/quit           Exit interactive mode

Examples:
  research python async programming
  list
  status abc123
  resume abc123
"""
        info(help_text)

    def _create_config(self, args) -> ResearchConfig:
        """Create configuration from arguments"""
        # Load base config
        if args.config and safe_exists(args.config):
            config = ResearchConfig.from_yaml(args.config)
        else:
            config = ResearchConfig()

        # Apply CLI overrides
        config.llm.provider = args.provider
        if args.model:
            config.llm.model = args.model

        # Set scraping config properly
        config.url_count = args.urls
        config.scrape_count = args.scrape
        config.scraping.max_concurrent = args.concurrent
        config.scraping.search_limit = args.urls  # Add for compatibility
        config.scraping.scrape_limit = args.scrape  # Add for compatibility

        config.output.directory = args.output
        if args.name:
            config.output.name = args.name

        # Set template in analysis config
        config.template = args.template

        config.interactive = args.interactive
        config.no_filter = args.no_filter
        config.no_analysis = args.no_analysis
        config.dry_run = args.dry_run
        config.verbose = args.verbose

        return config


def main():
    """Main entry point"""
    cli = EnhancedResearchCommand()
    sys.exit(asyncio.run(cli.run()))


if __name__ == "__main__":
    main()

======= config.py ======
# Copyright 2025 Franz und Franz GmbH
#
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#     http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.

"""
Configuration management for m1f-research
"""

from dataclasses import dataclass, field
from typing import Dict, List, Optional, Any
from pathlib import Path
import yaml
import os
from argparse import Namespace

from m1f.file_operations import (
    safe_read_text,
)


@dataclass
class LLMConfig:
    """LLM provider configuration"""

    provider: str = "claude"
    model: Optional[str] = None
    api_key_env: Optional[str] = None
    temperature: float = 0.7
    max_tokens: int = 4096
    cli_command: Optional[str] = None  # For CLI providers
    cli_args: List[str] = field(default_factory=list)


@dataclass
class ScrapingConfig:
    """Web scraping configuration"""

    search_limit: int = 20  # Number of URLs to search for
    scrape_limit: int = 10  # Maximum URLs to scrape
    timeout_range: str = "1-3"  # seconds
    timeout: int = 30  # Total timeout for requests in seconds
    delay: List[float] = field(
        default_factory=lambda: [1.0, 3.0]
    )  # delay range in seconds
    max_concurrent: int = 5
    retry_attempts: int = 2
    retries: int = 2  # Number of retries for failed requests
    user_agents: List[str] = field(
        default_factory=lambda: [
            "Mozilla/5.0 (m1f-research/0.1.0) AppleWebKit/537.36",
            "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36",
        ]
    )
    respect_robots_txt: bool = True
    headers: Dict[str, str] = field(default_factory=dict)


@dataclass
class OutputConfig:
    """Output configuration"""

    directory: Path = Path("./m1f/research")
    create_summary: bool = True
    create_index: bool = True
    bundle_prefix: str = "research"
    format: str = "markdown"
    include_metadata: bool = True


@dataclass
class AnalysisConfig:
    """Content analysis configuration"""

    relevance_threshold: float = 7.0
    duplicate_threshold: float = 0.8
    min_content_length: int = 100
    max_content_length: Optional[int] = None
    prefer_code_examples: bool = False
    prioritize_recent: bool = True
    language: str = "en"


@dataclass
class WorkflowConfig:
    """Workflow configuration for phased research"""

    expand_queries: bool = True  # Generate search query variations
    max_queries: int = 5  # Maximum expanded queries
    skip_review: bool = False  # Skip URL review interface
    crawl_depth: int = 0  # How many levels deep to crawl
    max_pages_per_site: int = 10  # Maximum pages per domain
    follow_external: bool = False  # Follow external links
    generate_analysis: bool = True  # Generate AI analysis
    analysis_type: str = "summary"  # Type of analysis to generate


@dataclass
class ResearchTemplate:
    """Research template configuration"""

    name: str
    description: str
    sources: List[str] = field(default_factory=list)
    analysis_focus: str = "general"
    url_count: int = 20
    scrape_count: int = 10
    analysis_config: Optional[AnalysisConfig] = None


@dataclass
class ResearchConfig:
    """Main research configuration"""

    # Core settings
    query: Optional[str] = None
    url_count: int = 20
    scrape_count: int = 10

    # Query control
    custom_queries: Optional[List[str]] = None
    interactive_queries: bool = False

    # Component configs
    llm: LLMConfig = field(default_factory=LLMConfig)
    scraping: ScrapingConfig = field(default_factory=ScrapingConfig)
    output: OutputConfig = field(default_factory=OutputConfig)
    analysis: AnalysisConfig = field(default_factory=AnalysisConfig)
    filtering: AnalysisConfig = field(
        default_factory=AnalysisConfig
    )  # For content filtering
    workflow: WorkflowConfig = field(default_factory=WorkflowConfig)  # Workflow phases

    # Behavior settings
    interactive: bool = False
    no_filter: bool = False
    no_analysis: bool = False
    dry_run: bool = False
    verbose: int = 0

    # Templates
    template: str = "general"
    templates: Dict[str, ResearchTemplate] = field(default_factory=dict)

    @classmethod
    def from_yaml(cls, path: Path) -> "ResearchConfig":
        """Load configuration from YAML file"""
        content = safe_read_text(path)
        data = yaml.safe_load(content)

        # Extract research section
        research_data = data.get("research", {})

        # Parse LLM config
        llm_data = research_data.get("llm", {})
        llm_config = LLMConfig(
            provider=llm_data.get("provider", "claude"),
            model=llm_data.get("model"),
            api_key_env=llm_data.get("api_key_env"),
            temperature=llm_data.get("temperature", 0.7),
            max_tokens=llm_data.get("max_tokens", 4096),
            cli_command=llm_data.get("cli_command"),
            cli_args=llm_data.get("cli_args", []),
        )

        # Parse CLI tools config
        if "cli_tools" in research_data:
            cli_tools = research_data["cli_tools"]
            if llm_config.provider in cli_tools:
                tool_config = cli_tools[llm_config.provider]
                llm_config.cli_command = tool_config.get("command", llm_config.provider)
                llm_config.cli_args = tool_config.get("args", [])

        # Parse scraping config
        scraping_data = research_data.get("scraping", {})
        scraping_config = ScrapingConfig(
            timeout_range=scraping_data.get("timeout_range", "1-3"),
            max_concurrent=scraping_data.get("max_concurrent", 5),
            retry_attempts=scraping_data.get("retry_attempts", 2),
            user_agents=scraping_data.get("user_agents", ScrapingConfig().user_agents),
            respect_robots_txt=scraping_data.get("respect_robots_txt", True),
            headers=scraping_data.get("headers", {}),
        )

        # Parse output config
        output_data = research_data.get("output", {})
        output_config = OutputConfig(
            directory=Path(output_data.get("directory", "./research-data")),
            create_summary=output_data.get("create_summary", True),
            create_index=output_data.get("create_index", True),
            bundle_prefix=output_data.get("bundle_prefix", "research"),
            format=output_data.get("format", "markdown"),
            include_metadata=output_data.get("include_metadata", True),
        )

        # Parse analysis config
        analysis_data = research_data.get("analysis", {})
        analysis_config = AnalysisConfig(
            relevance_threshold=analysis_data.get("relevance_threshold", 7.0),
            duplicate_threshold=analysis_data.get("duplicate_threshold", 0.8),
            min_content_length=analysis_data.get("min_content_length", 100),
            max_content_length=analysis_data.get("max_content_length"),
            prefer_code_examples=analysis_data.get("prefer_code_examples", False),
            prioritize_recent=analysis_data.get("prioritize_recent", True),
            language=analysis_data.get("language", "en"),
        )

        # Parse workflow config
        workflow_data = research_data.get("workflow", {})
        workflow_config = WorkflowConfig(
            expand_queries=workflow_data.get("expand_queries", True),
            max_queries=workflow_data.get("max_queries", 5),
            skip_review=workflow_data.get("skip_review", False),
            crawl_depth=workflow_data.get("crawl_depth", 0),
            max_pages_per_site=workflow_data.get("max_pages_per_site", 10),
            follow_external=workflow_data.get("follow_external", False),
            generate_analysis=workflow_data.get("generate_analysis", True),
            analysis_type=workflow_data.get("analysis_type", "summary"),
        )

        # Parse templates
        templates = {}
        templates_data = research_data.get("templates", {})
        for name, template_data in templates_data.items():
            # Create template-specific analysis config if provided
            template_analysis = None
            if "analysis" in template_data:
                ta = template_data["analysis"]
                template_analysis = AnalysisConfig(
                    relevance_threshold=ta.get(
                        "relevance_threshold", analysis_config.relevance_threshold
                    ),
                    duplicate_threshold=ta.get(
                        "duplicate_threshold", analysis_config.duplicate_threshold
                    ),
                    min_content_length=ta.get(
                        "min_content_length", analysis_config.min_content_length
                    ),
                    max_content_length=ta.get(
                        "max_content_length", analysis_config.max_content_length
                    ),
                    prefer_code_examples=ta.get(
                        "prefer_code_examples", analysis_config.prefer_code_examples
                    ),
                    prioritize_recent=ta.get(
                        "prioritize_recent", analysis_config.prioritize_recent
                    ),
                    language=ta.get("language", analysis_config.language),
                )

            templates[name] = ResearchTemplate(
                name=name,
                description=template_data.get("description", ""),
                sources=template_data.get("sources", ["web"]),
                analysis_focus=template_data.get("analysis_focus", "general"),
                url_count=template_data.get("url_count", 20),
                scrape_count=template_data.get("scrape_count", 10),
                analysis_config=template_analysis,
            )

        # Get defaults
        defaults = research_data.get("defaults", {})

        return cls(
            url_count=defaults.get("url_count", 20),
            scrape_count=defaults.get("scrape_count", 10),
            llm=llm_config,
            scraping=scraping_config,
            output=output_config,
            analysis=analysis_config,
            workflow=workflow_config,
            templates=templates,
        )

    @classmethod
    def from_args(cls, args: Namespace) -> "ResearchConfig":
        """Create configuration from command line arguments"""
        config = cls()

        # Basic settings
        config.query = args.query
        config.url_count = args.urls
        config.scrape_count = args.scrape
        config.interactive = args.interactive
        config.no_filter = args.no_filter
        config.no_analysis = args.no_analysis
        config.dry_run = args.dry_run
        config.verbose = args.verbose
        config.template = args.template

        # Query control settings
        config.custom_queries = (
            args.custom_queries if hasattr(args, "custom_queries") else None
        )
        config.interactive_queries = (
            args.interactive_queries if hasattr(args, "interactive_queries") else False
        )

        # Load from config file if provided
        if args.config:
            base_config = cls.from_yaml(args.config)
            # Merge with base config
            config.llm = base_config.llm
            config.workflow = base_config.workflow
            config.scraping = base_config.scraping
            config.output = base_config.output
            config.analysis = base_config.analysis
            config.templates = base_config.templates

        # Override with command line args
        config.llm.provider = args.provider
        if args.model:
            config.llm.model = args.model

        # Override workflow settings from command line
        if hasattr(args, "max_queries"):
            config.workflow.max_queries = args.max_queries

        # Output settings
        config.output.directory = args.output
        if args.name:
            config.output.bundle_prefix = args.name

        # Scraping settings
        config.scraping.max_concurrent = args.concurrent
        config.scraping.search_limit = (
            config.url_count
        )  # Set search limit from url_count
        config.scraping.scrape_limit = (
            config.scrape_count
        )  # Set scrape limit from scrape_count

        # Apply template if specified
        if config.template and config.template in config.templates:
            template = config.templates[config.template]
            config.url_count = template.url_count
            config.scrape_count = template.scrape_count
            if template.analysis_config:
                config.analysis = template.analysis_config

        # Set API key from environment if not set
        if not config.llm.api_key_env:
            if config.llm.provider == "claude":
                config.llm.api_key_env = "ANTHROPIC_API_KEY"
            elif config.llm.provider == "gemini":
                config.llm.api_key_env = "GOOGLE_API_KEY"

        return config

    def to_yaml(self) -> str:
        """Convert configuration to YAML string"""
        data = {
            "research": {
                "defaults": {
                    "url_count": self.url_count,
                    "scrape_count": self.scrape_count,
                },
                "llm": {
                    "provider": self.llm.provider,
                    "model": self.llm.model,
                    "api_key_env": self.llm.api_key_env,
                    "temperature": self.llm.temperature,
                    "max_tokens": self.llm.max_tokens,
                },
                "scraping": {
                    "timeout_range": self.scraping.timeout_range,
                    "max_concurrent": self.scraping.max_concurrent,
                    "retry_attempts": self.scraping.retry_attempts,
                    "user_agents": self.scraping.user_agents,
                    "respect_robots_txt": self.scraping.respect_robots_txt,
                },
                "output": {
                    "directory": str(self.output.directory),
                    "create_summary": self.output.create_summary,
                    "create_index": self.output.create_index,
                    "bundle_prefix": self.output.bundle_prefix,
                    "format": self.output.format,
                },
                "analysis": {
                    "relevance_threshold": self.analysis.relevance_threshold,
                    "duplicate_threshold": self.analysis.duplicate_threshold,
                    "min_content_length": self.analysis.min_content_length,
                    "max_content_length": self.analysis.max_content_length,
                    "prefer_code_examples": self.analysis.prefer_code_examples,
                    "prioritize_recent": self.analysis.prioritize_recent,
                    "language": self.analysis.language,
                },
                "templates": {
                    name: {
                        "description": template.description,
                        "sources": template.sources,
                        "analysis_focus": template.analysis_focus,
                        "url_count": template.url_count,
                        "scrape_count": template.scrape_count,
                    }
                    for name, template in self.templates.items()
                },
            }
        }

        return yaml.dump(data, default_flow_style=False, sort_keys=False)

    def get_timeout_range(self) -> tuple[float, float]:
        """Parse timeout range string to min/max values"""
        parts = self.scraping.timeout_range.split("-")
        if len(parts) == 2:
            return float(parts[0]), float(parts[1])
        else:
            val = float(parts[0])
            return val, val

======= content_filter.py ======
# Copyright 2025 Franz und Franz GmbH
#
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#     http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.

"""
Content filtering and quality assessment for m1f-research
"""
import re
import hashlib
from typing import List, Dict, Optional, Tuple
from collections import Counter
import logging

from .models import ScrapedContent, AnalyzedContent
from .config import AnalysisConfig

logger = logging.getLogger(__name__)


class ContentFilter:
    """
    Advanced content filtering with:
    - Content length validation
    - Language detection
    - Spam/ad detection
    - Code-to-text ratio analysis
    - Duplicate detection
    - Quality scoring
    """

    def __init__(self, config: AnalysisConfig):
        self.config = config
        self.seen_hashes = set()
        self.spam_patterns = self._load_spam_patterns()
        self.quality_indicators = self._load_quality_indicators()

    def filter_scraped_content(
        self, content_list: List[ScrapedContent]
    ) -> List[ScrapedContent]:
        """Filter scraped content based on quality criteria"""
        filtered = []

        for content in content_list:
            # Check if content passes all filters
            if self._passes_filters(content):
                filtered.append(content)
            else:
                logger.debug(f"Filtered out: {content.url}")

        logger.info(f"Filtered {len(content_list)} to {len(filtered)} items")
        return filtered

    def filter_analyzed_content(
        self, content_list: List[AnalyzedContent]
    ) -> List[AnalyzedContent]:
        """Filter analyzed content based on relevance and quality"""
        filtered = []

        for content in content_list:
            # Check relevance threshold
            if content.relevance_score < self.config.relevance_threshold:
                logger.debug(
                    f"Below relevance threshold: {content.url} ({content.relevance_score})"
                )
                continue

            # Check content length
            if not self._check_content_length(content.content):
                continue

            # Check for duplicates
            if self._is_duplicate(content.content):
                logger.debug(f"Duplicate content: {content.url}")
                continue

            filtered.append(content)

        return filtered

    def filter_content(self, content: str) -> Tuple[bool, str]:
        """
        Filter a single content item and return pass/fail with reason.

        Args:
            content: Content string to check

        Returns:
            Tuple of (passed: bool, reason: str)
        """
        # Check content length
        if not self._check_content_length(content):
            return (
                False,
                f"Content too short (min: {self.config.min_content_length} chars)",
            )

        # Check for spam/ads
        if self._is_spam(content):
            return False, "Spam/advertising content detected"

        # Check quality score
        quality_score = self._calculate_quality_score(content)
        if quality_score < 0.3:  # Minimum quality threshold
            return False, f"Low quality content (score: {quality_score:.2f})"

        # Check for duplicates
        if self._is_duplicate(content):
            return False, "Duplicate content"

        return True, "Passed all filters"

    def _passes_filters(self, content: ScrapedContent) -> bool:
        """Check if content passes all quality filters"""
        # Check content length
        if not self._check_content_length(content.content):
            return False

        # Check language (if configured)
        if self.config.language != "any":
            detected_lang = self._detect_language(content.content)
            if detected_lang != self.config.language:
                logger.debug(
                    f"Wrong language: {content.url} (detected: {detected_lang})"
                )
                return False

        # Check for spam/ads
        if self._is_spam(content.content):
            logger.debug(f"Spam detected: {content.url}")
            return False

        # Check quality score
        quality_score = self._calculate_quality_score(content.content)
        logger.debug(f"Quality score for {content.url}: {quality_score:.2f}")
        if quality_score < 0.3:  # Minimum quality threshold
            logger.debug(f"Low quality: {content.url} (score: {quality_score:.2f})")
            return False

        # Check for duplicates
        if self._is_duplicate(content.content):
            logger.debug(f"Duplicate: {content.url}")
            return False

        return True

    def _check_content_length(self, content: str) -> bool:
        """Check if content length is within acceptable range"""
        length = len(content)

        if length < self.config.min_content_length:
            return False

        if self.config.max_content_length and length > self.config.max_content_length:
            return False

        return True

    def _detect_language(self, content: str) -> str:
        """Simple language detection based on common words"""
        # This is a simplified implementation
        # In production, use langdetect or similar library

        english_words = {"the", "and", "is", "in", "to", "of", "a", "for", "with", "on"}
        spanish_words = {"el", "la", "de", "en", "y", "a", "los", "las", "un", "una"}
        french_words = {"le", "de", "la", "et", "Ã ", "les", "un", "une", "dans", "pour"}
        german_words = {
            "der",
            "die",
            "das",
            "und",
            "in",
            "von",
            "zu",
            "mit",
            "den",
            "ein",
        }

        # Extract words
        words = re.findall(r"\b\w+\b", content.lower())[:200]  # Check first 200 words
        word_set = set(words)

        # Count matches
        scores = {
            "en": len(word_set.intersection(english_words)),
            "es": len(word_set.intersection(spanish_words)),
            "fr": len(word_set.intersection(french_words)),
            "de": len(word_set.intersection(german_words)),
        }

        # Return language with highest score
        if max(scores.values()) > 0:
            return max(scores, key=scores.get)

        return "unknown"

    def _is_spam(self, content: str) -> bool:
        """Detect spam/ad content using patterns and heuristics"""
        content_lower = content.lower()

        # Check spam patterns
        spam_score = 0
        for pattern in self.spam_patterns:
            if pattern in content_lower:
                spam_score += 1

        # Check for excessive links
        links = re.findall(r"https?://[^\s]+", content)
        if len(links) > 20:  # Too many links
            spam_score += 2

        # Check for excessive capitalization
        if len(re.findall(r"[A-Z]{5,}", content)) > 10:
            spam_score += 1

        # Check for repeated phrases
        phrases = re.findall(r"\b\w+\s+\w+\s+\w+\b", content_lower)
        phrase_counts = Counter(phrases)
        if any(count >= 5 for count in phrase_counts.values()):
            spam_score += 1

        # Check for common spam indicators
        spam_indicators = [
            r"click here now",
            r"limited time offer",
            r"act now",
            r"100% free",
            r"no credit card",
            r"make money fast",
            r"work from home",
            r"congratulations you",
            r"you have been selected",
        ]

        for indicator in spam_indicators:
            if re.search(indicator, content_lower):
                spam_score += 2

        return spam_score >= 3

    def _calculate_quality_score(self, content: str) -> float:
        """Calculate overall quality score (0-1)"""
        scores = []

        # Content structure score
        structure_score = self._score_structure(content)
        scores.append(structure_score)

        # Readability score
        readability_score = self._score_readability(content)
        scores.append(readability_score)

        # Information density score
        density_score = self._score_information_density(content)
        scores.append(density_score)

        # Code quality score (for technical content)
        if self.config.prefer_code_examples:
            code_score = self._score_code_content(content)
            scores.append(code_score)

        return sum(scores) / len(scores)

    def _score_structure(self, content: str) -> float:
        """Score content structure (headings, paragraphs, lists)"""
        score = 0.5  # Base score

        # Check for headings
        headings = re.findall(r"^#{1,6}\s+.+", content, re.MULTILINE)
        if headings:
            score += min(len(headings) * 0.05, 0.2)

        # Check for lists
        lists = re.findall(r"^[\*\-]\s+.+", content, re.MULTILINE)
        if lists:
            score += min(len(lists) * 0.02, 0.1)

        # Check for code blocks
        code_blocks = re.findall(r"```[\s\S]*?```", content)
        if code_blocks:
            score += min(len(code_blocks) * 0.05, 0.2)

        return min(score, 1.0)

    def _score_readability(self, content: str) -> float:
        """Score content readability"""
        # Simple readability metrics
        sentences = re.split(r"[.!?]+", content)
        words = re.findall(r"\b\w+\b", content)

        if not sentences or not words:
            return 0.0

        # Average sentence length
        avg_sentence_length = len(words) / len(sentences)

        # Ideal range is 15-25 words per sentence
        if 15 <= avg_sentence_length <= 25:
            sentence_score = 1.0
        elif 10 <= avg_sentence_length <= 30:
            sentence_score = 0.7
        else:
            sentence_score = 0.4

        # Check for paragraph breaks
        paragraphs = re.split(r"\n\n+", content)
        if len(paragraphs) > 3:
            paragraph_score = 1.0
        else:
            paragraph_score = 0.5

        return (sentence_score + paragraph_score) / 2

    def _score_information_density(self, content: str) -> float:
        """Score information density and uniqueness"""
        words = re.findall(r"\b\w+\b", content.lower())

        if not words:
            return 0.0

        # Vocabulary richness
        unique_words = set(words)
        vocabulary_ratio = len(unique_words) / len(words)

        # Good range is 0.3-0.7
        if 0.3 <= vocabulary_ratio <= 0.7:
            vocab_score = 1.0
        elif 0.2 <= vocabulary_ratio <= 0.8:
            vocab_score = 0.7
        else:
            vocab_score = 0.4

        # Check for meaningful content (not just filler)
        meaningful_words = [w for w in words if len(w) > 3]
        meaningful_ratio = len(meaningful_words) / len(words)

        content_score = min(meaningful_ratio * 1.5, 1.0)

        return (vocab_score + content_score) / 2

    def _score_code_content(self, content: str) -> float:
        """Score code content quality and ratio"""
        # Find code blocks
        code_blocks = re.findall(r"```[\s\S]*?```", content)
        inline_code = re.findall(r"`[^`]+`", content)

        total_length = len(content)
        code_length = sum(len(block) for block in code_blocks) + sum(
            len(code) for code in inline_code
        )

        if total_length == 0:
            return 0.0

        code_ratio = code_length / total_length

        # For technical content, ideal code ratio is 0.2-0.5
        if 0.2 <= code_ratio <= 0.5:
            return 1.0
        elif 0.1 <= code_ratio <= 0.6:
            return 0.7
        elif code_ratio > 0:
            return 0.5
        else:
            return 0.2  # No code in technical content

    def _is_duplicate(self, content: str) -> bool:
        """Check if content is duplicate using content hashing"""
        # Normalize content for comparison
        normalized = self._normalize_content(content)

        # Create content hash
        content_hash = hashlib.sha256(normalized.encode()).hexdigest()

        if content_hash in self.seen_hashes:
            return True

        self.seen_hashes.add(content_hash)

        # Also check for near-duplicates using similarity
        # This is a simplified check - in production use more sophisticated methods
        for seen_hash in list(self.seen_hashes)[-10:]:  # Check last 10
            # Would implement similarity comparison here
            pass

        return False

    def _normalize_content(self, content: str) -> str:
        """Normalize content for duplicate detection"""
        # Remove extra whitespace
        normalized = re.sub(r"\s+", " ", content)

        # Remove punctuation for comparison
        normalized = re.sub(r"[^\w\s]", "", normalized)

        # Convert to lowercase
        normalized = normalized.lower().strip()

        return normalized

    def _load_spam_patterns(self) -> List[str]:
        """Load common spam patterns"""
        return [
            "viagra",
            "cialis",
            "casino",
            "poker",
            "lottery",
            "weight loss",
            "diet pills",
            "forex",
            "binary options",
            "get rich quick",
            "mlm",
            "work from home",
            "click here now",
            "buy now",
            "order now",
            "unsubscribe",
            "opt out",
            "remove me",
        ]

    def _load_quality_indicators(self) -> Dict[str, float]:
        """Load positive quality indicators"""
        return {
            "tutorial": 0.2,
            "guide": 0.2,
            "documentation": 0.3,
            "example": 0.2,
            "implementation": 0.2,
            "best practices": 0.3,
            "how to": 0.2,
            "reference": 0.2,
            "api": 0.1,
            "framework": 0.1,
        }

    def get_filter_stats(self) -> Dict[str, int]:
        """Get filtering statistics"""
        return {
            "total_seen": len(self.seen_hashes),
            "duplicate_checks": len(self.seen_hashes),
        }

======= deep_crawler.py ======
# Copyright 2025 Franz und Franz GmbH
#
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#     http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.

"""
Deep crawling implementation for recursive page discovery
"""

import logging
import asyncio
from typing import List, Dict, Set, Optional, Tuple
from urllib.parse import urljoin, urlparse, urlunparse
from dataclasses import dataclass
import hashlib
import re

try:
    from bs4 import BeautifulSoup
except ImportError:
    BeautifulSoup = None

logger = logging.getLogger(__name__)


@dataclass
class CrawlResult:
    """Result of a crawl operation"""

    url: str
    depth: int
    parent_url: Optional[str]
    discovered_urls: List[str]
    error: Optional[str] = None


class DeepCrawler:
    """Handles recursive crawling with depth control"""

    def __init__(
        self,
        max_depth: int = 0,
        max_pages_per_site: int = 10,
        follow_external: bool = False,
        url_manager=None,
    ):
        """
        Initialize the deep crawler

        Args:
            max_depth: Maximum crawl depth (0 = no deep crawling)
            max_pages_per_site: Maximum pages to crawl per site
            follow_external: Whether to follow external links
            url_manager: URLManager instance for database operations
        """
        self.max_depth = max_depth
        self.max_pages_per_site = max_pages_per_site
        self.follow_external = follow_external
        self.url_manager = url_manager
        self.crawled_urls: Set[str] = set()
        self.discovered_urls: Dict[str, int] = {}  # url -> depth
        self.pages_per_site: Dict[str, int] = {}  # domain -> count

    async def crawl_with_depth(
        self,
        start_url: str,
        html_content: str,
        current_depth: int = 0,
        parent_url: Optional[str] = None,
    ) -> CrawlResult:
        """
        Crawl a page and discover linked pages up to max_depth

        Args:
            start_url: The URL being crawled
            html_content: HTML content of the page
            current_depth: Current crawl depth
            parent_url: Parent URL that linked to this page

        Returns:
            CrawlResult with discovered URLs
        """
        logger.info(f"Crawling {start_url} at depth {current_depth}")

        # Mark as crawled
        self.crawled_urls.add(self._normalize_url(start_url))

        # Track pages per site
        domain = urlparse(start_url).netloc
        self.pages_per_site[domain] = self.pages_per_site.get(domain, 0) + 1

        discovered = []

        # Only extract links if we haven't reached max depth
        if current_depth < self.max_depth:
            try:
                discovered = await self._extract_links(start_url, html_content)

                # Filter discovered URLs
                filtered_urls = []
                for url in discovered:
                    if self._should_crawl(url, start_url, current_depth + 1):
                        filtered_urls.append(url)
                        # Track discovered URL with its depth
                        norm_url = self._normalize_url(url)
                        if norm_url not in self.discovered_urls:
                            self.discovered_urls[norm_url] = current_depth + 1

                discovered = filtered_urls
                logger.info(f"Discovered {len(discovered)} new URLs to crawl")

            except Exception as e:
                logger.error(f"Error extracting links from {start_url}: {e}")
                return CrawlResult(
                    url=start_url,
                    depth=current_depth,
                    parent_url=parent_url,
                    discovered_urls=[],
                    error=str(e),
                )

        return CrawlResult(
            url=start_url,
            depth=current_depth,
            parent_url=parent_url,
            discovered_urls=discovered,
        )

    async def _extract_links(self, base_url: str, html_content: str) -> List[str]:
        """Extract all links from HTML content"""
        if not BeautifulSoup:
            logger.warning("BeautifulSoup not available, cannot extract links")
            return []

        try:
            soup = BeautifulSoup(html_content, "html.parser")
            links = []

            # Find all link elements
            for tag in soup.find_all(["a", "link"]):
                href = tag.get("href")
                if href:
                    # Make absolute URL
                    absolute_url = urljoin(base_url, href)

                    # Clean and validate URL
                    if self._is_valid_url(absolute_url):
                        links.append(absolute_url)

            # Deduplicate
            return list(set(links))

        except Exception as e:
            logger.error(f"Error parsing HTML: {e}")
            return []

    def _should_crawl(self, url: str, parent_url: str, depth: int) -> bool:
        """Determine if a URL should be crawled"""
        # Check depth limit
        if depth > self.max_depth:
            return False

        # Check if already crawled
        norm_url = self._normalize_url(url)
        if norm_url in self.crawled_urls:
            return False

        # Check pages per site limit
        domain = urlparse(url).netloc
        if self.pages_per_site.get(domain, 0) >= self.max_pages_per_site:
            logger.debug(f"Reached max pages limit for {domain}")
            return False

        # Check external links policy
        if not self.follow_external:
            parent_domain = urlparse(parent_url).netloc
            url_domain = urlparse(url).netloc
            if parent_domain != url_domain:
                logger.debug(f"Skipping external URL: {url}")
                return False

        # Check URL patterns to exclude
        if self._should_exclude_url(url):
            return False

        return True

    def _should_exclude_url(self, url: str) -> bool:
        """Check if URL should be excluded from crawling"""
        # Common file extensions to exclude
        excluded_extensions = {
            ".pdf",
            ".jpg",
            ".jpeg",
            ".png",
            ".gif",
            ".svg",
            ".webp",
            ".mp3",
            ".mp4",
            ".avi",
            ".mov",
            ".wmv",
            ".zip",
            ".tar",
            ".gz",
            ".rar",
            ".7z",
            ".doc",
            ".docx",
            ".xls",
            ".xlsx",
            ".ppt",
            ".pptx",
            ".exe",
            ".dmg",
            ".pkg",
            ".deb",
            ".rpm",
        }

        url_lower = url.lower()
        for ext in excluded_extensions:
            if url_lower.endswith(ext):
                return True

        # Exclude common non-content URLs
        excluded_patterns = [
            r"/login",
            r"/signin",
            r"/signup",
            r"/register",
            r"/logout",
            r"/signout",
            r"/download",
            r"/print",
            r"mailto:",
            r"tel:",
            r"javascript:",
            r"#$",  # Anchor-only links
        ]

        for pattern in excluded_patterns:
            if re.search(pattern, url, re.IGNORECASE):
                return True

        return False

    def _is_valid_url(self, url: str) -> bool:
        """Check if URL is valid for crawling"""
        try:
            parsed = urlparse(url)

            # Must have scheme and netloc
            if not parsed.scheme or not parsed.netloc:
                return False

            # Only HTTP(S)
            if parsed.scheme not in ["http", "https"]:
                return False

            return True

        except Exception:
            return False

    def _normalize_url(self, url: str) -> str:
        """Normalize URL for deduplication"""
        try:
            parsed = urlparse(url.lower())

            # Remove fragments
            normalized = urlunparse(
                (
                    parsed.scheme,
                    parsed.netloc,
                    parsed.path.rstrip("/"),
                    parsed.params,
                    parsed.query,
                    "",  # No fragment
                )
            )

            return normalized

        except Exception:
            return url.lower()

    async def process_crawl_queue(
        self, initial_urls: List[str], scraper_callback=None
    ) -> Dict[str, CrawlResult]:
        """
        Process a queue of URLs with depth-first crawling

        Args:
            initial_urls: List of starting URLs
            scraper_callback: Async function to scrape a URL

        Returns:
            Dictionary of URL -> CrawlResult
        """
        results = {}
        queue = [(url, 0, None) for url in initial_urls]  # (url, depth, parent)

        while (
            queue and len(self.crawled_urls) < self.max_pages_per_site * 10
        ):  # Safety limit
            url, depth, parent = queue.pop(0)

            # Skip if already crawled
            norm_url = self._normalize_url(url)
            if norm_url in self.crawled_urls:
                continue

            # Scrape the page if callback provided
            html_content = None
            if scraper_callback:
                try:
                    scrape_result = await scraper_callback(url)
                    if scrape_result and hasattr(scrape_result, "html"):
                        html_content = scrape_result.html
                except Exception as e:
                    logger.error(f"Error scraping {url}: {e}")
                    results[url] = CrawlResult(
                        url=url,
                        depth=depth,
                        parent_url=parent,
                        discovered_urls=[],
                        error=str(e),
                    )
                    continue

            # Crawl and discover new URLs
            if html_content:
                result = await self.crawl_with_depth(url, html_content, depth, parent)
                results[url] = result

                # Add discovered URLs to queue
                for discovered_url in result.discovered_urls:
                    if depth + 1 <= self.max_depth:
                        queue.append((discovered_url, depth + 1, url))

            # Small delay to avoid overwhelming servers
            await asyncio.sleep(0.5)

        logger.info(f"Deep crawl complete: {len(results)} pages crawled")
        return results

    def get_crawl_statistics(self) -> Dict[str, any]:
        """Get statistics about the crawl"""
        return {
            "total_crawled": len(self.crawled_urls),
            "total_discovered": len(self.discovered_urls),
            "pages_per_site": dict(self.pages_per_site),
            "max_depth_reached": (
                max(self.discovered_urls.values()) if self.discovered_urls else 0
            ),
        }

    def reset(self):
        """Reset crawler state"""
        self.crawled_urls.clear()
        self.discovered_urls.clear()
        self.pages_per_site.clear()

======= job_manager.py ======
# Copyright 2025 Franz und Franz GmbH
#
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#     http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.

"""
Job management for m1f-research with persistence and resume support
"""
import logging
from pathlib import Path
from typing import Optional, Dict, Any, List
from datetime import datetime

from m1f.file_operations import (
    safe_exists,
    safe_mkdir,
)

from .research_db import ResearchDatabase, JobDatabase, ResearchJob
from .config import ResearchConfig

logger = logging.getLogger(__name__)


class JobManager:
    """Manages research jobs with persistence"""

    def __init__(self, base_dir: Path):
        self.base_dir = base_dir
        # Note: We'll need to call async init separately
        # For now, keep sync behavior for compatibility

        # Main research database
        self.main_db = ResearchDatabase(self.base_dir / "research_jobs.db")

    def create_job(self, query: str, config: ResearchConfig) -> ResearchJob:
        """Create a new research job"""
        # Create output directory with hierarchical structure
        now = datetime.now()
        output_dir = self.base_dir / now.strftime("%Y/%m/%d")

        # Create job with serializable config
        config_dict = self._serialize_config(config)
        job = ResearchJob.create_new(
            query=query, config=config_dict, output_dir=str(output_dir)
        )

        # Update output directory to include job ID
        job.output_dir = str(
            output_dir / f"{job.job_id}_{self._sanitize_query(query)[:30]}"
        )

        # Save to database
        self.main_db.create_job(job)

        # Create job directory
        job_path = Path(job.output_dir)
        # Note: We'll need to call safe_mkdir separately for full async support
        job_path.mkdir(parents=True, exist_ok=True)

        # Create job-specific database
        job_db = JobDatabase(job_path / "research.db")

        logger.info(f"Created job {job.job_id}: {query}")
        logger.info(f"Output directory: {job.output_dir}")

        return job

    def get_job(self, job_id: str) -> Optional[ResearchJob]:
        """Get an existing job by ID"""
        job = self.main_db.get_job(job_id)
        if not job:
            logger.error(f"Job {job_id} not found")
        return job

    def get_job_database(self, job: ResearchJob) -> JobDatabase:
        """Get the database for a specific job"""
        job_path = Path(job.output_dir)
        return JobDatabase(job_path / "research.db")

    def update_job_status(self, job_id: str, status: str):
        """Update job status"""
        self.main_db.update_job_status(job_id, status)
        logger.info(f"Updated job {job_id} status to: {status}")

    def update_job_stats(self, job: ResearchJob, **additional_stats):
        """Update job statistics from job database"""
        job_db = self.get_job_database(job)
        stats = job_db.get_stats()
        stats.update(additional_stats)
        self.main_db.update_job_stats(job.job_id, **stats)

    def list_jobs(
        self,
        status: Optional[str] = None,
        limit: Optional[int] = None,
        offset: int = 0,
        date_filter: Optional[str] = None,
        search_term: Optional[str] = None,
    ) -> List[Dict[str, Any]]:
        """List jobs with advanced filtering"""
        return self.main_db.list_jobs(status, limit, offset, date_filter, search_term)

    def count_jobs(
        self,
        status: Optional[str] = None,
        date_filter: Optional[str] = None,
        search_term: Optional[str] = None,
    ) -> int:
        """Count jobs matching filters"""
        return self.main_db.count_jobs(status, date_filter, search_term)

    def find_recent_job(self, query: str) -> Optional[ResearchJob]:
        """Find the most recent job for a similar query"""
        jobs = self.list_jobs(status="active")

        # Simple similarity check (can be improved)
        query_lower = query.lower()
        for job_data in jobs:
            if query_lower in job_data["query"].lower():
                return self.get_job(job_data["job_id"])

        return None

    async def create_symlink_to_latest(self, job: ResearchJob):
        """Create a symlink to the latest research bundle"""
        job_path = Path(job.output_dir)
        bundle_path = job_path / "research_bundle.md"

        if safe_exists(bundle_path):
            # Create symlink in base directory
            latest_link = self.base_dir / "latest_research.md"

            # Remove old symlink if exists
            if safe_exists(latest_link) or latest_link.is_symlink():
                latest_link.unlink()

            # Create relative symlink
            try:
                relative_path = Path("..") / bundle_path.relative_to(
                    self.base_dir.parent
                )
                latest_link.symlink_to(relative_path)
                logger.info(f"Created symlink: {latest_link} -> {relative_path}")
            except Exception as e:
                logger.warning(f"Could not create symlink: {e}")
                # Fallback: create absolute symlink
                try:
                    latest_link.symlink_to(bundle_path.absolute())
                except Exception as e2:
                    logger.error(f"Failed to create symlink: {e2}")

    def _sanitize_query(self, query: str) -> str:
        """Sanitize query for directory name"""
        safe_name = "".join(c if c.isalnum() or c in "- " else "_" for c in query)
        return safe_name.replace(" ", "-").lower()

    def get_job_info(self, job: ResearchJob) -> Dict[str, Any]:
        """Get comprehensive job information"""
        job_db = self.get_job_database(job)
        stats = job_db.get_stats()

        bundle_exists = safe_exists(Path(job.output_dir) / "research_bundle.md")

        return {
            "job_id": job.job_id,
            "query": job.query,
            "status": job.status,
            "created_at": job.created_at.isoformat(),
            "updated_at": job.updated_at.isoformat(),
            "output_dir": job.output_dir,
            "stats": stats,
            "bundle_exists": bundle_exists,
        }

    def cleanup_old_jobs(self, days: int = 30):
        """Clean up jobs older than specified days"""
        # TODO: Implement cleanup logic
        pass

    async def delete_job(self, job_id: str) -> Dict[str, Any]:
        """
        Delete a specific job completely (database + filesystem)

        Returns:
            Dict with deletion results and any errors
        """
        import shutil

        # Get job details first
        job = self.get_job(job_id)
        if not job:
            return {"error": f"Job {job_id} not found", "deleted": False}

        results = {
            "job_id": job_id,
            "query": job.query,
            "deleted": False,
            "database_deleted": False,
            "filesystem_deleted": False,
            "errors": [],
        }

        try:
            # Delete from database
            db_deleted = self.main_db.delete_job(job_id)
            results["database_deleted"] = db_deleted

            if not db_deleted:
                results["errors"].append("Failed to delete from database")

            # Delete filesystem data
            job_path = Path(job.output_dir)
            if safe_exists(job_path):
                try:
                    # Remove the entire job directory
                    shutil.rmtree(job_path, ignore_errors=False)
                    results["filesystem_deleted"] = True
                    logger.info(f"Deleted job directory: {job_path}")
                except Exception as e:
                    # Try with ignore_errors if permission issues
                    try:
                        shutil.rmtree(job_path, ignore_errors=True)
                        results["filesystem_deleted"] = True
                        results["errors"].append(f"Partial filesystem deletion: {e}")
                    except Exception as e2:
                        results["filesystem_deleted"] = False
                        results["errors"].append(f"Filesystem deletion failed: {e2}")
            else:
                results["filesystem_deleted"] = True  # Already gone

            results["deleted"] = (
                results["database_deleted"] and results["filesystem_deleted"]
            )

        except Exception as e:
            logger.error(f"Error deleting job {job_id}: {e}")
            results["errors"].append(str(e))

        return results

    async def delete_jobs(
        self,
        status: Optional[str] = None,
        date_filter: Optional[str] = None,
        search_term: Optional[str] = None,
        job_ids: Optional[List[str]] = None,
    ) -> Dict[str, Any]:
        """
        Delete multiple jobs based on filters or explicit IDs

        Args:
            status: Filter by job status
            date_filter: Filter by date
            search_term: Filter by search term
            job_ids: Explicit list of job IDs to delete

        Returns:
            Dict with deletion statistics and any errors
        """
        results = {
            "total_processed": 0,
            "successfully_deleted": 0,
            "failed_deletions": 0,
            "deleted_jobs": [],
            "failed_jobs": [],
            "errors": [],
        }

        try:
            # Get jobs to delete
            if job_ids:
                # Use explicit job IDs
                jobs_to_delete = []
                for job_id in job_ids:
                    job = self.get_job(job_id)
                    if job:
                        jobs_to_delete.append(
                            {
                                "job_id": job.job_id,
                                "query": job.query,
                                "status": job.status,
                                "output_dir": job.output_dir,
                            }
                        )
            else:
                # Use filters to find jobs
                jobs_to_delete = self.list_jobs(
                    status=status, date_filter=date_filter, search_term=search_term
                )

            results["total_processed"] = len(jobs_to_delete)

            # Delete each job
            for job_info in jobs_to_delete:
                job_id = job_info["job_id"]
                deletion_result = await self.delete_job(job_id)

                if deletion_result.get("deleted"):
                    results["successfully_deleted"] += 1
                    results["deleted_jobs"].append(
                        {"job_id": job_id, "query": job_info.get("query", "Unknown")}
                    )
                else:
                    results["failed_deletions"] += 1
                    results["failed_jobs"].append(
                        {
                            "job_id": job_id,
                            "query": job_info.get("query", "Unknown"),
                            "errors": deletion_result.get("errors", []),
                        }
                    )

        except Exception as e:
            logger.error(f"Error in bulk deletion: {e}")
            results["errors"].append(str(e))

        return results

    async def cleanup_job_raw_data(self, job_id: str) -> Dict[str, Any]:
        """
        Clean up raw data for a specific job while preserving aggregated data

        Returns:
            Dict with cleanup statistics
        """
        job = self.get_job(job_id)
        if not job:
            return {"error": f"Job {job_id} not found"}

        job_db = self.get_job_database(job)
        cleanup_stats = job_db.cleanup_raw_content()

        # Also clean up any HTML files in the job directory
        job_dir = Path(job.output_dir)
        html_files_deleted = 0
        space_freed = 0

        if safe_exists(job_dir):
            # Look for HTML files (if any were saved)
            for html_file in job_dir.glob("**/*.html"):
                try:
                    file_size = html_file.stat().st_size
                    html_file.unlink()
                    html_files_deleted += 1
                    space_freed += file_size
                except Exception as e:
                    logger.error(f"Error deleting {html_file}: {e}")

        cleanup_stats["html_files_deleted"] = html_files_deleted
        cleanup_stats["space_freed_mb"] = round(space_freed / (1024 * 1024), 2)

        logger.info(f"Cleaned up job {job_id}: {cleanup_stats}")
        return cleanup_stats

    async def cleanup_all_raw_data(self) -> Dict[str, Any]:
        """Clean up raw data for all jobs"""
        all_jobs = self.list_jobs()
        total_stats = {
            "jobs_cleaned": 0,
            "files_deleted": 0,
            "space_freed_mb": 0,
            "errors": [],
        }

        for job_info in all_jobs:
            try:
                stats = await self.cleanup_job_raw_data(job_info["job_id"])
                if "error" not in stats:
                    total_stats["jobs_cleaned"] += 1
                    total_stats["files_deleted"] += stats.get("html_files_deleted", 0)
                    total_stats["space_freed_mb"] += stats.get("space_freed_mb", 0)
                else:
                    total_stats["errors"].append(stats["error"])
            except Exception as e:
                total_stats["errors"].append(
                    f"Error cleaning job {job_info['job_id']}: {e}"
                )

        return total_stats

    def _serialize_config(self, config: ResearchConfig) -> Dict[str, Any]:
        """Convert ResearchConfig to serializable dict"""

        def serialize_value(val):
            if hasattr(val, "__dict__"):
                return {k: serialize_value(v) for k, v in val.__dict__.items()}
            elif isinstance(val, Path):
                return str(val)
            elif isinstance(val, (list, tuple)):
                return [serialize_value(v) for v in val]
            elif isinstance(val, dict):
                return {k: serialize_value(v) for k, v in val.items()}
            else:
                return val

        return serialize_value(config)

======= llm_interface.py ======
# Copyright 2025 Franz und Franz GmbH
#
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#     http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.

"""
LLM Provider interface and implementations for m1f-research
"""
from abc import ABC, abstractmethod
from typing import Dict, Any, List, Optional, Union
import os
import json
import subprocess
import aiohttp
import asyncio
from dataclasses import dataclass
from .prompt_utils import get_web_search_prompt
import anyio

# Claude SDK removed - using direct subprocess instead

# Import shared Claude utilities
from shared.claude_utils import (
    ClaudeConfig,
    ClaudeHTTPClient,
    ClaudeSessionManager,
    ClaudeErrorHandler,
)


@dataclass
class LLMResponse:
    """Standard response format from LLM providers"""

    content: str
    raw_response: Optional[Dict[str, Any]] = None
    usage: Optional[Dict[str, int]] = None
    error: Optional[str] = None


class LLMProvider(ABC):
    """Base class for LLM providers"""

    def __init__(self, api_key: Optional[str] = None, model: Optional[str] = None):
        self.api_key = api_key
        self.model = model or self.default_model

    @property
    @abstractmethod
    def default_model(self) -> str:
        """Default model for this provider"""
        pass

    @abstractmethod
    async def query(
        self, prompt: str, system: Optional[str] = None, **kwargs
    ) -> LLMResponse:
        """
        Query the LLM with a prompt

        Args:
            prompt: User prompt
            system: System prompt (optional)
            **kwargs: Provider-specific options

        Returns:
            LLMResponse object
        """
        pass

    @abstractmethod
    async def search_web(
        self, query: str, num_results: int = 20
    ) -> List[Dict[str, str]]:
        """
        Use LLM to search the web for URLs

        Args:
            query: Search query
            num_results: Number of results to return

        Returns:
            List of dicts with 'url', 'title', 'description'
        """
        pass

    @abstractmethod
    async def analyze_content(
        self, content: str, analysis_type: str = "relevance"
    ) -> Dict[str, Any]:
        """
        Analyze content using the LLM

        Args:
            content: Content to analyze
            analysis_type: Type of analysis (relevance, summary, key_points, etc.)

        Returns:
            Analysis results as dict
        """
        pass


class ClaudeProvider(LLMProvider):
    """Claude API provider via Anthropic"""

    def __init__(self, api_key: Optional[str] = None, model: Optional[str] = None):
        api_key = api_key or os.getenv("ANTHROPIC_API_KEY")
        super().__init__(api_key, model)

        # Use shared configuration and HTTP client
        self.config = ClaudeConfig(api_key=api_key, model=self.model)
        self.client = ClaudeHTTPClient(self.config)
        self.error_handler = ClaudeErrorHandler()

    @property
    def default_model(self) -> str:
        return "claude-3-opus-20240229"

    async def query(
        self, prompt: str, system: Optional[str] = None, **kwargs
    ) -> LLMResponse:
        """Query Claude API using shared HTTP client"""
        try:
            response = await self.client.send_request(
                prompt=prompt, system=system, **kwargs
            )

            return LLMResponse(
                content=response["content"][0]["text"],
                raw_response=response,
                usage=response.get("usage"),
            )
        except Exception as e:
            self.error_handler.handle_api_error(e, operation="Claude API query")
            return LLMResponse(content="", error=str(e))

    async def search_web(
        self, query: str, num_results: int = 20
    ) -> List[Dict[str, str]]:
        """Use Claude to generate search URLs"""
        prompt = f"""As a research assistant, help me find {num_results} relevant web resources about: "{query}"

Please suggest real, existing websites and resources that would be helpful for researching this topic. Return your suggestions as a JSON array where each entry has:
- url: A real website URL that likely contains information on this topic
- title: The expected page/site title
- description: What kind of information this resource likely contains

Focus on well-known, authoritative sources in this domain such as:
- Official documentation and guides
- Industry-leading blogs and publications
- Educational resources and tutorials
- Professional forums and communities

Example format:
[
  {{"url": "https://example.com/article", "title": "Article Title", "description": "Brief description"}}
]

Return ONLY the JSON array, no other text."""

        response = await self.query(prompt)

        if response.error:
            raise Exception(f"LLM error: {response.error}")

        try:
            # Extract JSON from response
            content = response.content.strip()

            # Handle various formats Claude might return
            if "```json" in content:
                start = content.find("```json") + 7
                end = content.find("```", start)
                if end != -1:
                    content = content[start:end].strip()
            elif "```" in content:
                start = content.find("```") + 3
                end = content.find("```", start)
                if end != -1:
                    content = content[start:end].strip()

            # Try to find JSON array brackets if not already present
            if not content.startswith("["):
                start_idx = content.find("[")
                if start_idx != -1:
                    end_idx = content.rfind("]") + 1
                    if end_idx > start_idx:
                        content = content[start_idx:end_idx]

            results = json.loads(content)

            # Validate and ensure required fields
            if not isinstance(results, list):
                raise ValueError("Response is not a JSON array")

            valid_results = []
            for result in results:
                if isinstance(result, dict) and "url" in result:
                    if "title" not in result:
                        result["title"] = "Untitled"
                    if "description" not in result:
                        result["description"] = ""
                    valid_results.append(result)

            return valid_results[:num_results]

        except (json.JSONDecodeError, ValueError) as e:
            raise Exception(f"Failed to parse LLM response as JSON: {str(e)}")

    async def analyze_content(
        self, content: str, analysis_type: str = "relevance"
    ) -> Dict[str, Any]:
        """Analyze content with Claude"""
        prompts = {
            "relevance": """Analyze this content and rate its relevance (0-10) for the research topic.
Return JSON with: relevance_score, reason, key_topics""",
            "summary": """Provide a concise summary of this content.
Return JSON with: summary, main_points (array), content_type""",
            "key_points": """Extract the key points from this content.
Return JSON with: key_points (array), technical_level, recommended_reading_order""",
        }

        prompt = prompts.get(analysis_type, prompts["relevance"])
        prompt = f"{prompt}\n\nContent:\n{content[:4000]}..."  # Limit content length

        response = await self.query(prompt)

        if response.error:
            raise Exception(f"Analysis error: {response.error}")

        try:
            content = response.content.strip()
            if content.startswith("```json"):
                content = content[7:]
            if content.endswith("```"):
                content = content[:-3]

            return json.loads(content)

        except json.JSONDecodeError:
            # Return basic analysis if JSON parsing fails
            return {
                "relevance_score": 5,
                "reason": "Could not parse analysis",
                "raw_response": response.content,
            }


class GeminiProvider(LLMProvider):
    """Google Gemini API provider"""

    def __init__(self, api_key: Optional[str] = None, model: Optional[str] = None):
        api_key = api_key or os.getenv("GOOGLE_API_KEY")
        super().__init__(api_key, model)
        self.base_url = "https://generativelanguage.googleapis.com/v1beta"

    @property
    def default_model(self) -> str:
        return "gemini-pro"

    def _validate_api_key(self):
        """Validate that API key is present"""
        if not self.api_key:
            raise ValueError("GOOGLE_API_KEY is required for Gemini provider")

    async def query(
        self, prompt: str, system: Optional[str] = None, **kwargs
    ) -> LLMResponse:
        """Query Gemini API"""
        self._validate_api_key()

        # Combine system and user prompts for Gemini
        full_prompt = prompt
        if system:
            full_prompt = f"{system}\n\n{prompt}"

        data = {
            "contents": [{"parts": [{"text": full_prompt}]}],
            "generationConfig": {
                "temperature": kwargs.get("temperature", 0.7),
                "topK": kwargs.get("top_k", 40),
                "topP": kwargs.get("top_p", 0.95),
                "maxOutputTokens": kwargs.get("max_tokens", 2048),
            },
        }

        try:
            async with aiohttp.ClientSession() as session:
                async with session.post(
                    f"{self.base_url}/models/{self.model}:generateContent?key={self.api_key}",
                    json=data,
                ) as response:
                    result = await response.json()

                    if response.status != 200:
                        return LLMResponse(
                            content="",
                            error=f"API error: {result.get('error', {}).get('message', 'Unknown error')}",
                        )

                    content = result["candidates"][0]["content"]["parts"][0]["text"]

                    return LLMResponse(
                        content=content,
                        raw_response=result,
                        usage=result.get("usageMetadata"),
                    )

        except Exception as e:
            return LLMResponse(content="", error=str(e))

    async def search_web(
        self, query: str, num_results: int = 20
    ) -> List[Dict[str, str]]:
        """Use Gemini to generate search URLs"""
        prompt = f"""As a research assistant, help me find {num_results} relevant web resources about: "{query}"

Please suggest real, existing websites and resources that would be helpful for researching this topic. Return your suggestions as a JSON array where each entry has:
- url: A real website URL that likely contains information on this topic
- title: The expected page/site title
- description: What kind of information this resource likely contains

Focus on well-known, authoritative sources.

Example format:
[
  {{"url": "https://example.com/article", "title": "Article Title", "description": "Brief description"}}
]

Return ONLY the JSON array."""

        response = await self.query(prompt)

        if response.error:
            raise Exception(f"LLM error: {response.error}")

        try:
            content = response.content.strip()

            # Handle various formats Gemini might return
            if "```json" in content:
                start = content.find("```json") + 7
                end = content.find("```", start)
                if end != -1:
                    content = content[start:end].strip()
            elif "```" in content:
                start = content.find("```") + 3
                end = content.find("```", start)
                if end != -1:
                    content = content[start:end].strip()

            # Try to find JSON array brackets if not already present
            if not content.startswith("["):
                start_idx = content.find("[")
                if start_idx != -1:
                    end_idx = content.rfind("]") + 1
                    if end_idx > start_idx:
                        content = content[start_idx:end_idx]

            results = json.loads(content)

            # Validate and ensure required fields
            if not isinstance(results, list):
                raise ValueError("Response is not a JSON array")

            valid_results = []
            for result in results:
                if isinstance(result, dict) and "url" in result:
                    if "title" not in result:
                        result["title"] = "Untitled"
                    if "description" not in result:
                        result["description"] = ""
                    valid_results.append(result)

            return valid_results[:num_results]

        except (json.JSONDecodeError, ValueError) as e:
            raise Exception(f"Failed to parse Gemini response as JSON: {str(e)}")

    async def analyze_content(
        self, content: str, analysis_type: str = "relevance"
    ) -> Dict[str, Any]:
        """Analyze content with Gemini"""
        # Similar implementation to Claude
        prompts = {
            "relevance": """Analyze this content and rate its relevance (0-10).
Return JSON with: relevance_score, reason, key_topics""",
            "summary": """Summarize this content.
Return JSON with: summary, main_points (array), content_type""",
            "key_points": """Extract key points.
Return JSON with: key_points (array), technical_level""",
        }

        prompt = prompts.get(analysis_type, prompts["relevance"])
        prompt = f"{prompt}\n\nContent:\n{content[:4000]}..."

        response = await self.query(prompt)

        if response.error:
            raise Exception(f"Analysis error: {response.error}")

        try:
            content = response.content.strip()
            if content.startswith("```json"):
                content = content[7:]
            if content.endswith("```"):
                content = content[:-3]

            return json.loads(content)

        except json.JSONDecodeError:
            return {
                "relevance_score": 5,
                "reason": "Could not parse analysis",
                "raw_response": response.content,
            }


class ClaudeCodeProvider(LLMProvider):
    """Claude Code provider using subprocess for direct CLI control"""

    def __init__(self, api_key: Optional[str] = None, model: Optional[str] = None):
        super().__init__(api_key="claude-code", model=model)
        self.error_handler = ClaudeErrorHandler()
        self.binary_path = self._find_claude_binary()

    def _find_claude_binary(self) -> str:
        """Find Claude binary in system"""
        # Try default command first
        try:
            result = subprocess.run(
                ["claude", "--version"], capture_output=True, text=True, timeout=5
            )
            if result.returncode == 0:
                return "claude"
        except (subprocess.CalledProcessError, FileNotFoundError):
            pass

        # Try known paths
        from shared.claude_utils import ClaudeBinaryFinder

        return ClaudeBinaryFinder.find()

    @property
    def default_model(self) -> str:
        return None  # Let Claude CLI use its default model

    async def query(
        self, prompt: str, system: Optional[str] = None, **kwargs
    ) -> LLMResponse:
        """Query Claude using direct subprocess call with streaming"""
        # Combine system and user prompts
        full_prompt = prompt
        if system:
            full_prompt = f"{system}\n\n{prompt}"

        try:
            # Build command with WebSearch tool enabled for URL discovery
            # Use stream-json format for real-time feedback
            cmd = [
                self.binary_path,
                "-p",
                "--allowedTools",
                "WebSearch",
                "--output-format",
                "stream-json",
                "--verbose",
            ]

            # Add model if specified and not None
            if self.model and self.model not in [None, "default"]:
                cmd.extend(["--model", self.model])

            # Show progress if requested
            show_progress = kwargs.get("show_progress", True)

            # Run subprocess in executor to avoid blocking
            loop = asyncio.get_event_loop()

            def run_claude():
                import time
                import sys
                import json

                # Try to import colors for better output
                try:
                    from shared.colors import info, dim
                except ImportError:
                    # Fallback if colors not available
                    def info(msg):
                        print(f"  {msg}", flush=True)

                    def dim(msg):
                        return msg

                # Use Popen for streaming like m1f_claude_runner
                process = subprocess.Popen(
                    cmd,
                    stdin=subprocess.PIPE,
                    stdout=subprocess.PIPE,
                    stderr=subprocess.PIPE,
                    text=True,
                    bufsize=1,
                    universal_newlines=True,
                )

                # Send prompt via stdin and close it
                process.stdin.write(full_prompt)
                process.stdin.close()

                # Collect output
                stdout_lines = []
                result_content = []
                start_time = time.time()
                last_progress_time = 0
                spinner_chars = ["â ‹", "â ™", "â ¹", "â ¸", "â ¼", "â ´", "â ¦", "â §", "â ‡", "â "]
                spinner_idx = 0

                # Show initial message
                if show_progress:
                    info("  ðŸ¤– Claude is processing your request...")

                # Read stdout line by line for real-time feedback
                while True:
                    line = process.stdout.readline()
                    if not line and process.poll() is not None:
                        break

                    if line:
                        line = line.rstrip()  # Keep internal spacing
                        stdout_lines.append(line)

                        # Parse JSON stream for progress updates
                        if show_progress:
                            current_time = time.time()
                            elapsed = current_time - start_time

                            try:
                                # Try to parse as JSON for stream-json format
                                json_obj = json.loads(line)
                                msg_type = json_obj.get("type", "")

                                # Show progress based on message type
                                if msg_type == "assistant":
                                    # Check for tool_use and text in assistant message
                                    message = json_obj.get("message", {})
                                    if isinstance(message, dict):
                                        content_parts = message.get("content", [])
                                        for part in content_parts:
                                            if isinstance(part, dict):
                                                if (
                                                    part.get("type") == "tool_use"
                                                    and part.get("name") == "WebSearch"
                                                ):
                                                    query_info = part.get(
                                                        "input", {}
                                                    ).get("query", "")
                                                    sys.stdout.write(
                                                        "\r" + " " * 80 + "\r"
                                                    )
                                                    info(
                                                        f'  ðŸ” WebSearch: "{query_info}"'
                                                    )
                                                elif part.get("type") == "text":
                                                    text = part.get("text", "")
                                                    # Check if this is the final JSON output
                                                    if "```json" in text:
                                                        sys.stdout.write(
                                                            "\r" + " " * 80 + "\r"
                                                        )
                                                        # Try to count URLs in the JSON
                                                        try:
                                                            import re

                                                            urls_found = len(
                                                                re.findall(
                                                                    r'"url":', text
                                                                )
                                                            )
                                                            if urls_found > 0:
                                                                info(
                                                                    f"  ðŸ“ Formatting {urls_found} URLs as JSON..."
                                                                )
                                                        except:
                                                            info(
                                                                f"  ðŸ“ Formatting results as JSON..."
                                                            )
                                elif msg_type == "user":
                                    # Check for tool_result in user message (WebSearch response)
                                    message = json_obj.get("message", {})
                                    if isinstance(message, dict):
                                        content_parts = message.get("content", [])
                                        for part in content_parts:
                                            if (
                                                isinstance(part, dict)
                                                and part.get("type") == "tool_result"
                                            ):
                                                content = part.get("content", "")
                                                if "Web search results" in content:
                                                    sys.stdout.write(
                                                        "\r" + " " * 80 + "\r"
                                                    )
                                                    # Try to count links in the response
                                                    try:
                                                        import re

                                                        links_match = re.search(
                                                            r"Links: \[(.*?)\]", content
                                                        )
                                                        if links_match:
                                                            links_str = (
                                                                links_match.group(1)
                                                            )
                                                            links_count = (
                                                                links_str.count(
                                                                    '"url":'
                                                                )
                                                            )
                                                            info(
                                                                f"  âœ… WebSearch returned {links_count} results"
                                                            )
                                                        else:
                                                            info(
                                                                f"  âœ… WebSearch results received"
                                                            )
                                                    except:
                                                        info(
                                                            f"  âœ… WebSearch results received"
                                                        )
                            except json.JSONDecodeError:
                                # Not JSON, show spinner for regular output
                                if current_time - last_progress_time > 0.3:
                                    sys.stdout.write("\r")
                                    sys.stdout.write(
                                        f"  {spinner_chars[spinner_idx]} Processing... [{elapsed:.1f}s]"
                                    )
                                    sys.stdout.flush()
                                    spinner_idx = (spinner_idx + 1) % len(spinner_chars)
                                    last_progress_time = current_time

                # Clear the spinner line
                if show_progress:
                    sys.stdout.write("\r")
                    sys.stdout.write(" " * 50)  # Clear the line
                    sys.stdout.write("\r")
                    sys.stdout.flush()
                    elapsed = time.time() - start_time
                    info(f"  âœ… Claude completed in {elapsed:.1f}s")

                # Get any stderr
                stderr = process.stderr.read()

                # Wait for process to finish
                process.wait(timeout=5)

                # Extract actual content from JSON stream
                final_content = ""
                for line in stdout_lines:
                    try:
                        json_obj = json.loads(line)
                        # Look for assistant messages with text content
                        if json_obj.get("type") == "assistant":
                            message = json_obj.get("message", {})
                            if isinstance(message, dict):
                                content_parts = message.get("content", [])
                                for part in content_parts:
                                    if (
                                        isinstance(part, dict)
                                        and part.get("type") == "text"
                                    ):
                                        final_content += part.get("text", "")
                    except json.JSONDecodeError:
                        # If not JSON, include as-is (shouldn't happen with stream-json)
                        pass

                # If no JSON content found, fall back to raw output
                if not final_content:
                    final_content = "\n".join(stdout_lines)

                return process.returncode, final_content, stderr

            returncode, stdout, stderr = await loop.run_in_executor(None, run_claude)

            if returncode != 0:
                error_msg = stderr.strip() if stderr else ""
                if not error_msg:
                    error_msg = f"Process exited with code {returncode}"
                return LLMResponse(content="", error=f"Claude error: {error_msg}")

            return LLMResponse(
                content=stdout.strip(),
                raw_response={"command": cmd, "returncode": returncode},
            )

        except subprocess.TimeoutExpired:
            return LLMResponse(content="", error="Claude request timed out")
        except Exception as e:
            self.error_handler.handle_api_error(e, operation="Claude CLI query")
            return LLMResponse(content="", error=str(e))

    async def search_web(
        self, query: str, num_results: int = 20
    ) -> List[Dict[str, str]]:
        """Use Claude with WebSearch tool to find actual URLs"""
        # Use WebSearch tool to find real URLs (not generate them)
        prompt = f"""Find {num_results} good URLs about: {query}

Use the WebSearch tool to find real, current URLs. After searching, extract the URLs from the search results and return them as a JSON array.

Return format (extract URLs from search results):
[
  {{"url": "https://example.com", "title": "Page Title", "description": "Brief description"}}
]"""

        response = await self.query(prompt)

        if response.error:
            raise Exception(f"Claude error: {response.error}")

        try:
            # Extract URLs from the response
            content = response.content.strip()

            import logging
            import re

            logger = logging.getLogger(__name__)
            logger.debug(f"Raw Claude response: {content[:500]}...")

            # Extract URLs using multiple methods
            urls_found = []

            # Method 1: Look for explicit URL patterns in the response
            url_pattern = r'https?://[^\s<>"{}|\\^`\[\]]+[^\s<>"{}|\\^`\[\].,;:!?)]'
            urls_in_text = re.findall(url_pattern, content)

            # Method 2: Look for structured URL mentions (like **URL** format)
            structured_pattern = r"\*\*(https?://[^*]+)\*\*"
            structured_urls = re.findall(structured_pattern, content)

            # Method 3: Try to parse as JSON if present
            try:
                if "```json" in content:
                    start = content.find("```json") + 7
                    end = content.find("```", start)
                    if end != -1:
                        json_content = content[start:end].strip()
                        results = json.loads(json_content)
                elif "[" in content and "]" in content:
                    start_idx = content.find("[")
                    end_idx = content.rfind("]") + 1
                    if end_idx > start_idx:
                        json_content = content[start_idx:end_idx]
                        results = json.loads(json_content)
                else:
                    results = []

                if isinstance(results, list):
                    for item in results:
                        if isinstance(item, dict) and "url" in item:
                            urls_found.append(item)
            except json.JSONDecodeError:
                pass

            # If no structured results, create them from found URLs
            if not urls_found:
                # Combine all found URLs and deduplicate
                all_urls = list(set(urls_in_text + structured_urls))

                for url in all_urls[:num_results]:
                    # Extract title from URL or use domain
                    title = url.split("/")[-1] or url.split("/")[2]
                    urls_found.append(
                        {
                            "url": url,
                            "title": title.replace("-", " ").replace("_", " ").title(),
                            "description": f"Resource about {query}",
                        }
                    )

            # Ensure we have valid results
            valid_results = []
            for result in urls_found:
                if isinstance(result, dict):
                    # Ensure required fields
                    if "url" not in result and isinstance(result.get("link"), str):
                        result["url"] = result["link"]
                    if "url" in result:
                        if "title" not in result:
                            result["title"] = result["url"].split("/")[-1] or "Untitled"
                        if "description" not in result:
                            result["description"] = ""
                        valid_results.append(result)

            if not valid_results:
                logger.warning(f"No URLs found in response. Content: {content[:500]}")
                return []

            return valid_results[:num_results]

        except Exception as e:
            import logging

            logger = logging.getLogger(__name__)
            logger.error(f"Failed to extract URLs from response: {str(e)}")
            logger.error(f"Response content: {response.content[:500]}")
            return []

    async def analyze_content(
        self, content: str, analysis_type: str = "relevance"
    ) -> Dict[str, Any]:
        """Analyze content with Claude Code"""

        prompts = {
            "relevance": """Analyze this content and rate its relevance (0-10) for the research topic.
Return JSON with: relevance_score, reason, key_topics""",
            "summary": """Provide a concise summary of this content.
Return JSON with: summary, main_points (array), content_type""",
            "key_points": """Extract the key points from this content.
Return JSON with: key_points (array), technical_level, recommended_reading_order""",
        }

        prompt = prompts.get(analysis_type, prompts["relevance"])
        prompt = f"{prompt}\n\nContent:\n{content[:4000]}..."  # Limit content length

        response = await self.query(prompt)

        if response.error:
            raise Exception(f"Analysis error: {response.error}")

        try:
            content = response.content.strip()
            if content.startswith("```json"):
                content = content[7:]
            if content.endswith("```"):
                content = content[:-3]

            return json.loads(content)

        except json.JSONDecodeError:
            # Return basic analysis if JSON parsing fails
            return {
                "relevance_score": 5,
                "reason": "Could not parse analysis",
                "raw_response": response.content,
            }


class CLIProvider(LLMProvider):
    """Provider for CLI-based LLM tools like gemini-cli"""

    def __init__(self, command: str = "gemini", model: Optional[str] = None):
        super().__init__(api_key="cli", model=model)
        self.command = command

    @property
    def default_model(self) -> str:
        return "default"

    async def query(
        self, prompt: str, system: Optional[str] = None, **kwargs
    ) -> LLMResponse:
        """Query via CLI command"""
        # Combine system and user prompts
        full_prompt = prompt
        if system:
            full_prompt = f"{system}\n\n{prompt}"

        try:
            # Only handle non-Claude CLI tools
            if self.command == "claude":
                # Redirect to ClaudeCodeProvider instead
                provider = ClaudeCodeProvider(model=self.model)
                return await provider.query(prompt, system, **kwargs)

            # Other CLI tools (like gemini-cli) use stdin
            cmd = [self.command]

            # Add model if specified
            if self.model != "default":
                cmd.extend(["--model", self.model])

            # Add any additional CLI args
            if "cli_args" in kwargs:
                cmd.extend(kwargs["cli_args"])

            # Run command asynchronously
            proc = await asyncio.create_subprocess_exec(
                *cmd,
                stdin=asyncio.subprocess.PIPE,
                stdout=asyncio.subprocess.PIPE,
                stderr=asyncio.subprocess.PIPE,
            )

            stdout, stderr = await proc.communicate(full_prompt.encode())

            if proc.returncode != 0:
                return LLMResponse(content="", error=f"CLI error: {stderr.decode()}")

            return LLMResponse(
                content=stdout.decode().strip(),
                raw_response={"command": cmd, "returncode": proc.returncode},
            )

        except Exception as e:
            return LLMResponse(content="", error=str(e))

    async def search_web(
        self, query: str, num_results: int = 20
    ) -> List[Dict[str, str]]:
        """Use CLI tool to generate search URLs"""
        # Redirect claude commands to ClaudeCodeProvider
        if self.command == "claude":
            provider = ClaudeCodeProvider(model=self.model)
            return await provider.search_web(query, num_results)

        prompt = f"""As a research assistant, help me find {num_results} relevant web resources about: "{query}"

Please suggest real, existing websites and resources that would be helpful for researching this topic. Return your suggestions as a JSON array where each entry has:
- url: A real website URL that likely contains information on this topic
- title: The expected page/site title
- description: What kind of information this resource likely contains

Example format:
[
  {{"url": "https://example.com/article", "title": "Article Title", "description": "Brief description"}}
]

Return ONLY the JSON array."""

        response = await self.query(prompt)

        if response.error:
            raise Exception(f"CLI error: {response.error}")

        try:
            content = response.content.strip()

            # Handle various formats CLI might return
            if "```json" in content:
                start = content.find("```json") + 7
                end = content.find("```", start)
                if end != -1:
                    content = content[start:end].strip()
            elif "```" in content:
                start = content.find("```") + 3
                end = content.find("```", start)
                if end != -1:
                    content = content[start:end].strip()

            # Try to find JSON array brackets if not already present
            if not content.startswith("["):
                start_idx = content.find("[")
                if start_idx != -1:
                    end_idx = content.rfind("]") + 1
                    if end_idx > start_idx:
                        content = content[start_idx:end_idx]

            results = json.loads(content)

            # Validate and ensure required fields
            if not isinstance(results, list):
                raise ValueError("Response is not a JSON array")

            valid_results = []
            for result in results:
                if isinstance(result, dict) and "url" in result:
                    if "title" not in result:
                        result["title"] = "Untitled"
                    if "description" not in result:
                        result["description"] = ""
                    valid_results.append(result)

            return valid_results[:num_results]

        except (json.JSONDecodeError, ValueError) as e:
            raise Exception(f"Failed to parse CLI response as JSON: {str(e)}")

    async def analyze_content(
        self, content: str, analysis_type: str = "relevance"
    ) -> Dict[str, Any]:
        """Analyze content via CLI"""
        # Redirect claude commands to ClaudeCodeProvider
        if self.command == "claude":
            provider = ClaudeCodeProvider(model=self.model)
            return await provider.analyze_content(content, analysis_type)

        prompts = {
            "relevance": "Rate relevance 0-10. Return JSON: relevance_score, reason",
            "summary": "Summarize. Return JSON: summary, main_points",
            "key_points": "Extract key points. Return JSON: key_points",
        }

        prompt = prompts.get(analysis_type, prompts["relevance"])
        prompt = f"{prompt}\n\nContent:\n{content[:2000]}..."

        response = await self.query(prompt)

        if response.error:
            raise Exception(f"Analysis error: {response.error}")

        try:
            content = response.content.strip()
            if content.startswith("```json"):
                content = content[7:]
            if content.endswith("```"):
                content = content[:-3]

            return json.loads(content)

        except json.JSONDecodeError:
            return {
                "relevance_score": 5,
                "reason": "Could not parse analysis",
                "raw_response": response.content,
            }


def get_provider(provider_name: str, **kwargs) -> LLMProvider:
    """Factory function to get LLM provider instance"""
    providers = {
        "claude": ClaudeProvider,  # Anthropic API
        "claude-code": ClaudeCodeProvider,  # Direct Claude CLI
        "gemini": GeminiProvider,
        "gemini-cli": lambda **kw: CLIProvider(command="gemini", model=kw.get("model")),
    }

    provider_class = providers.get(provider_name.lower())
    if not provider_class:
        raise ValueError(f"Unknown provider: {provider_name}")

    return provider_class(**kwargs)

======= models.py ======
# Copyright 2025 Franz und Franz GmbH
#
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#     http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.

"""
Data models for m1f-research
"""
from dataclasses import dataclass, field
from datetime import datetime
from typing import List, Dict, Any, Optional
from pathlib import Path


@dataclass
class ResearchResult:
    """Complete research result"""

    query: str
    job_id: str
    urls_found: int
    scraped_content: List["ScrapedContent"]
    analyzed_content: List["AnalyzedContent"]
    bundle_path: Optional["Path"] = None
    bundle_created: bool = False
    output_dir: Optional["Path"] = None
    generated_at: datetime = field(default_factory=datetime.now)
    config: Dict[str, Any] = field(default_factory=dict)


@dataclass
class ScrapedContent:
    """Scraped web content"""

    url: str
    title: str
    content: str  # HTML or markdown content
    content_type: str = ""
    scraped_at: datetime = field(default_factory=datetime.now)
    error: Optional[str] = None
    metadata: Dict[str, Any] = field(default_factory=dict)


@dataclass
class AnalyzedContent:
    """Analyzed content with relevance and insights"""

    url: str
    title: str
    content: str  # markdown content
    relevance_score: float  # 0-10
    key_points: List[str]
    summary: str
    content_type: Optional[str] = None  # tutorial, reference, blog, etc.
    analysis_metadata: Dict[str, Any] = field(default_factory=dict)

    # Compatibility with old API
    @property
    def metadata(self) -> Dict[str, Any]:
        return self.analysis_metadata


@dataclass
class ResearchSource:
    """A source for research (web, github, arxiv, etc.)"""

    name: str
    type: str
    weight: float = 1.0
    config: Dict[str, Any] = field(default_factory=dict)

======= orchestrator.py ======
# Copyright 2025 Franz und Franz GmbH
#
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#     http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.

"""
Enhanced research orchestrator with job management and persistence
"""

import asyncio
import os
from pathlib import Path
from typing import List, Dict, Any, Optional
from datetime import datetime
import json
import logging

from m1f.file_operations import (
    safe_open,
)

from .config import ResearchConfig
from .llm_interface import get_provider, LLMProvider
from .models import ResearchResult, ScrapedContent, AnalyzedContent
from .job_manager import JobManager
from .research_db import ResearchJob, JobDatabase
from .url_manager import URLManager
from .smart_scraper import EnhancedSmartScraper
from .content_filter import ContentFilter
from .analyzer import ContentAnalyzer
from .bundle_creator import SmartBundleCreator
from .readme_generator import ReadmeGenerator
from .query_expander import QueryExpander, ExpandedQuery
from .url_reviewer import URLReviewer
from .deep_crawler import DeepCrawler
from .analysis_generator import AnalysisGenerator
from .workflow_phases import WorkflowPhase, WorkflowManager, PhaseContext

logger = logging.getLogger(__name__)

try:
    from scrape_tool.scrapers.base import WebScraperBase as WebScraper
except ImportError:
    logger.warning("Could not import WebScraperBase from scrape_tool")
    WebScraper = None

try:
    from html2md_tool import HTML2MDConverter as HTMLToMarkdownConverter
except ImportError:
    logger.warning("Could not import HTML2MDConverter from html2md_tool")
    HTMLToMarkdownConverter = None


class EnhancedResearchOrchestrator:
    """Enhanced orchestrator with job persistence and resume support"""

    def __init__(self, config: ResearchConfig):
        self.config = config
        self.llm = self._init_llm()
        self.job_manager = JobManager(config.output.directory)
        self.current_job: Optional[ResearchJob] = None
        self.job_db: Optional[JobDatabase] = None
        self.url_manager: Optional[URLManager] = None
        self.progress_callback = None

        # New workflow components
        self.workflow_manager = WorkflowManager(self.job_manager, config)
        self.query_expander = QueryExpander(
            self.llm,
            max_queries=(
                getattr(config.workflow, "max_queries", 5)
                if hasattr(config, "workflow")
                else 5
            ),
        )
        self.url_reviewer = URLReviewer()
        self.deep_crawler = DeepCrawler(
            max_depth=(
                getattr(config.workflow, "crawl_depth", 0)
                if hasattr(config, "workflow")
                else 0
            ),
            max_pages_per_site=(
                getattr(config.workflow, "max_pages_per_site", 10)
                if hasattr(config, "workflow")
                else 10
            ),
            follow_external=(
                getattr(config.workflow, "follow_external", False)
                if hasattr(config, "workflow")
                else False
            ),
        )
        self.analysis_generator = AnalysisGenerator(self.llm)

    def _init_llm(self) -> Optional[LLMProvider]:
        """Initialize LLM provider from config"""
        if self.config.dry_run:
            return None

        try:
            # Determine effective provider with sensible defaults
            provider_name = (self.config.llm.provider or "claude").lower()

            # If user selected Claude but no API key is present, use Claude Code
            if provider_name == "claude" and not os.getenv("ANTHROPIC_API_KEY"):
                provider_name = "claude-code"
                logger.info("No ANTHROPIC_API_KEY found, using claude-code provider")

            return get_provider(
                provider_name,
                api_key=None,  # Providers will read from environment when needed
                model=self.config.llm.model,
            )
        except Exception as e:
            logger.error(f"Failed to initialize LLM provider: {e}")
            if not self.config.no_analysis:
                raise
            return None

    def set_progress_callback(self, callback):
        """Set callback for progress updates"""
        self.progress_callback = callback

    async def research(
        self, query: str, job_id: Optional[str] = None, urls_file: Optional[Path] = None
    ) -> ResearchResult:
        """
        Run research workflow with job management

        Args:
            query: Research query
            job_id: Existing job ID to resume
            urls_file: Optional file with additional URLs

        Returns:
            ResearchResult with all findings
        """
        logger.info(f"Starting research for: {query}")

        try:
            # Initialize or resume job
            if job_id:
                self.current_job = self.job_manager.get_job(job_id)
                if not self.current_job:
                    raise ValueError(f"Job {job_id} not found")
                logger.info(f"Resuming job {job_id}")

                # Determine resume phase
                resume_phase = self.workflow_manager.get_resumable_phase(job_id)
                if not resume_phase:
                    logger.info("Job already completed or failed")
                    return self._create_empty_result()
            else:
                self.current_job = self.job_manager.create_job(query, self.config)
                logger.info(f"Created new job {self.current_job.job_id}")
                resume_phase = WorkflowPhase.INITIALIZATION

                # Initialize workflow phase
                self.workflow_manager.set_phase(
                    self.current_job.job_id,
                    WorkflowPhase.INITIALIZATION,
                    {"query": query},
                )

            # Setup job database and URL manager
            self.job_db = self.job_manager.get_job_database(self.current_job)
            self.url_manager = URLManager(self.job_db)

            # Execute workflow phases
            return await self._execute_phased_workflow(query, urls_file, resume_phase)

        except Exception as e:
            logger.error(f"Research failed: {e}")
            if self.current_job:
                self.workflow_manager.set_phase(
                    self.current_job.job_id, WorkflowPhase.FAILED, {"error": str(e)}
                )
                self.job_manager.update_job_status(self.current_job.job_id, "failed")
            raise
        finally:
            # Always cleanup database connections to prevent file locking on Windows
            self.cleanup_databases()

    async def _execute_phased_workflow(
        self, query: str, urls_file: Optional[Path], start_phase: WorkflowPhase
    ) -> ResearchResult:
        """Execute the phased research workflow"""

        # Phase tracking
        current_phase = start_phase
        expanded_queries = [query]
        urls = []
        scraped_content = []
        filtered_content = []
        analyzed_content = []
        bundle_path = None
        analysis_path = None

        # Check if we should expand queries
        should_expand = hasattr(self.config, "workflow") and getattr(
            self.config.workflow, "expand_queries", True
        )

        # Phase 1: Query Expansion
        if current_phase == WorkflowPhase.INITIALIZATION:
            if should_expand and not self.config.dry_run:
                self.workflow_manager.transition_to(
                    self.current_job.job_id, WorkflowPhase.QUERY_EXPANSION
                )
                expanded_queries = await self._expand_queries(query)
            else:
                # Skip to URL collection
                self.workflow_manager.transition_to(
                    self.current_job.job_id, WorkflowPhase.URL_COLLECTION
                )
            current_phase = self.workflow_manager.get_phase(
                self.current_job.job_id
            ).phase
            logger.info(f"Current phase after expansion: {current_phase}")

        # Phase 2: URL Collection
        logger.info(f"Checking if should collect URLs. Current phase: {current_phase}")
        if current_phase in [
            WorkflowPhase.QUERY_EXPANSION,
            WorkflowPhase.URL_COLLECTION,
        ]:
            self.workflow_manager.transition_to(
                self.current_job.job_id, WorkflowPhase.URL_COLLECTION
            )
            logger.info(f"Collecting URLs for {len(expanded_queries)} queries")
            urls = await self._collect_urls_phased(expanded_queries, urls_file)
            logger.info(f"Collected {len(urls)} URLs")

            if not urls:
                logger.warning("No URLs found")
                self.workflow_manager.transition_to(
                    self.current_job.job_id, WorkflowPhase.COMPLETED
                )
                self.job_manager.update_job_status(self.current_job.job_id, "completed")
                return self._create_empty_result()

            current_phase = WorkflowPhase.URL_COLLECTION

        # Phase 3: URL Review
        should_review = hasattr(self.config, "workflow") and not getattr(
            self.config.workflow, "skip_review", False
        )

        if current_phase == WorkflowPhase.URL_COLLECTION and should_review:
            self.workflow_manager.transition_to(
                self.current_job.job_id, WorkflowPhase.URL_REVIEW
            )
            urls = await self._review_urls(urls)

            if not urls:
                logger.warning("All URLs rejected during review")
                self.workflow_manager.transition_to(
                    self.current_job.job_id, WorkflowPhase.COMPLETED
                )
                self.job_manager.update_job_status(self.current_job.job_id, "completed")
                return self._create_empty_result()

            current_phase = WorkflowPhase.URL_REVIEW

        # Phase 4: Crawling (with deep crawl support)
        if current_phase in [WorkflowPhase.URL_COLLECTION, WorkflowPhase.URL_REVIEW]:
            self.workflow_manager.transition_to(
                self.current_job.job_id, WorkflowPhase.CRAWLING
            )
            scraped_content = await self._scrape_urls_with_depth(urls)
            current_phase = WorkflowPhase.CRAWLING

        # Phase 5: Content Filtering
        if current_phase == WorkflowPhase.CRAWLING:
            filtered_content = await self._filter_content(scraped_content)

        # Phase 6: Bundling
        if current_phase == WorkflowPhase.CRAWLING:
            self.workflow_manager.transition_to(
                self.current_job.job_id, WorkflowPhase.BUNDLING
            )

            # Content Analysis (if enabled)
            if not self.config.no_analysis and self.llm:
                analyzed_content = await self._analyze_content(filtered_content)
            else:
                analyzed_content = [
                    self._scraped_to_analyzed(s) for s in filtered_content
                ]

            bundle_path = await self._create_bundle(analyzed_content, query)
            current_phase = WorkflowPhase.BUNDLING

        # Phase 7: Analysis Generation
        should_analyze = hasattr(self.config, "workflow") and getattr(
            self.config.workflow, "generate_analysis", True
        )

        if current_phase == WorkflowPhase.BUNDLING and should_analyze and bundle_path:
            self.workflow_manager.transition_to(
                self.current_job.job_id, WorkflowPhase.ANALYSIS
            )
            analysis_path = await self._generate_analysis(bundle_path, query)
            current_phase = WorkflowPhase.ANALYSIS

        # Mark as completed
        self.workflow_manager.transition_to(
            self.current_job.job_id, WorkflowPhase.COMPLETED
        )
        self.job_manager.update_job_stats(self.current_job)
        self.job_manager.update_job_status(self.current_job.job_id, "completed")

        # Create symlink to latest research
        await self.job_manager.create_symlink_to_latest(self.current_job)

        return ResearchResult(
            query=query,
            job_id=self.current_job.job_id,
            urls_found=len(urls),
            scraped_content=scraped_content,
            analyzed_content=analyzed_content,
            bundle_path=bundle_path,
            bundle_created=True,
            output_dir=Path(self.current_job.output_dir),
        )

    async def _collect_urls(
        self, query: str, urls_file: Optional[Path], resume: bool
    ) -> List[str]:
        """Collect URLs from LLM and/or file"""
        all_urls = []

        # Add URLs from file if provided
        if urls_file:
            added = await self.url_manager.add_urls_from_file(urls_file)
            logger.info(f"Added {added} URLs from file")

        # Get URLs from LLM if not resuming
        if not resume and not self.config.dry_run:
            logger.info("Searching for URLs using LLM...")
            if self.progress_callback:
                self.progress_callback(
                    "searching", 0, self.config.scraping.search_limit
                )
            try:
                llm_urls = await self.llm.search_web(
                    query, self.config.scraping.search_limit
                )
                added = self.url_manager.add_urls_from_list(llm_urls, source="llm")
                logger.info(f"Added {added} URLs from LLM search")
                if self.progress_callback:
                    self.progress_callback(
                        "searching", added, self.config.scraping.search_limit
                    )
            except Exception as e:
                logger.error(f"Error searching for URLs: {e}")
                if not urls_file:  # If no manual URLs, this is fatal
                    # Provide helpful error message for common issues
                    if "Failed to parse" in str(e) and "JSON" in str(e):
                        error_msg = (
                            "LLM failed to generate URLs. This can happen when:\n"
                            "1. Using Claude without API key (falls back to Claude Code which may refuse URL generation)\n"
                            "2. Query contains sensitive topics\n\n"
                            "Solutions:\n"
                            "- Set ANTHROPIC_API_KEY environment variable to use Claude API\n"
                            "- Use --provider gemini with GOOGLE_API_KEY set\n"
                            "- Provide URLs manually with --urls-file\n"
                            "- Try rephrasing your query"
                        )
                        raise Exception(error_msg) from e
                    raise

        # Get unscraped URLs
        all_urls = self.url_manager.get_unscraped_urls()
        logger.info(f"Total URLs to scrape: {len(all_urls)}")

        # Update stats
        self.job_manager.update_job_stats(
            self.current_job, total_urls=self.job_db.get_stats()["total_urls"]
        )

        # Limit URLs if configured
        if (
            self.config.scraping.scrape_limit
            and len(all_urls) > self.config.scraping.scrape_limit
        ):
            all_urls = all_urls[: self.config.scraping.scrape_limit]
            logger.info(f"Limited to {len(all_urls)} URLs")

        return all_urls

    async def _scrape_urls(self, urls: List[str]) -> List[ScrapedContent]:
        """Scrape URLs with smart delay management"""
        if self.config.dry_run:
            logger.info("DRY RUN: Would scrape URLs")
            return []

        scraped_content = []

        async with EnhancedSmartScraper(
            self.config.scraping, self.job_db, self.url_manager
        ) as scraper:
            # Set progress callback
            def scraping_progress(completed, total, percentage):
                logger.info(
                    f"Scraping progress: {completed}/{total} ({percentage:.1f}%)"
                )
                if self.progress_callback:
                    self.progress_callback("scraping", completed, total)
                if completed % 5 == 0:  # Update stats every 5 URLs
                    self.job_manager.update_job_stats(
                        self.current_job,
                        scraped_urls=self.job_db.get_stats()["scraped_urls"],
                    )

            scraper.set_progress_callback(scraping_progress)

            # Scrape URLs
            raw_content = await scraper.scrape_urls(urls)

            # Convert HTML to Markdown
            for scraped in raw_content:
                try:
                    # Use html2md tool if available
                    if HTMLToMarkdownConverter:
                        converter = HTMLToMarkdownConverter()
                        markdown = converter.convert_html(scraped.content)
                    else:
                        # Fallback to basic conversion
                        markdown = self._basic_html_to_markdown(scraped.content)

                    # Save to database
                    self.job_db.save_content(
                        url=scraped.url,
                        title=scraped.title,
                        markdown=markdown,
                        metadata={
                            "scraped_at": scraped.scraped_at.isoformat(),
                            "content_type": scraped.content_type,
                        },
                    )

                    # Update scraped content
                    scraped.content = markdown
                    scraped_content.append(scraped)

                except Exception as e:
                    logger.error(f"Error converting {scraped.url}: {e}")

        # Final stats update
        stats = scraper.get_statistics()
        logger.info(
            f"Scraping complete: {stats['successful_urls']} successful, "
            f"{stats['failed_urls']} failed"
        )

        self.job_manager.update_job_stats(self.current_job)

        return scraped_content

    async def _filter_content(
        self, content: List[ScrapedContent]
    ) -> List[ScrapedContent]:
        """Filter content for quality"""
        if self.config.no_filter:
            logger.info("Content filtering disabled")
            return content

        filter = ContentFilter(self.config.filtering)
        filtered = []

        for item in content:
            passed, reason = filter.filter_content(item.content)

            # Update database
            self.job_db.save_content(
                url=item.url,
                title=item.title,
                markdown=item.content,
                metadata={"scraped_at": item.scraped_at.isoformat()},
                filtered=not passed,
                filter_reason=reason,
            )

            if passed:
                filtered.append(item)
            else:
                logger.debug(f"Filtered out {item.url}: {reason}")

        logger.info(f"Filtered {len(content)} to {len(filtered)} items")

        # Update stats
        self.job_manager.update_job_stats(
            self.current_job, filtered_urls=len(content) - len(filtered)
        )

        return filtered

    async def _analyze_content(
        self, content: List[ScrapedContent]
    ) -> List[AnalyzedContent]:
        """Analyze content with LLM"""
        if not content:
            return []

        if self.config.no_analysis:
            # Convert to AnalyzedContent with defaults
            return [self._scraped_to_analyzed(s) for s in content]

        analyzer = ContentAnalyzer(self.llm, self.config.analysis)

        # Call the proper analyze_content method with the research query
        try:
            analyzed = await analyzer.analyze_content(content, self.current_job.query)

            # Save analysis to database
            for result in analyzed:
                self.job_db.save_analysis(
                    url=result.url,
                    relevance_score=result.relevance_score,
                    key_points=result.key_points,
                    content_type=result.content_type,
                    analysis_data={
                        "summary": result.summary,
                        "metadata": result.analysis_metadata,
                    },
                )

            # Sort by relevance
            analyzed.sort(key=lambda x: x.relevance_score, reverse=True)

            # Update stats
            self.job_manager.update_job_stats(
                self.current_job,
                analyzed_urls=len(analyzed),
            )

            return analyzed

        except Exception as e:
            logger.error(f"Error analyzing content: {e}")
            # Fallback to basic conversion
            return [self._scraped_to_analyzed(s) for s in content]

    async def _expand_queries(self, query: str) -> List[str]:
        """Expand query into multiple search variations"""

        # Check for custom queries first
        if self.config.custom_queries:
            # Validate custom queries
            valid_queries = [q for q in self.config.custom_queries if q and q.strip()]
            if not valid_queries:
                logger.warning("No valid custom queries provided, using original")
                return [query]
            logger.info(f"Using {len(valid_queries)} custom queries")
            return valid_queries

        # Check for interactive query mode
        if self.config.interactive_queries:
            logger.info("Entering interactive query mode...")
            return await self._prompt_for_queries(query)

        # Check if expansion should be skipped (max_queries=1)
        if (
            hasattr(self.config.workflow, "max_queries")
            and self.config.workflow.max_queries == 1
        ):
            logger.info("Query expansion disabled (max_queries=1)")
            return [query]

        # Normal expansion
        logger.info("Expanding search query...")

        try:
            expanded = await self.query_expander.expand_query(query)

            # Store expanded queries in database
            if hasattr(self.job_manager.main_db, "update_expanded_queries"):
                self.job_manager.main_db.update_expanded_queries(
                    self.current_job.job_id, expanded.expanded_queries
                )

            logger.info(f"Generated {len(expanded.expanded_queries)} query variations")
            return expanded.expanded_queries

        except Exception as e:
            logger.error(f"Query expansion failed: {e}")
            return [query]  # Fallback to original query

    async def _prompt_for_queries(self, original_query: str) -> List[str]:
        """Interactively prompt for custom query variations"""
        from shared.colors import info, success, dim

        queries = []
        info(f"\nOriginal query: {original_query}")
        info("Enter custom query variations (one per line, empty line to finish):")

        try:
            import sys

            line_num = 1
            while True:
                # Show prompt
                sys.stdout.write(f"{dim(f'{line_num}>')} ")
                sys.stdout.flush()

                # Read input
                line = input().strip()

                if not line:
                    # Empty line means done
                    break

                queries.append(line)
                line_num += 1

            if not queries:
                info("No custom queries entered, using original query")
                return [original_query]

            success(f"Using {len(queries)} custom queries")
            return queries

        except (EOFError, KeyboardInterrupt):
            info("\nInput cancelled, using original query")
            return [original_query]

    async def _collect_urls_phased(
        self, queries: List[str], urls_file: Optional[Path]
    ) -> List[str]:
        """Collect URLs for multiple queries"""
        all_urls = []

        # Add URLs from file if provided
        if urls_file:
            added = await self.url_manager.add_urls_from_file(urls_file)
            logger.info(f"Added {added} URLs from file")

        # Search with each query
        if not self.config.dry_run and self.llm:
            for i, query in enumerate(queries):
                logger.info(f"Searching with query {i+1}/{len(queries)}: {query}")

                if self.progress_callback:
                    self.progress_callback("searching", i, len(queries))

                try:
                    llm_urls = await self.llm.search_web(
                        query,
                        self.config.scraping.search_limit
                        // len(queries),  # Distribute limit
                    )
                    added = self.url_manager.add_urls_from_list(
                        llm_urls, source=f"llm_q{i+1}"
                    )
                    logger.info(f"Added {added} URLs from query {i+1}")

                except Exception as e:
                    logger.error(f"Error searching with query {i+1}: {e}")

        # Get all unscraped URLs
        all_urls = self.url_manager.get_unscraped_urls()
        logger.info(f"Total URLs collected: {len(all_urls)}")

        # Update stats
        self.job_manager.update_job_stats(
            self.current_job, total_urls=self.job_db.get_stats()["total_urls"]
        )

        # Apply limit if configured
        if (
            self.config.scraping.scrape_limit
            and len(all_urls) > self.config.scraping.scrape_limit
        ):
            all_urls = all_urls[: self.config.scraping.scrape_limit]
            logger.info(f"Limited to {len(all_urls)} URLs")

        return all_urls

    async def _review_urls(self, urls: List[str]) -> List[str]:
        """Interactive URL review"""
        logger.info("Starting interactive URL review...")

        # Load URLs into reviewer
        url_data = [{"url": url} for url in urls]
        self.url_reviewer.load_urls(url_data)

        # Run interactive review
        reviewed_urls, confirmed = await self.url_reviewer.interactive_review()

        if not confirmed:
            logger.warning("URL review cancelled by user")
            return []

        # Extract just the URLs
        return [item["url"] for item in reviewed_urls]

    async def _scrape_urls_with_depth(self, urls: List[str]) -> List[ScrapedContent]:
        """Scrape URLs with deep crawling support"""
        if self.config.dry_run:
            logger.info("DRY RUN: Would scrape URLs")
            return []

        scraped_content = []

        # Check if deep crawling is enabled
        crawl_depth = (
            getattr(self.config.workflow, "crawl_depth", 0)
            if hasattr(self.config, "workflow")
            else 0
        )

        if crawl_depth > 0:
            logger.info(f"Deep crawling enabled with depth {crawl_depth}")

            # Process initial URLs with deep crawler
            async def scrape_callback(url):
                # Use existing scraper
                async with EnhancedSmartScraper(
                    self.config.scraping, self.job_db, self.url_manager
                ) as scraper:
                    results = await scraper.scrape_urls([url])
                    return results[0] if results else None

            # Run deep crawl
            crawl_results = await self.deep_crawler.process_crawl_queue(
                urls, scrape_callback
            )

            # Get all discovered URLs
            all_urls = list(crawl_results.keys())

            # Log crawl statistics
            stats = self.deep_crawler.get_crawl_statistics()
            logger.info(f"Deep crawl statistics: {stats}")
        else:
            all_urls = urls

        # Regular scraping (handles both deep and normal)
        scraped_content = await self._scrape_urls(all_urls)

        return scraped_content

    async def _generate_analysis(self, bundle_path: Path, query: str) -> Optional[Path]:
        """Generate separate analysis document"""
        logger.info("Generating research analysis...")

        try:
            # Set output directory
            self.analysis_generator.output_dir = Path(self.current_job.output_dir)

            # Generate analysis
            analysis_type = (
                getattr(self.config.workflow, "analysis_type", "summary")
                if hasattr(self.config, "workflow")
                else "summary"
            )

            result = await self.analysis_generator.generate_analysis(
                bundle_path, query, analysis_type=analysis_type
            )

            if result.error:
                logger.error(f"Analysis generation failed: {result.error}")
                return None

            # Save analysis
            analysis_path = Path(self.current_job.output_dir) / "research_analysis.md"
            with open(analysis_path, "w", encoding="utf-8") as f:
                f.write(result.content)

            logger.info(f"Analysis saved to: {analysis_path}")
            return analysis_path

        except Exception as e:
            logger.error(f"Error generating analysis: {e}")
            return None

    async def _create_bundle(self, content: List[AnalyzedContent], query: str) -> Path:
        """Create the final research bundle"""
        if self.config.dry_run:
            logger.info("DRY RUN: Would create bundle")
            return Path(self.current_job.output_dir)

        output_dir = Path(self.current_job.output_dir)

        # Create bundle
        bundle_creator = SmartBundleCreator(
            llm_provider=self.llm if not self.config.no_analysis else None,
            config=self.config.output,
            research_config=self.config,
        )

        bundle_path = await bundle_creator.create_bundle(
            content, query, output_dir, synthesis=None  # TODO: Add synthesis generation
        )

        # Create prominent bundle file
        await self._create_prominent_bundle(output_dir, content, query)

        logger.info(f"Bundle created at: {bundle_path}")
        return bundle_path

    async def _create_prominent_bundle(
        self, output_dir: Path, content: List[AnalyzedContent], query: str
    ):
        """Create the prominent research_bundle.md file"""
        bundle_path = output_dir / "research_bundle.md"

        # Create header
        bundle_content = f"""# Research Bundle: {query}

**Generated**: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}  
**Job ID**: {self.current_job.job_id}  
**Total Sources**: {len(content)}

---

## Research Summary

This research bundle contains {len(content)} carefully selected sources about "{query}".

"""

        # Add table of contents
        bundle_content += "## Table of Contents\n\n"
        for i, item in enumerate(content, 1):
            title = item.title or f"Source {i}"
            bundle_content += f"{i}. [{title}](#{i}-{self._slugify(title)})\n"

        bundle_content += "\n---\n\n"

        # Add all content
        for i, item in enumerate(content, 1):
            title = item.title or f"Source {i}"
            bundle_content += f"## {i}. {title}\n\n"
            bundle_content += f"**Source**: {item.url}\n"

            if hasattr(item, "relevance_score"):
                bundle_content += f"**Relevance**: {item.relevance_score}/10\n"

            if hasattr(item, "key_points") and item.key_points:
                bundle_content += "\n### Key Points:\n"
                for point in item.key_points:
                    bundle_content += f"- {point}\n"

            bundle_content += f"\n### Content:\n\n{item.content}\n\n"
            bundle_content += "---\n\n"

        # Write bundle
        with safe_open(bundle_path, "w", encoding="utf-8") as f:
            if f:
                f.write(bundle_content)

        logger.info(f"Created prominent bundle: {bundle_path}")

        # Also create research summary
        summary_path = output_dir / "research_summary.md"
        summary_content = f"""# Research Summary: {query}

**Job ID**: {self.current_job.job_id}  
**Date**: {datetime.now().strftime('%Y-%m-%d')}

## Overview

Research on "{query}" yielded {len(content)} high-quality sources.

## Top Sources

"""

        for i, item in enumerate(content[:5], 1):  # Top 5
            summary_content += f"{i}. **{item.title}**\n"
            if hasattr(item, "summary"):
                summary_content += f"   - {item.summary[:200]}...\n"
            summary_content += f"   - [Link]({item.url})\n\n"

        with safe_open(summary_path, "w", encoding="utf-8") as f:
            if f:
                f.write(summary_content)

    def _scraped_to_analyzed(self, scraped: ScrapedContent) -> AnalyzedContent:
        """Convert ScrapedContent to AnalyzedContent"""
        return AnalyzedContent(
            url=scraped.url,
            title=scraped.title,
            content=scraped.content,
            relevance_score=5.0,  # Default
            key_points=[],
            summary="",
            content_type="unknown",
            analysis_metadata={},
        )

    def _basic_html_to_markdown(self, html: str) -> str:
        """Basic HTML to Markdown conversion"""
        import re

        # Remove script and style tags
        html = re.sub(
            r"<script[^>]*>.*?</script>", "", html, flags=re.DOTALL | re.IGNORECASE
        )
        html = re.sub(
            r"<style[^>]*>.*?</style>", "", html, flags=re.DOTALL | re.IGNORECASE
        )

        # Basic conversions
        conversions = [
            (r"<h1[^>]*>(.*?)</h1>", r"# \1\n"),
            (r"<h2[^>]*>(.*?)</h2>", r"## \1\n"),
            (r"<h3[^>]*>(.*?)</h3>", r"### \1\n"),
            (r"<p[^>]*>(.*?)</p>", r"\1\n\n"),
            (r"<strong[^>]*>(.*?)</strong>", r"**\1**"),
            (r"<b[^>]*>(.*?)</b>", r"**\1**"),
            (r"<em[^>]*>(.*?)</em>", r"*\1*"),
            (r"<i[^>]*>(.*?)</i>", r"*\1*"),
            (r'<a[^>]+href="([^"]+)"[^>]*>(.*?)</a>', r"[\2](\1)"),
            (r"<br[^>]*>", "\n"),
            (r"<[^>]+>", ""),  # Remove remaining tags
        ]

        for pattern, replacement in conversions:
            html = re.sub(pattern, replacement, html, flags=re.IGNORECASE | re.DOTALL)

        # Clean up
        html = re.sub(r"\n{3,}", "\n\n", html)
        return html.strip()

    def _slugify(self, text: str) -> str:
        """Create URL-safe slug from text"""
        import re

        text = re.sub(r"[^\w\s-]", "", text.lower())
        text = re.sub(r"[-\s]+", "-", text)
        return text[:50]

    def _create_empty_result(self) -> ResearchResult:
        """Create empty result when no URLs found"""
        return ResearchResult(
            query=self.current_job.query if self.current_job else "",
            job_id=self.current_job.job_id if self.current_job else "",
            urls_found=0,
            scraped_content=[],
            analyzed_content=[],
            bundle_path=(
                Path(self.current_job.output_dir) if self.current_job else Path()
            ),
            bundle_created=False,
            output_dir=(
                Path(self.current_job.output_dir) if self.current_job else Path()
            ),
        )

    async def get_job_status(self, job_id: str) -> Dict[str, Any]:
        """Get status of a research job"""
        job = self.job_manager.get_job(job_id)
        if not job:
            return {"error": f"Job {job_id} not found"}

        return self.job_manager.get_job_info(job)

    async def list_jobs(self, status: Optional[str] = None) -> List[Dict[str, Any]]:
        """List all research jobs"""
        return self.job_manager.list_jobs(status)

    def cleanup_databases(self):
        """Clean up database connections to prevent file locking issues on Windows"""
        try:
            if self.job_db:
                self.job_db.cleanup()
                logger.debug("Cleaned up job database connections")

            if self.job_manager and hasattr(self.job_manager, "main_db"):
                self.job_manager.main_db.cleanup()
                logger.debug("Cleaned up main database connections")

            # Force garbage collection to ensure connections are closed
            import gc

            gc.collect()

        except Exception as e:
            logger.warning(f"Error during database cleanup: {e}")

    def __del__(self):
        """Ensure cleanup when object is destroyed"""
        try:
            self.cleanup_databases()
        except Exception:
            pass  # Ignore errors during cleanup in destructor

======= output.py ======
# Copyright 2025 Franz und Franz GmbH
#
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#     http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.

"""
Enhanced output formatting for m1f-research CLI
"""

import sys
import json
from typing import Any, Dict, List, Optional
from datetime import datetime
import shutil
from pathlib import Path

# Use unified colorama module
from shared.colors import Colors, COLORAMA_AVAILABLE


class OutputFormatter:
    """Handles formatted output for m1f-research"""

    def __init__(self, format: str = "text", verbose: int = 0, quiet: bool = False):
        self.format = format
        self.verbose = verbose
        self.quiet = quiet

        # Disable colors if not TTY or if requested
        if not sys.stdout.isatty() or format == "json":
            Colors.disable()

        # Track if we're in JSON mode
        self._json_buffer = [] if format == "json" else None

    def print(self, message: str = "", level: str = "info", end: str = "\n", **kwargs):
        """Print a message with appropriate formatting"""
        if self.quiet and level != "error":
            return

        if self.format == "json":
            self._json_buffer.append(
                {
                    "type": "message",
                    "level": level,
                    "message": message,
                    "timestamp": datetime.now().isoformat(),
                    **kwargs,
                }
            )
        else:
            print(message, end=end)

    def success(self, message: str, **kwargs):
        """Print success message"""
        if self.format == "json":
            self._json_buffer.append({"type": "success", "message": message, **kwargs})
        else:
            self.print(f"{Colors.GREEN}âœ… {message}{Colors.RESET}")

    def error(self, message: str, suggestion: Optional[str] = None, **kwargs):
        """Print error message with optional suggestion"""
        if self.format == "json":
            self._json_buffer.append(
                {
                    "type": "error",
                    "message": message,
                    "suggestion": suggestion,
                    **kwargs,
                }
            )
        else:
            self.print(f"{Colors.RED}âŒ Error: {message}{Colors.RESET}", level="error")
            if suggestion:
                self.print(f"{Colors.YELLOW}ðŸ’¡ Suggestion: {suggestion}{Colors.RESET}")

    def warning(self, message: str, **kwargs):
        """Print warning message"""
        if self.format == "json":
            self._json_buffer.append({"type": "warning", "message": message, **kwargs})
        else:
            self.print(f"{Colors.YELLOW}âš ï¸  {message}{Colors.RESET}")

    def info(self, message: str, **kwargs):
        """Print info message"""
        if self.format == "json":
            self._json_buffer.append({"type": "info", "message": message, **kwargs})
        else:
            self.print(f"{Colors.CYAN}â„¹ï¸  {message}{Colors.RESET}")

    def debug(self, message: str, **kwargs):
        """Print debug message (only if verbose)"""
        if self.verbose < 2:
            return

        if self.format == "json":
            self._json_buffer.append({"type": "debug", "message": message, **kwargs})
        else:
            self.print(f"{Colors.BRIGHT_BLACK}ðŸ” {message}{Colors.RESET}")

    def header(self, title: str, subtitle: Optional[str] = None):
        """Print a section header"""
        if self.format == "json":
            self._json_buffer.append(
                {"type": "header", "title": title, "subtitle": subtitle}
            )
        else:
            self.print()
            self.print(f"{Colors.BOLD}{Colors.BLUE}{title}{Colors.RESET}")
            if subtitle:
                self.print(f"{Colors.DIM}{subtitle}{Colors.RESET}")
            self.print()

    def progress(self, current: int, total: int, message: str = ""):
        """Show progress bar"""
        if self.quiet or self.format == "json":
            return

        # Calculate percentage
        percentage = (current / total * 100) if total > 0 else 0

        # Terminal width
        term_width = shutil.get_terminal_size().columns
        bar_width = min(40, term_width - 30)

        # Build progress bar
        filled = int(bar_width * current / total) if total > 0 else 0
        bar = "â–ˆ" * filled + "â–‘" * (bar_width - filled)

        # Build message
        msg = f"\r{Colors.CYAN}[{bar}] {percentage:>5.1f}% {message}{Colors.RESET}"

        # Print without newline
        sys.stdout.write(msg)
        sys.stdout.flush()

        # Add newline when complete
        if current >= total:
            self.print()

    def table(
        self,
        headers: List[str],
        rows: List[List[str]],
        highlight_search: Optional[str] = None,
    ):
        """Print a formatted table"""
        if self.format == "json":
            self._json_buffer.append(
                {"type": "table", "headers": headers, "rows": rows}
            )
            return

        # Calculate column widths
        widths = [len(h) for h in headers]
        for row in rows:
            for i, cell in enumerate(row):
                widths[i] = max(widths[i], len(str(cell)))

        # Ensure we don't exceed terminal width
        term_width = shutil.get_terminal_size().columns
        total_width = sum(widths) + len(widths) * 3 - 1

        if total_width > term_width:
            # Scale down widths proportionally
            scale = term_width / total_width
            widths = [int(w * scale) for w in widths]

        # Print header
        header_line = " | ".join(h.ljust(w)[:w] for h, w in zip(headers, widths))
        self.print(f"{Colors.BOLD}{header_line}{Colors.RESET}")
        self.print("-" * len(header_line))

        # Print rows
        for row in rows:
            row_cells = []
            for cell, width in zip(row, widths):
                cell_str = str(cell)[:width].ljust(width)

                # Highlight search term if present
                if highlight_search and highlight_search.lower() in cell_str.lower():
                    cell_str = cell_str.replace(
                        highlight_search,
                        f"{Colors.YELLOW}{Colors.BOLD}{highlight_search}{Colors.RESET}",
                    )

                row_cells.append(cell_str)

            self.print(" | ".join(row_cells))

    def job_status(self, job: Dict[str, Any]):
        """Print formatted job status"""
        if self.format == "json":
            self._json_buffer.append({"type": "job_status", "job": job})
            return

        self.header(f"ðŸ“‹ Job Status: {job['job_id']}")

        # Basic info
        self.print(f"{Colors.BOLD}Query:{Colors.RESET} {job['query']}")

        # Color-code status
        status = job["status"]
        if status == "completed":
            status_colored = f"{Colors.GREEN}{status}{Colors.RESET}"
        elif status == "active":
            status_colored = f"{Colors.YELLOW}{status}{Colors.RESET}"
        else:
            status_colored = f"{Colors.RED}{status}{Colors.RESET}"

        self.print(f"{Colors.BOLD}Status:{Colors.RESET} {status_colored}")

        self.print(f"{Colors.BOLD}Created:{Colors.RESET} {job['created_at']}")
        self.print(f"{Colors.BOLD}Updated:{Colors.RESET} {job['updated_at']}")
        self.print(f"{Colors.BOLD}Output:{Colors.RESET} {job['output_dir']}")

        # Statistics
        self.print(f"\n{Colors.BOLD}Statistics:{Colors.RESET}")
        stats = job["stats"]
        self.print(f"  Total URLs: {stats['total_urls']}")
        self.print(f"  Scraped: {stats['scraped_urls']}")
        self.print(f"  Filtered: {stats['filtered_urls']}")
        self.print(f"  Analyzed: {stats['analyzed_urls']}")

        if job.get("bundle_exists"):
            self.print(f"\n{Colors.GREEN}âœ… Research bundle available{Colors.RESET}")

    def list_item(self, item: str, indent: int = 0):
        """Print a list item"""
        if self.format == "json":
            self._json_buffer.append(
                {"type": "list_item", "item": item, "indent": indent}
            )
        else:
            prefix = "  " * indent + "â€¢ "
            self.print(f"{prefix}{item}")

    def confirm(self, prompt: str, default: bool = False) -> bool:
        """Ask for user confirmation"""
        if self.format == "json" or self.quiet:
            return default

        suffix = " [Y/n]" if default else " [y/N]"
        response = input(f"{Colors.YELLOW}{prompt}{suffix}: {Colors.RESET}").lower()

        if not response:
            return default

        return response in ("y", "yes")

    def get_json_output(self) -> str:
        """Get JSON output (for JSON format)"""
        if self.format != "json":
            return ""

        return json.dumps(self._json_buffer, indent=2)

    def cleanup(self):
        """Clean up and output JSON if needed"""
        if self.format == "json" and self._json_buffer:
            print(self.get_json_output())


class ProgressTracker:
    """Track and display progress for long operations"""

    def __init__(self, formatter: OutputFormatter, total: int, message: str = ""):
        self.formatter = formatter
        self.total = total
        self.current = 0
        self.message = message
        self.start_time = datetime.now()

    def update(self, increment: int = 1, message: Optional[str] = None):
        """Update progress"""
        self.current += increment
        if message:
            self.message = message

        # Calculate ETA
        if self.current > 0:
            elapsed = (datetime.now() - self.start_time).total_seconds()
            rate = self.current / elapsed
            remaining = (self.total - self.current) / rate if rate > 0 else 0
            eta = f" ETA: {int(remaining)}s" if remaining > 1 else ""
        else:
            eta = ""

        self.formatter.progress(self.current, self.total, f"{self.message}{eta}")

    def complete(self, message: Optional[str] = None):
        """Mark as complete"""
        self.current = self.total
        if message:
            self.message = message
        self.formatter.progress(self.current, self.total, self.message)

======= prompt_utils.py ======
# Copyright 2025 Franz und Franz GmbH
#
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#     http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.

"""
Prompt utilities for m1f-research using shared prompt loader
"""

from pathlib import Path
from shared.prompts import PromptLoader, format_prompt

# Initialize loader with research-specific prompts
_loader = PromptLoader(
    [
        Path(__file__).parent.parent / "shared" / "prompts" / "research",
        Path(__file__).parent / "prompts",  # Fallback to local prompts if any
    ]
)


def get_web_search_prompt(query: str, num_results: int = 20) -> str:
    """Get formatted web search prompt."""
    return _loader.format("llm/web_search.md", query=query, num_results=num_results)


def get_analysis_prompt(
    template_name: str, prompt_type: str, query: str, url: str, content: str
) -> str:
    """Get formatted analysis prompt for a specific template."""
    # Try template-specific prompt first
    prompt_name = f"analysis/{template_name}_{prompt_type}.md"

    # Set appropriate fallback - always use general as fallback since it exists
    fallback_name = f"analysis/general_{prompt_type}.md"

    try:
        base_prompt = _loader.load_with_fallback(prompt_name, fallback_name)
    except FileNotFoundError:
        # Ultimate fallback
        base_prompt = _loader.load("analysis/default_analysis.md")

    # For template-specific prompts, we need to add the full analysis structure
    if "Return ONLY valid JSON" not in base_prompt:
        analysis_template = _loader.load("analysis/default_analysis.md")
        # Replace the focus section with template-specific content
        base_prompt = (
            f"{base_prompt}\n\nURL: {{url}}\n\nContent:\n{{content}}\n\n"
            + analysis_template.split("Content:")[1].strip()
        )

    # Only pass url if the template contains {url}
    if "{url}" in base_prompt:
        return format_prompt(base_prompt, query=query, url=url, content=content)
    else:
        return format_prompt(base_prompt, query=query, content=content)


def get_synthesis_prompt(query: str, summaries: str) -> str:
    """Get formatted synthesis prompt."""
    return _loader.format("analysis/synthesis.md", query=query, summaries=summaries)


def get_subtopic_grouping_prompt(query: str, summaries: str) -> str:
    """Get formatted subtopic grouping prompt."""
    return _loader.format(
        "bundle/subtopic_grouping.md", query=query, summaries=summaries
    )


def get_topic_summary_prompt(topic: str, summaries: str) -> str:
    """Get formatted topic summary prompt."""
    return _loader.format("bundle/topic_summary.md", topic=topic, summaries=summaries)

======= query_expander.py ======
# Copyright 2025 Franz und Franz GmbH
#
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#     http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.

"""
Query expansion for comprehensive research coverage
"""

import json
import logging
from typing import List, Optional, Dict, Any
from dataclasses import dataclass

logger = logging.getLogger(__name__)


@dataclass
class ExpandedQuery:
    """Represents an expanded search query"""

    original_query: str
    expanded_queries: List[str]
    expansion_metadata: Dict[str, Any]


class QueryExpander:
    """Expands research queries into multiple search variations"""

    def __init__(self, llm_provider=None, max_queries: int = 5):
        """
        Initialize the query expander

        Args:
            llm_provider: LLM provider instance for query expansion
            max_queries: Maximum number of expanded queries to generate
        """
        self.llm_provider = llm_provider
        self.max_queries = max_queries

    async def expand_query(
        self, query: str, context: Optional[str] = None
    ) -> ExpandedQuery:
        """
        Expand a single query into multiple search variations

        Args:
            query: The original research query
            context: Optional context about the research goals

        Returns:
            ExpandedQuery object with original and expanded queries
        """
        logger.info(f"Expanding query: {query}")

        # If no LLM provider, return original query only
        if not self.llm_provider:
            logger.warning("No LLM provider configured, returning original query only")
            return ExpandedQuery(
                original_query=query,
                expanded_queries=[query],
                expansion_metadata={"method": "no_expansion"},
            )

        try:
            # Create expansion prompt
            prompt = self._create_expansion_prompt(query, context)

            # Call LLM for expansion
            response = await self.llm_provider.query(prompt)

            if response.error:
                logger.error(f"LLM error during query expansion: {response.error}")
                return self._fallback_expansion(query)

            # Parse expanded queries
            expanded_queries = self._parse_expansion_response(response.content, query)

            # Ensure we don't exceed max_queries
            if len(expanded_queries) > self.max_queries:
                expanded_queries = expanded_queries[: self.max_queries]

            # Always include original query if not already present
            if query not in expanded_queries:
                expanded_queries.insert(0, query)

            logger.info(f"Generated {len(expanded_queries)} query variations")

            return ExpandedQuery(
                original_query=query,
                expanded_queries=expanded_queries,
                expansion_metadata={
                    "method": "llm_expansion",
                    "provider": self.llm_provider.__class__.__name__,
                    "query_count": len(expanded_queries),
                },
            )

        except Exception as e:
            logger.error(f"Error during query expansion: {e}")
            return self._fallback_expansion(query)

    def _create_expansion_prompt(
        self, query: str, context: Optional[str] = None
    ) -> str:
        """Create the prompt for query expansion"""
        context_str = f"\nResearch context: {context}" if context else ""

        prompt = f"""Generate {self.max_queries} different search queries related to: "{query}"{context_str}

Create variations that will help find comprehensive information about this topic. Include:
- Different phrasings of the same concept
- Related subtopics and aspects
- Technical and non-technical variations
- Specific and broad versions

Return ONLY a JSON array of search queries, no explanation:
["query 1", "query 2", "query 3", ...]

Important:
- Each query should be a complete search phrase
- Avoid duplicates
- Keep queries relevant to the original topic
- Make queries that would work well in web search"""

        return prompt

    def _parse_expansion_response(
        self, response: str, original_query: str
    ) -> List[str]:
        """Parse the LLM response to extract expanded queries"""
        try:
            # Try to extract JSON array from response
            response = response.strip()

            # Handle various response formats
            if response.startswith("```json"):
                response = response[7:]
                if response.endswith("```"):
                    response = response[:-3]
            elif response.startswith("```"):
                response = response[3:]
                if response.endswith("```"):
                    response = response[:-3]

            # Find JSON array in response
            start_idx = response.find("[")
            end_idx = response.rfind("]") + 1

            if start_idx != -1 and end_idx > start_idx:
                response = response[start_idx:end_idx]

            # Parse JSON
            queries = json.loads(response)

            if not isinstance(queries, list):
                raise ValueError("Response is not a JSON array")

            # Validate and clean queries
            valid_queries = []
            seen = set()

            for q in queries:
                if isinstance(q, str) and q.strip():
                    clean_q = q.strip()
                    # Avoid duplicates (case-insensitive)
                    if clean_q.lower() not in seen:
                        valid_queries.append(clean_q)
                        seen.add(clean_q.lower())

            return valid_queries

        except (json.JSONDecodeError, ValueError) as e:
            logger.error(f"Failed to parse expansion response: {e}")
            # Try to extract queries as lines if JSON parsing fails
            lines = response.split("\n")
            queries = []
            for line in lines:
                line = line.strip()
                if line and not line.startswith("#") and not line.startswith("//"):
                    # Remove common prefixes like "1.", "- ", etc.
                    import re

                    line = re.sub(r"^[\d\-\*\â€¢\.]+\s*", "", line)
                    if line and line != original_query:
                        queries.append(line)

            return queries[: self.max_queries] if queries else []

    def _fallback_expansion(self, query: str) -> ExpandedQuery:
        """Fallback expansion when LLM is unavailable or fails"""
        # Simple rule-based expansion
        expanded = [query]

        # Add some basic variations
        if "how to" in query.lower():
            expanded.append(query.replace("how to", "guide to", 1))
            expanded.append(query.replace("how to", "tutorial", 1))

        if "best" in query.lower():
            expanded.append(query.replace("best", "top", 1))
            expanded.append(query.replace("best", "recommended", 1))

        # Add year if not present
        import datetime

        current_year = datetime.datetime.now().year
        if str(current_year) not in query and str(current_year - 1) not in query:
            expanded.append(f"{query} {current_year}")

        # Remove duplicates
        expanded = list(dict.fromkeys(expanded))[: self.max_queries]

        return ExpandedQuery(
            original_query=query,
            expanded_queries=expanded,
            expansion_metadata={"method": "fallback_expansion"},
        )

    def combine_results(self, expanded_queries: List[ExpandedQuery]) -> List[str]:
        """
        Combine multiple expanded query results into a single list

        Args:
            expanded_queries: List of ExpandedQuery objects

        Returns:
            Deduplicated list of all queries
        """
        all_queries = []
        seen = set()

        for eq in expanded_queries:
            for q in eq.expanded_queries:
                if q.lower() not in seen:
                    all_queries.append(q)
                    seen.add(q.lower())

        return all_queries

======= readme_generator.py ======
# Copyright 2025 Franz und Franz GmbH
#
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#     http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.

"""
README generator for research bundles
"""
import logging
from pathlib import Path
from typing import List, Dict, Any, Optional
from datetime import datetime
import json

from m1f.file_operations import (
    safe_open,
)

from .models import AnalyzedContent
from .config import ResearchConfig

logger = logging.getLogger(__name__)


class ReadmeGenerator:
    """
    Generate comprehensive README files for research bundles with:
    - Executive summary
    - Key findings
    - Source overview
    - Usage instructions
    - Citation information
    """

    def __init__(self, config: ResearchConfig):
        self.config = config

    async def generate_readme(
        self,
        content_list: List[AnalyzedContent],
        research_query: str,
        output_dir: Path,
        topic_groups: Optional[Dict[str, List[AnalyzedContent]]] = None,
        synthesis: Optional[str] = None,
    ) -> Path:
        """
        Generate a README.md file for the research bundle

        Args:
            content_list: List of analyzed content
            research_query: Original research query
            output_dir: Directory containing the bundle
            topic_groups: Optional topic groupings
            synthesis: Optional research synthesis

        Returns:
            Path to the generated README file
        """
        readme_path = output_dir / "README.md"

        lines = []

        # Title and description
        lines.append(f"# Research Bundle: {research_query}")
        lines.append("")
        lines.append(
            f"Generated on {datetime.now().strftime('%Y-%m-%d %H:%M:%S')} using m1f-research"
        )
        lines.append("")

        # Quick stats
        lines.append("## Quick Stats")
        lines.append("")
        lines.append(f"- **Total Sources**: {len(content_list)}")
        if topic_groups:
            lines.append(f"- **Topics Covered**: {len(topic_groups)}")

        avg_relevance = (
            sum(item.relevance_score for item in content_list) / len(content_list)
            if content_list
            else 0
        )
        lines.append(f"- **Average Relevance**: {avg_relevance:.1f}/10")

        # Content type distribution
        content_types = {}
        for item in content_list:
            ct = item.content_type or "unknown"
            content_types[ct] = content_types.get(ct, 0) + 1

        lines.append(
            f"- **Content Types**: {', '.join(f'{k} ({v})' for k, v in content_types.items())}"
        )
        lines.append("")

        # Executive summary
        lines.append("## Executive Summary")
        lines.append("")

        if synthesis:
            lines.append(synthesis)
        else:
            lines.append(
                f"This research bundle contains {len(content_list)} curated sources about '{research_query}'. "
            )
            lines.append(
                "The sources have been analyzed for relevance and organized for easy navigation."
            )
        lines.append("")

        # Key findings
        if content_list:
            lines.append("## Key Findings")
            lines.append("")

            # Top 3 most relevant sources
            top_sources = sorted(
                content_list, key=lambda x: x.relevance_score, reverse=True
            )[:3]
            lines.append("### Most Relevant Sources")
            lines.append("")
            for i, source in enumerate(top_sources, 1):
                lines.append(
                    f"{i}. **[{source.title}]({source.url})** (Relevance: {source.relevance_score}/10)"
                )
                if source.summary:
                    lines.append(f"   - {source.summary[:150]}...")
                lines.append("")

            # Common themes
            if topic_groups and len(topic_groups) > 1:
                lines.append("### Main Topics")
                lines.append("")
                for topic, items in list(topic_groups.items())[:5]:
                    lines.append(f"- **{topic}**: {len(items)} sources")
                lines.append("")

        # How to use this bundle
        lines.append("## How to Use This Bundle")
        lines.append("")
        lines.append("1. **Quick Overview**: Start with the executive summary above")
        lines.append("2. **Deep Dive**: Open `research-bundle.md` for the full content")
        lines.append(
            "3. **Navigation**: Use the table of contents to jump to specific sources"
        )
        lines.append(
            "4. **By Topic**: Sources are organized by subtopic for logical flow"
        )
        lines.append("5. **Metadata**: Check `metadata.json` for additional details")
        lines.append("")

        # Source overview
        lines.append("## Source Overview")
        lines.append("")

        if topic_groups:
            for topic, items in topic_groups.items():
                lines.append(f"### {topic}")
                lines.append("")
                for item in items[:3]:  # Show top 3 per topic
                    lines.append(
                        f"- [{item.title}]({item.url}) - {item.relevance_score}/10"
                    )
                if len(items) > 3:
                    lines.append(f"- ...and {len(items) - 3} more")
                lines.append("")
        else:
            # Simple list if no topic groups
            for item in content_list[:10]:
                lines.append(
                    f"- [{item.title}]({item.url}) - {item.relevance_score}/10"
                )
            if len(content_list) > 10:
                lines.append(f"- ...and {len(content_list) - 10} more sources")
            lines.append("")

        # Research methodology
        lines.append("## Research Methodology")
        lines.append("")
        lines.append("This research was conducted using the following approach:")
        lines.append("")
        lines.append(
            f"1. **Search**: Found {self.config.url_count} potential sources using {self.config.llm.provider}"
        )
        lines.append(
            f"2. **Scrape**: Downloaded content from top {self.config.scrape_count} URLs"
        )
        lines.append(f"3. **Filter**: Applied quality and relevance filters")
        if not self.config.no_analysis:
            lines.append(
                f"4. **Analyze**: Used LLM to score relevance and extract key points"
            )
            lines.append(f"5. **Organize**: Grouped content by topics for logical flow")
        lines.append("")

        # Configuration used
        lines.append("### Configuration")
        lines.append("")
        lines.append("```yaml")
        lines.append(f"provider: {self.config.llm.provider}")
        lines.append(f"relevance_threshold: {self.config.analysis.relevance_threshold}")
        lines.append(f"min_content_length: {self.config.analysis.min_content_length}")
        if hasattr(self.config, "template") and self.config.template:
            lines.append(f"template: {self.config.template}")
        lines.append("```")
        lines.append("")

        # Files in this bundle
        lines.append("## Files in This Bundle")
        lines.append("")
        lines.append("- `README.md` - This file")
        lines.append("- `research-bundle.md` - Complete research content")
        if self.config.output.create_index:
            lines.append("- `index.md` - Alternative navigation by topic and relevance")
        if self.config.output.include_metadata:
            lines.append("- `metadata.json` - Detailed source metadata")
            lines.append("- `search_results.json` - Original search results")
        lines.append("")

        # Citation information
        lines.append("## Citation")
        lines.append("")
        lines.append("If you use this research bundle, please cite:")
        lines.append("")
        lines.append("```")
        lines.append(f"Research Bundle: {research_query}")
        lines.append(
            f"Generated by m1f-research on {datetime.now().strftime('%Y-%m-%d')}"
        )
        lines.append(f"Sources: {len(content_list)} web resources")
        lines.append("```")
        lines.append("")

        # License and attribution
        lines.append("## License & Attribution")
        lines.append("")
        lines.append("This research bundle aggregates content from various sources. ")
        lines.append("Each source retains its original copyright and license. ")
        lines.append("Please refer to individual sources for their specific terms.")
        lines.append("")

        # Footer
        lines.append("---")
        lines.append("")
        lines.append(
            "*Generated by [m1f-research](https://github.com/m1f/m1f) - AI-powered research tool*"
        )

        # Write README
        readme_content = "\n".join(lines)
        with safe_open(readme_path, "w", encoding="utf-8") as f:
            if f:
                f.write(readme_content)

        logger.info(f"Generated README at: {readme_path}")
        return readme_path

    async def generate_citation_file(
        self, content_list: List[AnalyzedContent], research_query: str, output_dir: Path
    ):
        """Generate a CITATIONS.md file with proper citations for all sources"""
        citations_path = output_dir / "CITATIONS.md"

        lines = []
        lines.append("# Citations")
        lines.append("")
        lines.append(f"Sources used in research for: {research_query}")
        lines.append("")

        # Group by domain for organization
        by_domain = {}
        for item in content_list:
            from urllib.parse import urlparse

            domain = urlparse(item.url).netloc
            if domain not in by_domain:
                by_domain[domain] = []
            by_domain[domain].append(item)

        # Generate citations by domain
        for domain, items in sorted(by_domain.items()):
            lines.append(f"## {domain}")
            lines.append("")

            for item in sorted(items, key=lambda x: x.title):
                lines.append(f"- **{item.title}**")
                lines.append(f"  - URL: {item.url}")
                lines.append(f"  - Accessed: {datetime.now().strftime('%Y-%m-%d')}")
                lines.append(f"  - Relevance Score: {item.relevance_score}/10")
                lines.append("")

        # Write citations file
        with safe_open(citations_path, "w", encoding="utf-8") as f:
            if f:
                f.write("\n".join(lines))

        logger.info(f"Generated citations at: {citations_path}")

======= research_db.py ======
# Copyright 2025 Franz und Franz GmbH
#
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#     http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.

"""
Database management for m1f-research with dual DB system
"""
import sqlite3
import json
import uuid
from pathlib import Path
from datetime import datetime
from typing import Dict, List, Optional, Any, Tuple
from dataclasses import dataclass, asdict
import logging

logger = logging.getLogger(__name__)


@dataclass
class ResearchJob:
    """Research job data model"""

    job_id: str
    query: str
    created_at: datetime
    updated_at: datetime
    status: str  # active, completed, failed
    config: Dict[str, Any]
    output_dir: str
    phase: Optional[str] = None  # Current workflow phase
    phase_data: Optional[Dict[str, Any]] = None  # Phase-specific data

    @classmethod
    def create_new(
        cls, query: str, config: Dict[str, Any], output_dir: str
    ) -> "ResearchJob":
        """Create a new research job"""
        now = datetime.now()
        return cls(
            job_id=str(uuid.uuid4())[:8],  # Short ID for convenience
            query=query,
            created_at=now,
            updated_at=now,
            status="active",
            config=config,
            output_dir=output_dir,
        )


class ResearchDatabase:
    """Main research jobs database manager"""

    def __init__(self, db_path: Path):
        self.db_path = db_path
        self.db_path.parent.mkdir(parents=True, exist_ok=True)
        self._connection_pool = []  # Track connections for cleanup
        self._init_database()

    def _init_database(self):
        """Initialize the main research database"""
        with sqlite3.connect(str(self.db_path)) as conn:
            conn.execute(
                """
                CREATE TABLE IF NOT EXISTS jobs (
                    job_id TEXT PRIMARY KEY,
                    query TEXT NOT NULL,
                    created_at TIMESTAMP NOT NULL,
                    updated_at TIMESTAMP NOT NULL,
                    status TEXT NOT NULL,
                    config TEXT NOT NULL,
                    output_dir TEXT NOT NULL,
                    phase TEXT DEFAULT 'initialization',
                    phase_data TEXT DEFAULT '{}',
                    expanded_queries TEXT DEFAULT '[]'
                )
            """
            )

            conn.execute(
                """
                CREATE TABLE IF NOT EXISTS job_stats (
                    job_id TEXT PRIMARY KEY,
                    total_urls INTEGER DEFAULT 0,
                    scraped_urls INTEGER DEFAULT 0,
                    filtered_urls INTEGER DEFAULT 0,
                    analyzed_urls INTEGER DEFAULT 0,
                    FOREIGN KEY (job_id) REFERENCES jobs(job_id)
                )
            """
            )

            conn.commit()

    def create_job(self, job: ResearchJob) -> str:
        """Create a new research job"""
        with sqlite3.connect(str(self.db_path)) as conn:
            conn.execute(
                """INSERT INTO jobs (job_id, query, created_at, updated_at, status, config, output_dir)
                   VALUES (?, ?, ?, ?, ?, ?, ?)""",
                (
                    job.job_id,
                    job.query,
                    job.created_at.isoformat(),
                    job.updated_at.isoformat(),
                    job.status,
                    json.dumps(job.config),
                    job.output_dir,
                ),
            )

            # Initialize stats
            conn.execute("INSERT INTO job_stats (job_id) VALUES (?)", (job.job_id,))

            conn.commit()

        logger.info(f"Created new job: {job.job_id} for query: {job.query}")
        return job.job_id

    def get_job(self, job_id: str) -> Optional[ResearchJob]:
        """Get a research job by ID"""
        with sqlite3.connect(str(self.db_path)) as conn:
            conn.row_factory = sqlite3.Row
            cursor = conn.execute("SELECT * FROM jobs WHERE job_id = ?", (job_id,))
            row = cursor.fetchone()

            if row:
                job = ResearchJob(
                    job_id=row["job_id"],
                    query=row["query"],
                    created_at=datetime.fromisoformat(row["created_at"]),
                    updated_at=datetime.fromisoformat(row["updated_at"]),
                    status=row["status"],
                    config=json.loads(row["config"]),
                    output_dir=row["output_dir"],
                )
                # Add phase information if available
                if "phase" in row.keys():
                    job.phase = row["phase"]
                if "phase_data" in row.keys():
                    job.phase_data = (
                        json.loads(row["phase_data"]) if row["phase_data"] else {}
                    )
                return job

        return None

    def update_job_status(self, job_id: str, status: str):
        """Update job status"""
        with sqlite3.connect(str(self.db_path)) as conn:
            conn.execute(
                "UPDATE jobs SET status = ?, updated_at = ? WHERE job_id = ?",
                (status, datetime.now().isoformat(), job_id),
            )
            conn.commit()

    def update_job_phase(
        self, job_id: str, phase: str, phase_data: Optional[Dict] = None
    ):
        """Update job phase and optional phase-specific data"""
        with sqlite3.connect(str(self.db_path)) as conn:
            if phase_data:
                conn.execute(
                    "UPDATE jobs SET phase = ?, phase_data = ?, updated_at = ? WHERE job_id = ?",
                    (phase, json.dumps(phase_data), datetime.now().isoformat(), job_id),
                )
            else:
                conn.execute(
                    "UPDATE jobs SET phase = ?, updated_at = ? WHERE job_id = ?",
                    (phase, datetime.now().isoformat(), job_id),
                )
            conn.commit()
            logger.info(f"Updated job {job_id} to phase: {phase}")

    def update_expanded_queries(self, job_id: str, queries: List[str]):
        """Update expanded queries for a job"""
        with sqlite3.connect(str(self.db_path)) as conn:
            conn.execute(
                "UPDATE jobs SET expanded_queries = ?, updated_at = ? WHERE job_id = ?",
                (json.dumps(queries), datetime.now().isoformat(), job_id),
            )
            conn.commit()
            logger.info(f"Updated job {job_id} with {len(queries)} expanded queries")

    def update_job_stats(self, job_id: str, **stats):
        """Update job statistics"""
        with sqlite3.connect(str(self.db_path)) as conn:
            # Build dynamic update query
            updates = []
            values = []
            for key, value in stats.items():
                if key in [
                    "total_urls",
                    "scraped_urls",
                    "filtered_urls",
                    "analyzed_urls",
                ]:
                    updates.append(f"{key} = ?")
                    values.append(value)

            if updates:
                values.append(job_id)
                query = f"UPDATE job_stats SET {', '.join(updates)} WHERE job_id = ?"
                conn.execute(query, values)
                conn.commit()

    def list_jobs(
        self,
        status: Optional[str] = None,
        limit: Optional[int] = None,
        offset: int = 0,
        date_filter: Optional[str] = None,
        search_term: Optional[str] = None,
    ) -> List[Dict[str, Any]]:
        """
        List jobs with advanced filtering options

        Args:
            status: Filter by job status
            limit: Maximum number of results
            offset: Number of results to skip (for pagination)
            date_filter: Date filter in Y-M-D or Y-M format
            search_term: Search term to filter queries
        """
        with sqlite3.connect(str(self.db_path)) as conn:
            conn.row_factory = sqlite3.Row

            query = """
                SELECT j.*, s.total_urls, s.scraped_urls, s.filtered_urls, s.analyzed_urls
                FROM jobs j
                LEFT JOIN job_stats s ON j.job_id = s.job_id
                WHERE 1=1
            """
            params = []

            # Status filter
            if status:
                query += " AND j.status = ?"
                params.append(status)

            # Search term filter
            if search_term:
                query += " AND j.query LIKE ?"
                params.append(f"%{search_term}%")

            # Date filter
            if date_filter:
                if len(date_filter) == 10:  # Y-M-D format
                    query += " AND DATE(j.created_at) = ?"
                    params.append(date_filter)
                elif len(date_filter) == 7:  # Y-M format
                    query += " AND strftime('%Y-%m', j.created_at) = ?"
                    params.append(date_filter)
                elif len(date_filter) == 4:  # Y format
                    query += " AND strftime('%Y', j.created_at) = ?"
                    params.append(date_filter)

            # Order by created_at
            query += " ORDER BY j.created_at DESC"

            # Pagination
            if limit:
                query += f" LIMIT {limit} OFFSET {offset}"

            cursor = conn.execute(query, params)

            jobs = []
            for row in cursor:
                jobs.append(
                    {
                        "job_id": row["job_id"],
                        "query": row["query"],
                        "created_at": row["created_at"],
                        "updated_at": row["updated_at"],
                        "status": row["status"],
                        "output_dir": row["output_dir"],
                        "stats": {
                            "total_urls": row["total_urls"] or 0,
                            "scraped_urls": row["scraped_urls"] or 0,
                            "filtered_urls": row["filtered_urls"] or 0,
                            "analyzed_urls": row["analyzed_urls"] or 0,
                        },
                    }
                )

            return jobs

    def count_jobs(
        self,
        status: Optional[str] = None,
        date_filter: Optional[str] = None,
        search_term: Optional[str] = None,
    ) -> int:
        """Count jobs matching filters (for pagination)"""
        with sqlite3.connect(str(self.db_path)) as conn:
            query = "SELECT COUNT(*) FROM jobs WHERE 1=1"
            params = []

            if status:
                query += " AND status = ?"
                params.append(status)

            if search_term:
                query += " AND query LIKE ?"
                params.append(f"%{search_term}%")

            if date_filter:
                if len(date_filter) == 10:  # Y-M-D format
                    query += " AND DATE(created_at) = ?"
                    params.append(date_filter)
                elif len(date_filter) == 7:  # Y-M format
                    query += " AND strftime('%Y-%m', created_at) = ?"
                    params.append(date_filter)
                elif len(date_filter) == 4:  # Y format
                    query += " AND strftime('%Y', created_at) = ?"
                    params.append(date_filter)

            cursor = conn.execute(query, params)
            return cursor.fetchone()[0]

    def delete_job(self, job_id: str) -> bool:
        """Delete a job from the database"""
        try:
            with sqlite3.connect(str(self.db_path)) as conn:
                # Delete from job_stats first (foreign key constraint)
                conn.execute("DELETE FROM job_stats WHERE job_id = ?", (job_id,))

                # Delete from jobs table
                cursor = conn.execute("DELETE FROM jobs WHERE job_id = ?", (job_id,))

                conn.commit()

                # Return True if a job was actually deleted
                deleted = cursor.rowcount > 0
                if deleted:
                    logger.info(f"Deleted job {job_id} from database")
                else:
                    logger.warning(f"Job {job_id} not found in database")
                return deleted

        except Exception as e:
            logger.error(f"Error deleting job {job_id}: {e}")
            return False

    def close_all_connections(self):
        """Close all database connections"""
        try:
            # Force garbage collection to close any remaining connections
            import gc

            gc.collect()

            # On Windows, sometimes we need to explicitly close the database
            # SQLite doesn't have persistent connections with context managers,
            # but we can force cleanup
            logger.debug(f"Closing database connections for {self.db_path}")
        except Exception as e:
            logger.warning(f"Error during database cleanup: {e}")

    def cleanup(self):
        """Clean up database resources"""
        self.close_all_connections()


class JobDatabase:
    """Per-job database for URL and content tracking"""

    def __init__(self, db_path: Path):
        self.db_path = db_path
        self.db_path.parent.mkdir(parents=True, exist_ok=True)
        self._connection_pool = []  # Track connections for cleanup
        self._init_database()

    def _init_database(self):
        """Initialize the job-specific database"""
        with sqlite3.connect(str(self.db_path)) as conn:
            # URL tracking table
            conn.execute(
                """
                CREATE TABLE IF NOT EXISTS urls (
                    url TEXT PRIMARY KEY,
                    normalized_url TEXT,
                    host TEXT,
                    added_by TEXT NOT NULL,
                    added_at TIMESTAMP NOT NULL,
                    scraped_at TIMESTAMP,
                    status_code INTEGER,
                    content_checksum TEXT,
                    error_message TEXT,
                    depth INTEGER DEFAULT 0,
                    parent_url TEXT,
                    review_status TEXT DEFAULT 'pending'
                )
            """
            )

            # Content storage
            conn.execute(
                """
                CREATE TABLE IF NOT EXISTS content (
                    url TEXT PRIMARY KEY,
                    title TEXT,
                    markdown TEXT NOT NULL,
                    metadata TEXT,
                    word_count INTEGER,
                    filtered BOOLEAN DEFAULT 0,
                    filter_reason TEXT,
                    FOREIGN KEY (url) REFERENCES urls(url)
                )
            """
            )

            # Analysis results
            conn.execute(
                """
                CREATE TABLE IF NOT EXISTS analysis (
                    url TEXT PRIMARY KEY,
                    relevance_score REAL,
                    key_points TEXT,
                    content_type TEXT,
                    analysis_data TEXT,
                    analyzed_at TIMESTAMP,
                    FOREIGN KEY (url) REFERENCES urls(url)
                )
            """
            )

            # Create indexes
            conn.execute("CREATE INDEX IF NOT EXISTS idx_urls_host ON urls(host)")
            conn.execute(
                "CREATE INDEX IF NOT EXISTS idx_urls_scraped ON urls(scraped_at)"
            )
            conn.execute(
                "CREATE INDEX IF NOT EXISTS idx_content_filtered ON content(filtered)"
            )

            conn.commit()

    def add_urls(self, urls: List[Dict[str, str]], added_by: str = "llm") -> int:
        """Add URLs to the database"""
        added_count = 0

        with sqlite3.connect(str(self.db_path)) as conn:
            for url_data in urls:
                url = url_data.get("url", "")
                if not url:
                    continue

                try:
                    # Normalize URL
                    from urllib.parse import urlparse, urlunparse

                    parsed = urlparse(url)
                    normalized = urlunparse(
                        (
                            parsed.scheme.lower(),
                            parsed.netloc.lower(),
                            parsed.path.rstrip("/"),
                            parsed.params,
                            parsed.query,
                            "",
                        )
                    )

                    conn.execute(
                        """INSERT OR IGNORE INTO urls 
                           (url, normalized_url, host, added_by, added_at)
                           VALUES (?, ?, ?, ?, ?)""",
                        (
                            url,
                            normalized,
                            parsed.netloc,
                            added_by,
                            datetime.now().isoformat(),
                        ),
                    )

                    if conn.total_changes > added_count:
                        added_count = conn.total_changes

                except Exception as e:
                    logger.error(f"Error adding URL {url}: {e}")

            conn.commit()

        return added_count

    def get_unscraped_urls(self) -> List[str]:
        """Get all URLs that haven't been scraped yet"""
        with sqlite3.connect(str(self.db_path)) as conn:
            cursor = conn.execute("SELECT url FROM urls WHERE scraped_at IS NULL")
            return [row[0] for row in cursor]

    def get_urls_by_host(self) -> Dict[str, List[str]]:
        """Get URLs grouped by host"""
        with sqlite3.connect(str(self.db_path)) as conn:
            cursor = conn.execute(
                "SELECT host, url FROM urls WHERE scraped_at IS NULL ORDER BY host"
            )

            urls_by_host = {}
            for host, url in cursor:
                if host not in urls_by_host:
                    urls_by_host[host] = []
                urls_by_host[host].append(url)

            return urls_by_host

    def mark_url_scraped(
        self,
        url: str,
        status_code: int,
        content_checksum: Optional[str] = None,
        error_message: Optional[str] = None,
    ):
        """Mark a URL as scraped"""
        with sqlite3.connect(str(self.db_path)) as conn:
            conn.execute(
                """UPDATE urls 
                   SET scraped_at = ?, status_code = ?, 
                       content_checksum = ?, error_message = ?
                   WHERE url = ?""",
                (
                    datetime.now().isoformat(),
                    status_code,
                    content_checksum,
                    error_message,
                    url,
                ),
            )
            conn.commit()

    def save_content(
        self,
        url: str,
        title: str,
        markdown: str,
        metadata: Dict[str, Any],
        filtered: bool = False,
        filter_reason: Optional[str] = None,
    ):
        """Save scraped content"""
        word_count = len(markdown.split())

        with sqlite3.connect(str(self.db_path)) as conn:
            conn.execute(
                """INSERT OR REPLACE INTO content 
                   (url, title, markdown, metadata, word_count, filtered, filter_reason)
                   VALUES (?, ?, ?, ?, ?, ?, ?)""",
                (
                    url,
                    title,
                    markdown,
                    json.dumps(metadata),
                    word_count,
                    filtered,
                    filter_reason,
                ),
            )
            conn.commit()

    def save_analysis(
        self,
        url: str,
        relevance_score: float,
        key_points: List[str],
        content_type: str,
        analysis_data: Dict[str, Any],
    ):
        """Save content analysis results"""
        with sqlite3.connect(str(self.db_path)) as conn:
            conn.execute(
                """INSERT OR REPLACE INTO analysis 
                   (url, relevance_score, key_points, content_type, 
                    analysis_data, analyzed_at)
                   VALUES (?, ?, ?, ?, ?, ?)""",
                (
                    url,
                    relevance_score,
                    json.dumps(key_points),
                    content_type,
                    json.dumps(analysis_data),
                    datetime.now().isoformat(),
                ),
            )
            conn.commit()

    def get_content_for_bundle(self) -> List[Dict[str, Any]]:
        """Get all non-filtered content for bundle creation"""
        with sqlite3.connect(str(self.db_path)) as conn:
            conn.row_factory = sqlite3.Row
            cursor = conn.execute(
                """
                SELECT c.*, a.relevance_score, a.key_points, a.content_type
                FROM content c
                LEFT JOIN analysis a ON c.url = a.url
                WHERE c.filtered = 0
                ORDER BY a.relevance_score DESC NULLS LAST
            """
            )

            content = []
            for row in cursor:
                content.append(
                    {
                        "url": row["url"],
                        "title": row["title"],
                        "markdown": row["markdown"],
                        "metadata": json.loads(row["metadata"]),
                        "word_count": row["word_count"],
                        "relevance_score": row["relevance_score"],
                        "key_points": (
                            json.loads(row["key_points"]) if row["key_points"] else []
                        ),
                        "content_type": row["content_type"],
                    }
                )

            return content

    def get_stats(self) -> Dict[str, int]:
        """Get job statistics"""
        with sqlite3.connect(str(self.db_path)) as conn:
            stats = {}

            # Total URLs
            cursor = conn.execute("SELECT COUNT(*) FROM urls")
            stats["total_urls"] = cursor.fetchone()[0]

            # Scraped URLs
            cursor = conn.execute(
                "SELECT COUNT(*) FROM urls WHERE scraped_at IS NOT NULL"
            )
            stats["scraped_urls"] = cursor.fetchone()[0]

            # Filtered URLs
            cursor = conn.execute("SELECT COUNT(*) FROM content WHERE filtered = 1")
            stats["filtered_urls"] = cursor.fetchone()[0]

            # Analyzed URLs
            cursor = conn.execute("SELECT COUNT(*) FROM analysis")
            stats["analyzed_urls"] = cursor.fetchone()[0]

            return stats

    def get_raw_content_files(self) -> List[Dict[str, Any]]:
        """Get information about raw HTML content that can be cleaned"""
        # Since we don't store raw HTML files anymore (we convert to markdown immediately),
        # this returns an empty list. In future, we could track original HTML if needed.
        return []

    def cleanup_raw_content(self) -> Dict[str, int]:
        """
        Clean up raw HTML data while preserving aggregated data
        Returns counts of cleaned items
        """
        # Currently, we don't store raw HTML separately
        # This is a placeholder for future implementation
        return {"files_deleted": 0, "space_freed": 0}

    def close_all_connections(self):
        """Close all database connections"""
        try:
            # Force garbage collection to close any remaining connections
            import gc

            gc.collect()

            # On Windows, sometimes we need to explicitly close the database
            # SQLite doesn't have persistent connections with context managers,
            # but we can force cleanup
            logger.debug(f"Closing database connections for {self.db_path}")
        except Exception as e:
            logger.warning(f"Error during database cleanup: {e}")

    def cleanup(self):
        """Clean up database resources"""
        self.close_all_connections()

======= scraper.py ======
# Copyright 2025 Franz und Franz GmbH
#
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#     http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.

"""
Smart scraper with advanced features for m1f-research
"""
import asyncio
import random
import aiohttp
from typing import List, Dict, Optional, Any
from datetime import datetime
import logging
from urllib.parse import urlparse, urljoin
import re

from .models import ScrapedContent
from .config import ScrapingConfig

logger = logging.getLogger(__name__)


class SmartScraper:
    """
    Advanced web scraper with:
    - Random timeouts for politeness
    - Concurrent scraping with rate limiting
    - Auto-retry on failures
    - Progress tracking
    - Robots.txt respect
    """

    def __init__(self, config: ScrapingConfig):
        self.config = config
        self.session: Optional[aiohttp.ClientSession] = None
        self.semaphore = asyncio.Semaphore(config.max_concurrent)
        self.progress_callback = None
        self.total_urls = 0
        self.completed_urls = 0
        self.failed_urls = []

    async def __aenter__(self):
        """Async context manager entry"""
        self.session = aiohttp.ClientSession()
        return self

    async def __aexit__(self, exc_type, exc_val, exc_tb):
        """Async context manager exit"""
        if self.session:
            await self.session.close()

    def set_progress_callback(self, callback):
        """Set callback for progress updates"""
        self.progress_callback = callback

    async def scrape_urls(self, urls: List[Dict[str, str]]) -> List[ScrapedContent]:
        """
        Scrape multiple URLs concurrently

        Args:
            urls: List of dicts with 'url', 'title', 'description'

        Returns:
            List of successfully scraped content
        """
        self.total_urls = len(urls)
        self.completed_urls = 0
        self.failed_urls = []

        # Create scraping tasks
        tasks = [self._scrape_with_retry(url_info) for url_info in urls]

        # Execute concurrently
        results = await asyncio.gather(*tasks, return_exceptions=True)

        # Filter out failures and exceptions
        scraped_content = []
        for result in results:
            if isinstance(result, ScrapedContent):
                scraped_content.append(result)
            elif isinstance(result, Exception):
                logger.error(f"Scraping exception: {result}")

        logger.info(f"Scraped {len(scraped_content)}/{len(urls)} URLs successfully")
        return scraped_content

    async def _scrape_with_retry(
        self, url_info: Dict[str, str]
    ) -> Optional[ScrapedContent]:
        """Scrape a single URL with retry logic"""
        url = url_info["url"]

        for attempt in range(self.config.retry_attempts):
            try:
                result = await self._scrape_single_url(url_info)
                if result:
                    return result

            except Exception as e:
                logger.warning(f"Attempt {attempt + 1} failed for {url}: {e}")
                if attempt < self.config.retry_attempts - 1:
                    # Exponential backoff
                    await asyncio.sleep(2**attempt)

        # All attempts failed
        self.failed_urls.append(url)
        return None

    async def _scrape_single_url(
        self, url_info: Dict[str, str]
    ) -> Optional[ScrapedContent]:
        """Scrape a single URL with rate limiting"""
        async with self.semaphore:
            url = url_info["url"]

            # Random delay for politeness
            min_delay, max_delay = self._parse_timeout_range()
            delay = random.uniform(min_delay, max_delay)
            await asyncio.sleep(delay)

            # Check robots.txt if enabled
            if self.config.respect_robots_txt and not await self._check_robots_txt(url):
                logger.info(f"Skipping {url} due to robots.txt")
                return None

            # Prepare headers
            headers = {
                "User-Agent": random.choice(self.config.user_agents),
                **self.config.headers,
            }

            try:
                async with self.session.get(
                    url,
                    headers=headers,
                    timeout=aiohttp.ClientTimeout(total=30),
                    allow_redirects=True,
                ) as response:
                    # Update progress
                    self.completed_urls += 1
                    if self.progress_callback:
                        self.progress_callback(self.completed_urls, self.total_urls)

                    if response.status == 200:
                        html = await response.text()

                        # Convert to markdown
                        markdown = await self._html_to_markdown(html, url)

                        return ScrapedContent(
                            url=str(response.url),  # Use final URL after redirects
                            title=url_info.get("title", self._extract_title(html)),
                            content=markdown,
                            scraped_at=datetime.now(),
                            metadata={
                                "status_code": response.status,
                                "content_type": response.headers.get(
                                    "Content-Type", ""
                                ),
                                "content_length": len(html),
                                "final_url": str(response.url),
                            },
                        )
                    else:
                        logger.warning(f"HTTP {response.status} for {url}")
                        return None

            except asyncio.TimeoutError:
                logger.error(f"Timeout scraping {url}")
                return None
            except Exception as e:
                logger.error(f"Error scraping {url}: {e}")
                return None

    async def _html_to_markdown(self, html: str, base_url: str) -> str:
        """Convert HTML to Markdown"""
        try:
            # Try to import and use existing converters
            from markdownify import markdownify

            # Configure markdownify for better output
            markdown = markdownify(
                html,
                heading_style="ATX",
                bullets="-",
                code_language="python",
                wrap=True,
                wrap_width=80,
            )

            # Fix relative URLs
            markdown = self._fix_relative_urls(markdown, base_url)

            return markdown

        except ImportError:
            # Fallback to basic conversion
            return self._basic_html_to_markdown(html)

    def _basic_html_to_markdown(self, html: str) -> str:
        """Basic HTML to Markdown conversion"""
        # Remove script and style tags
        html = re.sub(
            r"<script[^>]*>.*?</script>", "", html, flags=re.DOTALL | re.IGNORECASE
        )
        html = re.sub(
            r"<style[^>]*>.*?</style>", "", html, flags=re.DOTALL | re.IGNORECASE
        )

        # Convert common tags
        conversions = [
            (r"<h1[^>]*>(.*?)</h1>", r"# \1\n"),
            (r"<h2[^>]*>(.*?)</h2>", r"## \1\n"),
            (r"<h3[^>]*>(.*?)</h3>", r"### \1\n"),
            (r"<h4[^>]*>(.*?)</h4>", r"#### \1\n"),
            (r"<h5[^>]*>(.*?)</h5>", r"##### \1\n"),
            (r"<h6[^>]*>(.*?)</h6>", r"###### \1\n"),
            (r"<p[^>]*>(.*?)</p>", r"\1\n\n"),
            (r"<br[^>]*>", "\n"),
            (r"<strong[^>]*>(.*?)</strong>", r"**\1**"),
            (r"<b[^>]*>(.*?)</b>", r"**\1**"),
            (r"<em[^>]*>(.*?)</em>", r"*\1*"),
            (r"<i[^>]*>(.*?)</i>", r"*\1*"),
            (r"<code[^>]*>(.*?)</code>", r"`\1`"),
            (r'<a[^>]+href="([^"]+)"[^>]*>(.*?)</a>', r"[\2](\1)"),
            (r"<li[^>]*>(.*?)</li>", r"- \1\n"),
            (r"<ul[^>]*>", "\n"),
            (r"</ul>", "\n"),
            (r"<ol[^>]*>", "\n"),
            (r"</ol>", "\n"),
        ]

        for pattern, replacement in conversions:
            html = re.sub(pattern, replacement, html, flags=re.DOTALL | re.IGNORECASE)

        # Remove remaining tags
        html = re.sub(r"<[^>]+>", "", html)

        # Clean up whitespace
        html = re.sub(r"\n\s*\n\s*\n", "\n\n", html)
        html = html.strip()

        return html

    def _fix_relative_urls(self, markdown: str, base_url: str) -> str:
        """Convert relative URLs to absolute URLs"""
        # Parse base URL
        parsed_base = urlparse(base_url)
        base_domain = f"{parsed_base.scheme}://{parsed_base.netloc}"

        # Fix markdown links
        def fix_link(match):
            text = match.group(1)
            url = match.group(2)

            # Skip if already absolute
            if url.startswith(("http://", "https://", "mailto:", "#")):
                return match.group(0)

            # Convert to absolute
            if url.startswith("/"):
                url = base_domain + url
            else:
                url = urljoin(base_url, url)

            return f"[{text}]({url})"

        markdown = re.sub(r"\[([^\]]+)\]\(([^)]+)\)", fix_link, markdown)

        return markdown

    def _extract_title(self, html: str) -> str:
        """Extract title from HTML"""
        title_match = re.search(
            r"<title[^>]*>(.*?)</title>", html, re.IGNORECASE | re.DOTALL
        )
        if title_match:
            title = title_match.group(1).strip()
            # Clean up title
            title = re.sub(r"\s+", " ", title)
            return title[:200]  # Limit length
        return "Untitled"

    async def _check_robots_txt(self, url: str) -> bool:
        """Check if URL is allowed by robots.txt"""
        # Simple implementation - in production would use robotparser
        parsed = urlparse(url)
        robots_url = f"{parsed.scheme}://{parsed.netloc}/robots.txt"

        try:
            async with self.session.get(
                robots_url, timeout=aiohttp.ClientTimeout(total=5)
            ) as response:
                if response.status == 200:
                    robots_txt = await response.text()
                    # Very basic check - just look for explicit disallow
                    path = parsed.path or "/"
                    if f"Disallow: {path}" in robots_txt:
                        return False
        except:
            # If we can't check robots.txt, assume it's OK
            pass

        return True

    def _parse_timeout_range(self) -> tuple[float, float]:
        """Parse timeout range string"""
        parts = self.config.timeout_range.split("-")
        if len(parts) == 2:
            return float(parts[0]), float(parts[1])
        else:
            val = float(parts[0])
            return val, val

    def get_stats(self) -> Dict[str, Any]:
        """Get scraping statistics"""
        return {
            "total_urls": self.total_urls,
            "completed_urls": self.completed_urls,
            "failed_urls": len(self.failed_urls),
            "success_rate": (
                self.completed_urls / self.total_urls if self.total_urls > 0 else 0
            ),
            "failed_url_list": self.failed_urls,
        }

======= smart_scraper.py ======
# Copyright 2025 Franz und Franz GmbH
#
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#     http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.

"""
Enhanced smart scraper with per-host delay management for m1f-research
"""
import asyncio
import random
import aiohttp
import hashlib
from typing import List, Dict, Optional, Any, Tuple
from datetime import datetime
from collections import defaultdict
import logging
from urllib.parse import urlparse, urljoin
import re

from .models import ScrapedContent
from .config import ScrapingConfig
from .url_manager import URLManager
from .research_db import JobDatabase

logger = logging.getLogger(__name__)


class HostDelayManager:
    """Manages delays per host to be polite to servers"""

    def __init__(
        self, delay_range: Tuple[float, float] = (1.0, 3.0), threshold: int = 3
    ):
        self.delay_range = delay_range
        self.threshold = threshold
        self.host_request_count = defaultdict(int)
        self.last_request_time = {}

    async def wait_if_needed(self, url: str):
        """Wait if we're making too many requests to the same host"""
        host = urlparse(url).netloc
        self.host_request_count[host] += 1

        # Only delay if we've made more than threshold requests to this host
        if self.host_request_count[host] > self.threshold:
            delay = random.uniform(*self.delay_range)
            logger.debug(
                f"Delaying {delay:.1f}s for host {host} (request #{self.host_request_count[host]})"
            )
            await asyncio.sleep(delay)
        else:
            logger.debug(
                f"No delay for host {host} (request #{self.host_request_count[host]})"
            )

    def get_host_stats(self) -> Dict[str, int]:
        """Get request counts per host"""
        return dict(self.host_request_count)


class EnhancedSmartScraper:
    """
    Enhanced web scraper with:
    - Per-host delay management
    - Database integration
    - Content checksum tracking
    - Better error handling
    """

    def __init__(
        self, config: ScrapingConfig, job_db: JobDatabase, url_manager: URLManager
    ):
        self.config = config
        self.job_db = job_db
        self.url_manager = url_manager
        self.session: Optional[aiohttp.ClientSession] = None
        self.semaphore = asyncio.Semaphore(config.max_concurrent)
        self.delay_manager = HostDelayManager(
            delay_range=(config.delay[0], config.delay[1]), threshold=3
        )

        # Progress tracking
        self.progress_callback = None
        self.total_urls = 0
        self.completed_urls = 0
        self.successful_urls = 0
        self.failed_urls = []

    async def __aenter__(self):
        """Async context manager entry"""
        headers = {"User-Agent": random.choice(self.config.user_agents)}
        timeout = aiohttp.ClientTimeout(total=self.config.timeout)
        self.session = aiohttp.ClientSession(headers=headers, timeout=timeout)
        return self

    async def __aexit__(self, exc_type, exc_val, exc_tb):
        """Async context manager exit"""
        if self.session:
            await self.session.close()

    def set_progress_callback(self, callback):
        """Set callback for progress updates"""
        self.progress_callback = callback

    async def scrape_urls(self, urls: List[str]) -> List[ScrapedContent]:
        """
        Scrape multiple URLs with smart host-based delays
        """
        self.total_urls = len(urls)
        self.completed_urls = 0
        self.successful_urls = 0
        self.failed_urls = []

        # Group URLs by host for smart scheduling
        urls_by_host = defaultdict(list)
        for url in urls:
            host = urlparse(url).netloc
            urls_by_host[host].append(url)

        logger.info(
            f"Scraping {len(urls)} URLs from {len(urls_by_host)} different hosts"
        )

        # Create tasks with mixed hosts for better parallelism
        tasks = []
        url_queue = []

        # Interleave URLs from different hosts
        max_urls = max(len(urls) for urls in urls_by_host.values())
        for i in range(max_urls):
            for host, host_urls in urls_by_host.items():
                if i < len(host_urls):
                    url_queue.append(host_urls[i])

        # Create scraping tasks
        for url in url_queue:
            task = self._scrape_with_semaphore(url)
            tasks.append(task)

        # Run all tasks
        results = await asyncio.gather(*tasks, return_exceptions=True)

        # Process results
        scraped_content = []
        for url, result in zip(url_queue, results):
            if isinstance(result, Exception):
                logger.error(f"Failed to scrape {url}: {result}")
                self.failed_urls.append(url)
            elif result:
                scraped_content.append(result)
                self.successful_urls += 1

        # Log statistics
        logger.info(
            f"Scraping complete: {self.successful_urls}/{self.total_urls} successful"
        )
        if self.failed_urls:
            logger.warning(f"Failed to scrape {len(self.failed_urls)} URLs")

        host_stats = self.delay_manager.get_host_stats()
        logger.info(
            f"Requests per host: {dict(list(host_stats.items())[:5])}..."
        )  # Show first 5

        return scraped_content

    async def _scrape_with_semaphore(self, url: str) -> Optional[ScrapedContent]:
        """Scrape a single URL with semaphore control"""
        async with self.semaphore:
            return await self._scrape_url(url)

    async def _scrape_url(self, url: str) -> Optional[ScrapedContent]:
        """Scrape a single URL with retries and smart delays"""
        # Apply per-host delay if needed
        await self.delay_manager.wait_if_needed(url)

        # Check robots.txt if configured
        if not await self._check_robots_txt(url):
            logger.info(f"Skipping {url} due to robots.txt")
            self.url_manager.mark_url_scraped(
                url, -1, error_message="Blocked by robots.txt"
            )
            self._update_progress()
            return None

        # Try scraping with retries
        for attempt in range(self.config.retries):
            try:
                async with self.session.get(url) as response:
                    if response.status == 200:
                        content = await response.text()

                        # Calculate content checksum
                        content_checksum = hashlib.sha256(content.encode()).hexdigest()

                        # Create scraped content object
                        scraped = ScrapedContent(
                            url=url,
                            title=self._extract_title(content),
                            content=content,
                            content_type=response.headers.get("Content-Type", ""),
                            scraped_at=datetime.now(),
                        )

                        # Mark as scraped in database
                        self.url_manager.mark_url_scraped(
                            url, response.status, content_checksum=content_checksum
                        )

                        self._update_progress()
                        return scraped

                    else:
                        error_msg = f"HTTP {response.status}"
                        if attempt < self.config.retries - 1:
                            logger.warning(f"{error_msg} for {url}, retrying...")
                            await asyncio.sleep(2**attempt)  # Exponential backoff
                        else:
                            logger.warning(f"{error_msg} for {url}")
                            self.url_manager.mark_url_scraped(
                                url, response.status, error_message=error_msg
                            )
                            self._update_progress()
                            return None

            except asyncio.TimeoutError:
                error_msg = "Timeout"
                if attempt < self.config.retries - 1:
                    logger.warning(f"Timeout for {url}, retrying...")
                    await asyncio.sleep(2**attempt)
                else:
                    logger.error(
                        f"Timeout for {url} after {self.config.retries} attempts"
                    )
                    self.url_manager.mark_url_scraped(url, -1, error_message=error_msg)
                    self._update_progress()
                    return None

            except Exception as e:
                error_msg = str(e)
                logger.error(f"Error scraping {url}: {e}")
                if attempt < self.config.retries - 1:
                    await asyncio.sleep(2**attempt)
                else:
                    self.url_manager.mark_url_scraped(url, -1, error_message=error_msg)
                    self._update_progress()
                    return None

        return None

    async def _check_robots_txt(self, url: str) -> bool:
        """Check if URL is allowed by robots.txt"""
        if not self.config.respect_robots_txt:
            return True

        try:
            parsed = urlparse(url)
            robots_url = f"{parsed.scheme}://{parsed.netloc}/robots.txt"

            # Simple check - just see if robots.txt mentions the path
            async with self.session.get(robots_url) as response:
                if response.status == 200:
                    content = await response.text()
                    path = parsed.path or "/"

                    # Very basic robots.txt parsing
                    lines = content.lower().split("\n")
                    user_agent_applies = False

                    for line in lines:
                        line = line.strip()
                        if line.startswith("user-agent:"):
                            user_agent_applies = "*" in line or "bot" in line
                        elif user_agent_applies and line.startswith("disallow:"):
                            disallowed = line.split(":", 1)[1].strip()
                            if disallowed and path.lower().startswith(disallowed):
                                return False

            return True

        except Exception as e:
            logger.debug(f"Could not check robots.txt for {url}: {e}")
            return True  # Allow if we can't check

    def _extract_title(self, html: str) -> str:
        """Extract title from HTML"""
        match = re.search(r"<title[^>]*>([^<]+)</title>", html, re.IGNORECASE)
        if match:
            return match.group(1).strip()

        # Try h1 as fallback
        match = re.search(r"<h1[^>]*>([^<]+)</h1>", html, re.IGNORECASE)
        if match:
            return match.group(1).strip()

        return "Untitled"

    def _update_progress(self):
        """Update progress and call callback if set"""
        self.completed_urls += 1
        if self.progress_callback:
            progress = (self.completed_urls / self.total_urls) * 100
            self.progress_callback(self.completed_urls, self.total_urls, progress)

    def get_statistics(self) -> Dict[str, Any]:
        """Get scraping statistics"""
        return {
            "total_urls": self.total_urls,
            "completed_urls": self.completed_urls,
            "successful_urls": self.successful_urls,
            "failed_urls": len(self.failed_urls),
            "host_stats": self.delay_manager.get_host_stats(),
        }

======= url_manager.py ======
# Copyright 2025 Franz und Franz GmbH
#
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#     http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.

"""
URL management for m1f-research with file support and deduplication
"""
import logging
from pathlib import Path
from typing import List, Dict, Any, Optional
from urllib.parse import urlparse, urlunparse

from m1f.file_operations import (
    safe_exists,
    safe_read_text,
)

from .research_db import JobDatabase

logger = logging.getLogger(__name__)


class URLManager:
    """Manages URL collection, deduplication, and tracking"""

    def __init__(self, job_db: JobDatabase):
        self.job_db = job_db

    def add_urls_from_list(
        self, urls: List[Dict[str, str]], source: str = "llm"
    ) -> int:
        """Add URLs from a list (LLM-generated or manual)"""
        return self.job_db.add_urls(urls, added_by=source)

    async def add_urls_from_file(self, file_path: Path) -> int:
        """Add URLs from a text file (one URL per line)"""
        if not safe_exists(file_path):
            logger.error(f"URL file not found: {file_path}")
            return 0

        urls = []
        try:
            content = safe_read_text(file_path)
            for line in content.splitlines():
                line = line.strip()
                if line and not line.startswith("#"):  # Skip comments
                    # Support optional title after URL
                    parts = line.split("\t", 1)
                    url = parts[0].strip()
                    title = parts[1].strip() if len(parts) > 1 else ""

                    if url.startswith(("http://", "https://")):
                        urls.append(
                            {
                                "url": url,
                                "title": title,
                                "description": f"From file: {file_path.name}",
                            }
                        )

        except Exception as e:
            logger.error(f"Error reading URL file {file_path}: {e}")
            return 0

        logger.info(f"Found {len(urls)} URLs in {file_path}")
        return self.add_urls_from_list(urls, source="manual")

    def get_unscraped_urls(self) -> List[str]:
        """Get all URLs that haven't been scraped yet"""
        return self.job_db.get_unscraped_urls()

    def get_urls_grouped_by_host(self) -> Dict[str, List[str]]:
        """Get unscraped URLs grouped by host for smart delay management"""
        return self.job_db.get_urls_by_host()

    def normalize_url(self, url: str) -> str:
        """Normalize a URL for deduplication"""
        try:
            parsed = urlparse(url)

            # Normalize components
            scheme = parsed.scheme.lower()
            netloc = parsed.netloc.lower()
            path = parsed.path.rstrip("/")

            # Remove default ports
            if netloc.endswith(":80") and scheme == "http":
                netloc = netloc[:-3]
            elif netloc.endswith(":443") and scheme == "https":
                netloc = netloc[:-4]

            # Reconstruct URL
            normalized = urlunparse(
                (
                    scheme,
                    netloc,
                    path,
                    parsed.params,
                    parsed.query,
                    "",  # Remove fragment
                )
            )

            return normalized

        except Exception as e:
            logger.warning(f"Could not normalize URL {url}: {e}")
            return url

    def deduplicate_urls(self, urls: List[str]) -> List[str]:
        """Remove duplicate URLs based on normalization"""
        seen = set()
        unique = []

        for url in urls:
            normalized = self.normalize_url(url)
            if normalized not in seen:
                seen.add(normalized)
                unique.append(url)

        if len(urls) != len(unique):
            logger.info(f"Deduplicated {len(urls)} URLs to {len(unique)} unique URLs")

        return unique

    def get_host_from_url(self, url: str) -> str:
        """Extract host from URL"""
        try:
            return urlparse(url).netloc
        except:
            return "unknown"

    def create_url_batches(self, max_per_host: int = 5) -> List[List[str]]:
        """Create URL batches for parallel scraping with host limits"""
        urls_by_host = self.get_urls_grouped_by_host()
        batches = []

        # First pass: Add up to max_per_host from each host
        current_batch = []
        host_counts = {}

        for host, urls in urls_by_host.items():
            for url in urls[:max_per_host]:
                current_batch.append(url)
                host_counts[host] = host_counts.get(host, 0) + 1

                # Create new batch when we have enough diversity
                if len(current_batch) >= 10:  # Batch size
                    batches.append(current_batch)
                    current_batch = []

        # Add remaining URLs
        if current_batch:
            batches.append(current_batch)

        # Second pass: Add remaining URLs from hosts with many URLs
        for host, urls in urls_by_host.items():
            if len(urls) > max_per_host:
                remaining = urls[max_per_host:]
                for i in range(0, len(remaining), max_per_host):
                    batch = remaining[i : i + max_per_host]
                    batches.append(batch)

        logger.info(f"Created {len(batches)} URL batches for scraping")
        return batches

    def mark_url_scraped(
        self,
        url: str,
        status_code: int,
        content_checksum: Optional[str] = None,
        error_message: Optional[str] = None,
    ):
        """Mark a URL as scraped"""
        self.job_db.mark_url_scraped(url, status_code, content_checksum, error_message)

    def get_stats(self) -> Dict[str, int]:
        """Get URL statistics"""
        return self.job_db.get_stats()

======= url_reviewer.py ======
# Copyright 2025 Franz und Franz GmbH
#
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#     http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.

"""
Interactive URL review interface for research curation
"""

import logging
from typing import List, Dict, Optional, Tuple
from dataclasses import dataclass
import re

try:
    from shared.colors import Colors, info, success, warning, error
except ImportError:
    # Fallback if colors not available
    class Colors:
        CYAN = ""
        GREEN = ""
        YELLOW = ""
        RED = ""
        BOLD = ""
        DIM = ""
        RESET = ""

    def info(msg):
        print(msg)

    def success(msg):
        print(msg)

    def warning(msg):
        print(msg)

    def error(msg):
        print(msg)


logger = logging.getLogger(__name__)


@dataclass
class URLItem:
    """Represents a URL for review"""

    url: str
    title: Optional[str] = None
    description: Optional[str] = None
    source: Optional[str] = None
    status: str = "pending"
    index: Optional[int] = None


class URLReviewer:
    """Interactive interface for reviewing and curating URLs"""

    def __init__(self, url_manager=None):
        """
        Initialize the URL reviewer

        Args:
            url_manager: URLManager instance for database operations
        """
        self.url_manager = url_manager
        self.urls: List[URLItem] = []
        self.deleted_urls: List[URLItem] = []
        self.filter_term: Optional[str] = None

    def load_urls(self, urls: List[Dict]) -> None:
        """
        Load URLs for review

        Args:
            urls: List of URL dictionaries with url, title, description
        """
        self.urls = []
        for i, url_data in enumerate(urls):
            if isinstance(url_data, dict):
                item = URLItem(
                    url=url_data.get("url", ""),
                    title=url_data.get("title"),
                    description=url_data.get("description"),
                    source=url_data.get("source", "search"),
                    status=url_data.get("status", "pending"),
                    index=i + 1,
                )
            else:
                # Handle plain URL strings
                item = URLItem(url=str(url_data), index=i + 1)

            self.urls.append(item)

        logger.info(f"Loaded {len(self.urls)} URLs for review")

    async def interactive_review(self) -> Tuple[List[Dict], bool]:
        """
        Run interactive review session

        Returns:
            Tuple of (reviewed URLs list, confirm to proceed)
        """
        if not self.urls:
            warning("No URLs to review")
            return [], False

        info(f"\n{Colors.BOLD}=== URL Review Interface ==={Colors.RESET}")
        info(f"Total URLs: {len(self.urls)}")
        info("Type 'help' for commands\n")

        while True:
            # Display current URLs
            self._display_urls()

            # Get user command
            try:
                command = input(f"\n{Colors.CYAN}review> {Colors.RESET}").strip()
            except (EOFError, KeyboardInterrupt):
                info("\nReview cancelled")
                return [], False

            if not command:
                continue

            # Process command
            result = self._process_command(command)

            if result == "confirm":
                # User confirmed, return filtered URLs
                final_urls = [
                    {
                        "url": item.url,
                        "title": item.title,
                        "description": item.description,
                        "source": item.source,
                    }
                    for item in self.urls
                    if item.status != "deleted"
                ]

                success(f"\nâœ“ Proceeding with {len(final_urls)} URLs")
                return final_urls, True

            elif result == "cancel":
                warning("\nReview cancelled")
                return [], False

    def _display_urls(self, page_size: int = 20) -> None:
        """Display URLs in a formatted table"""
        # Apply filter if set
        display_urls = self._get_filtered_urls()

        if not display_urls:
            warning("No URLs to display (all filtered/deleted)")
            return

        # Table header
        print(
            f"\n{Colors.BOLD}{'ID':<4} {'Status':<10} {'Title/URL':<60} {'Source':<10}{Colors.RESET}"
        )
        print("â”€" * 85)

        # Display URLs
        for item in display_urls[:page_size]:
            # Status color
            if item.status == "deleted":
                status_color = Colors.RED
                status_symbol = "âœ—"
            elif item.status == "reviewed":
                status_color = Colors.GREEN
                status_symbol = "âœ“"
            else:
                status_color = Colors.YELLOW
                status_symbol = "?"

            # Display title or URL
            display_text = (
                item.title[:57] + "..."
                if item.title and len(item.title) > 60
                else (item.title or item.url[:60])
            )

            print(
                f"{item.index:<4} {status_color}{status_symbol} {item.status:<8}{Colors.RESET} {display_text:<60} {item.source or 'unknown':<10}"
            )

        if len(display_urls) > page_size:
            info(
                f"\n... and {len(display_urls) - page_size} more. Use 'show all' to see all URLs"
            )

        # Summary
        total = len(self.urls)
        deleted = len([u for u in self.urls if u.status == "deleted"])
        reviewed = len([u for u in self.urls if u.status == "reviewed"])
        pending = total - deleted - reviewed

        print(
            f"\n{Colors.DIM}Total: {total} | Pending: {pending} | Reviewed: {reviewed} | Deleted: {deleted}{Colors.RESET}"
        )

    def _get_filtered_urls(self) -> List[URLItem]:
        """Get URLs based on current filter"""
        if not self.filter_term:
            return [u for u in self.urls if u.status != "deleted"]

        term = self.filter_term.lower()
        filtered = []

        for item in self.urls:
            if item.status == "deleted":
                continue

            # Search in URL, title, and description
            if (
                term in item.url.lower()
                or (item.title and term in item.title.lower())
                or (item.description and term in item.description.lower())
            ):
                filtered.append(item)

        return filtered

    def _process_command(self, command: str) -> Optional[str]:
        """Process user command"""
        parts = command.split()
        if not parts:
            return None

        cmd = parts[0].lower()

        if cmd == "help" or cmd == "h":
            self._show_help()

        elif cmd == "delete" or cmd == "d":
            if len(parts) > 1:
                self._delete_urls(parts[1])
            else:
                error("Usage: delete <id> or delete <id1,id2,id3>")

        elif cmd == "keep" or cmd == "k":
            if len(parts) > 1:
                self._keep_urls(parts[1])
            else:
                error("Usage: keep <id> or keep <id1,id2,id3>")

        elif cmd == "search" or cmd == "s":
            if len(parts) > 1:
                search_term = " ".join(parts[1:])
                self.filter_term = search_term
                info(f"Filtering URLs containing: {search_term}")
            else:
                error("Usage: search <term>")

        elif cmd == "clear":
            self.filter_term = None
            info("Filter cleared")

        elif cmd == "show":
            if len(parts) > 1 and parts[1] == "all":
                self._display_urls(page_size=1000)
            elif len(parts) > 1 and parts[1] == "deleted":
                self._show_deleted()
            else:
                self._display_urls()

        elif cmd == "restore":
            if len(parts) > 1:
                self._restore_urls(parts[1])
            else:
                error("Usage: restore <id> or restore <id1,id2,id3>")

        elif cmd == "confirm" or cmd == "c":
            remaining = len([u for u in self.urls if u.status != "deleted"])
            if remaining == 0:
                error("No URLs remaining. Add URLs or restore deleted ones.")
            else:
                response = (
                    input(f"Proceed with {remaining} URLs? [y/N]: ").strip().lower()
                )
                if response == "y":
                    return "confirm"

        elif cmd == "cancel" or cmd == "quit" or cmd == "q":
            return "cancel"

        elif cmd == "stats":
            self._show_stats()

        else:
            warning(f"Unknown command: {cmd}. Type 'help' for commands")

        return None

    def _show_help(self) -> None:
        """Show help message"""
        help_text = f"""
{Colors.BOLD}Available Commands:{Colors.RESET}
  {Colors.CYAN}delete <id>{Colors.RESET}    - Delete URL(s). Examples: delete 1  delete 1,2,3  delete 1-5
  {Colors.CYAN}keep <id>{Colors.RESET}      - Mark URL(s) as reviewed/kept
  {Colors.CYAN}search <term>{Colors.RESET}  - Filter URLs containing term
  {Colors.CYAN}clear{Colors.RESET}          - Clear current filter
  {Colors.CYAN}show all{Colors.RESET}       - Show all URLs (no pagination)
  {Colors.CYAN}show deleted{Colors.RESET}   - Show deleted URLs
  {Colors.CYAN}restore <id>{Colors.RESET}   - Restore deleted URL(s)
  {Colors.CYAN}stats{Colors.RESET}          - Show review statistics
  {Colors.CYAN}confirm{Colors.RESET}        - Confirm and proceed with remaining URLs
  {Colors.CYAN}cancel{Colors.RESET}         - Cancel review and exit
  {Colors.CYAN}help{Colors.RESET}           - Show this help

{Colors.DIM}Shortcuts: d=delete, k=keep, s=search, c=confirm, q=quit{Colors.RESET}
"""
        print(help_text)

    def _delete_urls(self, id_spec: str) -> None:
        """Delete specified URLs"""
        ids = self._parse_id_spec(id_spec)
        deleted_count = 0

        for url_id in ids:
            for item in self.urls:
                if item.index == url_id and item.status != "deleted":
                    item.status = "deleted"
                    deleted_count += 1
                    break

        if deleted_count > 0:
            success(f"Deleted {deleted_count} URL(s)")
        else:
            warning("No matching URLs found to delete")

    def _keep_urls(self, id_spec: str) -> None:
        """Mark URLs as reviewed/kept"""
        ids = self._parse_id_spec(id_spec)
        kept_count = 0

        for url_id in ids:
            for item in self.urls:
                if item.index == url_id and item.status != "deleted":
                    item.status = "reviewed"
                    kept_count += 1
                    break

        if kept_count > 0:
            success(f"Marked {kept_count} URL(s) as reviewed")
        else:
            warning("No matching URLs found")

    def _restore_urls(self, id_spec: str) -> None:
        """Restore deleted URLs"""
        ids = self._parse_id_spec(id_spec)
        restored_count = 0

        for url_id in ids:
            for item in self.urls:
                if item.index == url_id and item.status == "deleted":
                    item.status = "pending"
                    restored_count += 1
                    break

        if restored_count > 0:
            success(f"Restored {restored_count} URL(s)")
        else:
            warning("No deleted URLs found to restore")

    def _parse_id_spec(self, id_spec: str) -> List[int]:
        """Parse ID specification (e.g., "1", "1,2,3", "1-5")"""
        ids = []

        # Handle comma-separated IDs
        if "," in id_spec:
            for part in id_spec.split(","):
                part = part.strip()
                if "-" in part:
                    # Range within comma-separated list
                    ids.extend(self._parse_range(part))
                else:
                    try:
                        ids.append(int(part))
                    except ValueError:
                        pass

        # Handle range
        elif "-" in id_spec:
            ids = self._parse_range(id_spec)

        # Single ID
        else:
            try:
                ids.append(int(id_spec))
            except ValueError:
                pass

        return ids

    def _parse_range(self, range_spec: str) -> List[int]:
        """Parse range specification (e.g., "1-5")"""
        try:
            parts = range_spec.split("-")
            if len(parts) == 2:
                start = int(parts[0])
                end = int(parts[1])
                return list(range(start, end + 1))
        except ValueError:
            pass
        return []

    def _show_deleted(self) -> None:
        """Show deleted URLs"""
        deleted = [u for u in self.urls if u.status == "deleted"]

        if not deleted:
            info("No deleted URLs")
            return

        print(f"\n{Colors.BOLD}Deleted URLs:{Colors.RESET}")
        print("â”€" * 60)

        for item in deleted:
            display_text = item.title or item.url[:50]
            print(f"{Colors.DIM}{item.index:<4} {display_text}{Colors.RESET}")

        info(f"\nTotal deleted: {len(deleted)}")
        info("Use 'restore <id>' to restore URLs")

    def _show_stats(self) -> None:
        """Show review statistics"""
        total = len(self.urls)
        deleted = len([u for u in self.urls if u.status == "deleted"])
        reviewed = len([u for u in self.urls if u.status == "reviewed"])
        pending = total - deleted - reviewed

        # Count by source
        sources = {}
        for item in self.urls:
            if item.status != "deleted":
                source = item.source or "unknown"
                sources[source] = sources.get(source, 0) + 1

        print(f"\n{Colors.BOLD}Review Statistics:{Colors.RESET}")
        print("â”€" * 40)
        print(f"Total URLs:     {total}")
        print(f"Pending:        {pending}")
        print(f"Reviewed:       {reviewed}")
        print(f"Deleted:        {deleted}")
        print(f"Remaining:      {total - deleted}")

        if sources:
            print(f"\n{Colors.BOLD}Sources:{Colors.RESET}")
            for source, count in sorted(
                sources.items(), key=lambda x: x[1], reverse=True
            ):
                print(f"  {source:<15} {count}")

======= workflow_phases.py ======
# Copyright 2025 Franz und Franz GmbH
#
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#     http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.

"""
Workflow phase definitions and management for m1f-research
"""

from enum import Enum
from dataclasses import dataclass
from typing import Optional, Dict, Any, List
import logging

logger = logging.getLogger(__name__)


class WorkflowPhase(Enum):
    """Defines the phases of the research workflow"""

    INITIALIZATION = "initialization"
    QUERY_EXPANSION = "query_expansion"
    URL_COLLECTION = "url_collection"
    URL_REVIEW = "url_review"
    CRAWLING = "crawling"
    BUNDLING = "bundling"
    ANALYSIS = "analysis"
    COMPLETED = "completed"
    FAILED = "failed"

    @classmethod
    def get_next_phase(
        cls, current_phase: "WorkflowPhase"
    ) -> Optional["WorkflowPhase"]:
        """Get the next phase in the workflow"""
        phase_order = [
            cls.INITIALIZATION,
            cls.QUERY_EXPANSION,
            cls.URL_COLLECTION,
            cls.URL_REVIEW,
            cls.CRAWLING,
            cls.BUNDLING,
            cls.ANALYSIS,
            cls.COMPLETED,
        ]

        try:
            current_index = phase_order.index(current_phase)
            if current_index < len(phase_order) - 1:
                return phase_order[current_index + 1]
        except ValueError:
            logger.warning(f"Unknown phase: {current_phase}")

        return None

    @classmethod
    def can_resume_from(cls, phase: "WorkflowPhase") -> bool:
        """Check if a phase can be resumed from"""
        # Can't resume from completed or failed states
        return phase not in [cls.COMPLETED, cls.FAILED]


@dataclass
class PhaseContext:
    """Context data for a workflow phase"""

    phase: WorkflowPhase
    data: Dict[str, Any]
    completed: bool = False
    error: Optional[str] = None

    def to_dict(self) -> Dict[str, Any]:
        """Convert to dictionary for storage"""
        return {
            "phase": self.phase.value,
            "data": self.data,
            "completed": self.completed,
            "error": self.error,
        }

    @classmethod
    def from_dict(cls, data: Dict[str, Any]) -> "PhaseContext":
        """Create from dictionary"""
        return cls(
            phase=WorkflowPhase(data["phase"]),
            data=data.get("data", {}),
            completed=data.get("completed", False),
            error=data.get("error"),
        )


class WorkflowManager:
    """Manages workflow phases and transitions"""

    def __init__(self, job_manager=None, config=None):
        """
        Initialize workflow manager

        Args:
            job_manager: JobManager instance for persistence
            config: Research configuration
        """
        self.job_manager = job_manager
        self.config = config
        self.current_phase: Optional[WorkflowPhase] = None
        self.phase_data: Dict[str, Any] = {}

    def set_phase(
        self, job_id: str, phase: WorkflowPhase, data: Optional[Dict[str, Any]] = None
    ):
        """
        Set the current workflow phase

        Args:
            job_id: Job ID
            phase: New phase
            data: Optional phase-specific data
        """
        self.current_phase = phase
        if data:
            self.phase_data = data

        # Persist to database if job_manager available
        if self.job_manager and hasattr(self.job_manager.main_db, "update_job_phase"):
            self.job_manager.main_db.update_job_phase(job_id, phase.value, data)

        logger.info(f"Workflow phase set to: {phase.value}")

    def get_phase(self, job_id: str) -> Optional[PhaseContext]:
        """
        Get current phase context for a job

        Args:
            job_id: Job ID

        Returns:
            PhaseContext or None
        """
        if not self.job_manager:
            return None

        job = self.job_manager.get_job(job_id)
        if not job:
            return None

        # Get phase from job data
        phase_str = getattr(job, "phase", "initialization")
        phase_data = getattr(job, "phase_data", {})

        try:
            phase = WorkflowPhase(phase_str)
            return PhaseContext(phase=phase, data=phase_data)
        except ValueError:
            logger.error(f"Invalid phase in job: {phase_str}")
            return None

    def can_transition_to(
        self, from_phase: WorkflowPhase, to_phase: WorkflowPhase
    ) -> bool:
        """
        Check if transition between phases is valid

        Args:
            from_phase: Current phase
            to_phase: Target phase

        Returns:
            True if transition is valid
        """
        # Can always transition to FAILED
        if to_phase == WorkflowPhase.FAILED:
            return True

        # Check if it's the natural next phase
        next_phase = WorkflowPhase.get_next_phase(from_phase)
        if next_phase == to_phase:
            return True

        # Allow skipping certain phases based on config
        if self.config:
            # Can skip query expansion if disabled
            if (
                from_phase == WorkflowPhase.INITIALIZATION
                and to_phase == WorkflowPhase.URL_COLLECTION
                and not getattr(self.config.workflow, "expand_queries", True)
            ):
                return True

            # Can skip URL review if disabled
            if (
                from_phase == WorkflowPhase.URL_COLLECTION
                and to_phase == WorkflowPhase.CRAWLING
                and getattr(self.config.workflow, "skip_review", False)
            ):
                return True

            # Can skip analysis if disabled
            if (
                from_phase == WorkflowPhase.BUNDLING
                and to_phase == WorkflowPhase.COMPLETED
                and not getattr(self.config.workflow, "generate_analysis", True)
            ):
                return True

        return False

    def transition_to(
        self,
        job_id: str,
        new_phase: WorkflowPhase,
        data: Optional[Dict[str, Any]] = None,
    ) -> bool:
        """
        Transition to a new phase if valid

        Args:
            job_id: Job ID
            new_phase: Target phase
            data: Optional phase data

        Returns:
            True if transition successful
        """
        current_context = self.get_phase(job_id)

        if not current_context:
            # No current phase, allow initialization
            if new_phase == WorkflowPhase.INITIALIZATION:
                self.set_phase(job_id, new_phase, data)
                return True
            return False

        if self.can_transition_to(current_context.phase, new_phase):
            self.set_phase(job_id, new_phase, data)
            return True

        logger.warning(
            f"Invalid phase transition: {current_context.phase.value} -> {new_phase.value}"
        )
        return False

    def mark_phase_complete(
        self, job_id: str, data: Optional[Dict[str, Any]] = None
    ) -> bool:
        """
        Mark current phase as complete and transition to next

        Args:
            job_id: Job ID
            data: Optional completion data

        Returns:
            True if successful
        """
        current_context = self.get_phase(job_id)

        if not current_context:
            logger.error("No current phase to mark complete")
            return False

        # Mark current phase complete
        current_context.completed = True
        if data:
            current_context.data.update(data)

        # Persist completion
        self.set_phase(job_id, current_context.phase, current_context.data)

        # Determine next phase
        next_phase = self.get_next_phase_with_config(current_context.phase)

        if next_phase:
            return self.transition_to(job_id, next_phase)

        return True

    def get_next_phase_with_config(
        self, current_phase: WorkflowPhase
    ) -> Optional[WorkflowPhase]:
        """
        Get next phase considering configuration

        Args:
            current_phase: Current phase

        Returns:
            Next phase or None
        """
        if not self.config:
            return WorkflowPhase.get_next_phase(current_phase)

        # Handle phase skipping based on config
        next_phase = WorkflowPhase.get_next_phase(current_phase)

        while next_phase:
            # Skip query expansion if disabled
            if next_phase == WorkflowPhase.QUERY_EXPANSION and not getattr(
                self.config.workflow, "expand_queries", True
            ):
                next_phase = WorkflowPhase.get_next_phase(next_phase)
                continue

            # Skip URL review if disabled
            if next_phase == WorkflowPhase.URL_REVIEW and getattr(
                self.config.workflow, "skip_review", False
            ):
                next_phase = WorkflowPhase.get_next_phase(next_phase)
                continue

            # Skip analysis if disabled
            if next_phase == WorkflowPhase.ANALYSIS and not getattr(
                self.config.workflow, "generate_analysis", True
            ):
                next_phase = WorkflowPhase.get_next_phase(next_phase)
                continue

            return next_phase

        return None

    def get_resumable_phase(self, job_id: str) -> Optional[WorkflowPhase]:
        """
        Get the phase to resume from for a job

        Args:
            job_id: Job ID

        Returns:
            Phase to resume from or None
        """
        context = self.get_phase(job_id)

        if not context:
            return WorkflowPhase.INITIALIZATION

        # If current phase is not complete, resume from there
        if not context.completed and WorkflowPhase.can_resume_from(context.phase):
            return context.phase

        # Otherwise, resume from next phase
        next_phase = self.get_next_phase_with_config(context.phase)

        if next_phase and WorkflowPhase.can_resume_from(next_phase):
            return next_phase

        return None

    def get_phase_summary(self, job_id: str) -> Dict[str, Any]:
        """
        Get summary of all phases for a job

        Args:
            job_id: Job ID

        Returns:
            Dictionary with phase information
        """
        context = self.get_phase(job_id)

        if not context:
            return {
                "current_phase": None,
                "completed_phases": [],
                "next_phase": WorkflowPhase.INITIALIZATION.value,
                "can_resume": True,
            }

        # Determine completed phases
        completed_phases = []
        phase_order = [
            WorkflowPhase.INITIALIZATION,
            WorkflowPhase.QUERY_EXPANSION,
            WorkflowPhase.URL_COLLECTION,
            WorkflowPhase.URL_REVIEW,
            WorkflowPhase.CRAWLING,
            WorkflowPhase.BUNDLING,
            WorkflowPhase.ANALYSIS,
        ]

        for phase in phase_order:
            if phase == context.phase:
                if context.completed:
                    completed_phases.append(phase.value)
                break
            completed_phases.append(phase.value)

        next_phase = self.get_next_phase_with_config(context.phase)

        return {
            "current_phase": context.phase.value,
            "completed_phases": completed_phases,
            "next_phase": next_phase.value if next_phase else None,
            "can_resume": WorkflowPhase.can_resume_from(context.phase),
            "phase_data": context.data,
            "error": context.error,
        }
