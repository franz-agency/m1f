======= __init__.py ======
"""
m1f-research: AI-powered research extension for m1f

This module provides functionality to research any topic by:
- Using LLMs to find relevant URLs
- Scraping web content
- Converting HTML to Markdown
- Creating organized bundles from research findings
"""

from .cli import EnhancedResearchCommand, main
from .llm_interface import (
    LLMProvider,
    ClaudeProvider,
    ClaudeCodeProvider,
    GeminiProvider,
    CLIProvider,
    get_provider,
)
from .config import (
    ResearchConfig,
    LLMConfig,
    ScrapingConfig,
    OutputConfig,
    AnalysisConfig,
)
from .orchestrator import EnhancedResearchOrchestrator
from .models import ResearchResult, ScrapedContent, AnalyzedContent, ResearchSource
from .scraper import SmartScraper
from .content_filter import ContentFilter
from .analyzer import ContentAnalyzer
from .bundle_creator import SmartBundleCreator
from .readme_generator import ReadmeGenerator
from .analysis_templates import TEMPLATES, get_template
from .job_manager import JobManager
from .research_db import ResearchDatabase, JobDatabase, ResearchJob
from .url_manager import URLManager
from .smart_scraper import EnhancedSmartScraper

try:
    from .._version import __version__, __version_info__
except ImportError:
    # Fallback for when running as a script
    __version__ = "3.7.2"
    __version_info__ = (3, 7, 2)
__all__ = [
    # Version
    "__version__",
    "__version_info__",
    # CLI
    "EnhancedResearchCommand",
    "main",
    # LLM
    "LLMProvider",
    "ClaudeProvider",
    "GeminiProvider",
    "CLIProvider",
    "get_provider",
    # Config
    "ResearchConfig",
    "LLMConfig",
    "ScrapingConfig",
    "OutputConfig",
    "AnalysisConfig",
    # Core
    "EnhancedResearchOrchestrator",
    "SmartScraper",
    "EnhancedSmartScraper",
    "ContentFilter",
    "ContentAnalyzer",
    "SmartBundleCreator",
    "ReadmeGenerator",
    # Job Management
    "JobManager",
    "ResearchDatabase",
    "JobDatabase",
    "ResearchJob",
    "URLManager",
    # Models
    "ResearchResult",
    "ScrapedContent",
    "AnalyzedContent",
    "ResearchSource",
    # Templates
    "TEMPLATES",
    "get_template",
]

======= __main__.py ======
# Copyright 2025 Franz und Franz GmbH
# SPDX-License-Identifier: Apache-2.0

"""
Entry point for running m1f-research as a module
"""
from .cli import main

if __name__ == "__main__":
    main()

======= analysis_templates.py ======
# Copyright 2025 Franz und Franz GmbH
#
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#     http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.

"""
Analysis templates for different research types
"""
from typing import Dict, Any, List
from dataclasses import dataclass


@dataclass
class AnalysisTemplate:
    """Template for content analysis"""

    name: str
    description: str
    focus_areas: List[str]
    evaluation_criteria: Dict[str, float]  # criterion -> weight
    prompt_paths: Dict[str, str]  # analysis type -> prompt file path
    content_preferences: Dict[str, Any]


# Technical analysis template
TECHNICAL_TEMPLATE = AnalysisTemplate(
    name="technical",
    description="For implementation details and code examples",
    focus_areas=[
        "implementation_details",
        "code_examples",
        "performance_considerations",
        "best_practices",
        "common_pitfalls",
    ],
    evaluation_criteria={
        "code_quality": 0.3,
        "practical_examples": 0.3,
        "depth_of_explanation": 0.2,
        "accuracy": 0.2,
    },
    prompt_paths={
        "relevance": "analysis/technical_relevance.md",
        "key_points": "analysis/technical_key_points.md",
    },
    content_preferences={
        "prefer_code_examples": True,
        "min_code_ratio": 0.2,
        "preferred_content_types": ["tutorial", "documentation", "code"],
        "relevance_boost_keywords": [
            "implementation",
            "example",
            "code",
            "performance",
            "optimization",
            "pattern",
            "practice",
            "tutorial",
        ],
    },
)


# Academic analysis template
ACADEMIC_TEMPLATE = AnalysisTemplate(
    name="academic",
    description="For theoretical understanding and research papers",
    focus_areas=[
        "theoretical_foundations",
        "research_methodology",
        "citations_references",
        "empirical_evidence",
        "future_directions",
    ],
    evaluation_criteria={
        "theoretical_depth": 0.3,
        "citations_quality": 0.2,
        "methodology_rigor": 0.2,
        "novelty": 0.15,
        "clarity": 0.15,
    },
    prompt_paths={
        "relevance": "analysis/academic_relevance.md",
        "key_points": "analysis/academic_key_points.md",
    },
    content_preferences={
        "prefer_code_examples": False,
        "min_citation_count": 5,
        "preferred_content_types": ["research", "paper", "study", "analysis"],
        "relevance_boost_keywords": [
            "research",
            "study",
            "theory",
            "framework",
            "methodology",
            "findings",
            "conclusion",
            "hypothesis",
            "evidence",
        ],
    },
)


# Tutorial analysis template
TUTORIAL_TEMPLATE = AnalysisTemplate(
    name="tutorial",
    description="For step-by-step guides and learning resources",
    focus_areas=[
        "learning_progression",
        "clear_instructions",
        "practical_exercises",
        "prerequisite_coverage",
        "common_mistakes",
    ],
    evaluation_criteria={
        "clarity": 0.3,
        "completeness": 0.25,
        "practical_examples": 0.25,
        "learning_curve": 0.2,
    },
    prompt_paths={
        "relevance": "analysis/tutorial_relevance.md",
        "key_points": "analysis/tutorial_key_points.md",
    },
    content_preferences={
        "prefer_code_examples": True,
        "prefer_numbered_steps": True,
        "preferred_content_types": ["tutorial", "guide", "howto", "walkthrough"],
        "relevance_boost_keywords": [
            "step-by-step",
            "tutorial",
            "guide",
            "learn",
            "example",
            "exercise",
            "practice",
            "beginner",
            "getting started",
        ],
    },
)


# Reference analysis template
REFERENCE_TEMPLATE = AnalysisTemplate(
    name="reference",
    description="For API documentation and reference materials",
    focus_areas=[
        "api_completeness",
        "parameter_documentation",
        "return_value_specs",
        "usage_examples",
        "error_handling",
    ],
    evaluation_criteria={
        "completeness": 0.3,
        "accuracy": 0.3,
        "examples": 0.2,
        "organization": 0.2,
    },
    prompt_paths={
        "relevance": "analysis/reference_relevance.md",
        "key_points": "analysis/reference_key_points.md",
    },
    content_preferences={
        "prefer_code_examples": True,
        "prefer_structured_data": True,
        "preferred_content_types": ["documentation", "reference", "api", "spec"],
        "relevance_boost_keywords": [
            "api",
            "reference",
            "documentation",
            "parameters",
            "returns",
            "method",
            "function",
            "class",
            "interface",
            "specification",
        ],
    },
)


# General analysis template (default)
GENERAL_TEMPLATE = AnalysisTemplate(
    name="general",
    description="Balanced analysis for any topic",
    focus_areas=[
        "main_concepts",
        "practical_applications",
        "examples_illustrations",
        "pros_and_cons",
        "related_topics",
    ],
    evaluation_criteria={
        "relevance": 0.3,
        "clarity": 0.25,
        "depth": 0.25,
        "practicality": 0.2,
    },
    prompt_paths={
        "relevance": "analysis/general_relevance.md",
        "key_points": "analysis/general_key_points.md",
    },
    content_preferences={
        "prefer_code_examples": False,
        "balanced_content": True,
        "preferred_content_types": None,  # No preference
        "relevance_boost_keywords": [],
    },
)


# Template registry
TEMPLATES = {
    "technical": TECHNICAL_TEMPLATE,
    "academic": ACADEMIC_TEMPLATE,
    "tutorial": TUTORIAL_TEMPLATE,
    "reference": REFERENCE_TEMPLATE,
    "general": GENERAL_TEMPLATE,
}


def get_template(name: str) -> AnalysisTemplate:
    """Get analysis template by name"""
    return TEMPLATES.get(name, GENERAL_TEMPLATE)


def apply_template_scoring(
    template: AnalysisTemplate, analysis_results: Dict[str, Any]
) -> float:
    """Apply template-specific scoring weights to analysis results"""
    weighted_score = 0.0
    total_weight = 0.0

    # Map analysis results to template criteria
    criteria_scores = {
        "relevance": analysis_results.get("relevance_score", 5.0),
        "clarity": estimate_clarity_score(analysis_results),
        "completeness": estimate_completeness_score(analysis_results),
        "accuracy": analysis_results.get("technical_accuracy", 7.0),
        "practical_examples": estimate_example_score(analysis_results),
        "depth": estimate_depth_score(analysis_results),
    }

    # Apply template weights
    for criterion, weight in template.evaluation_criteria.items():
        if criterion in criteria_scores:
            weighted_score += criteria_scores[criterion] * weight
            total_weight += weight

    # Normalize to 0-10 scale
    return (weighted_score / total_weight) if total_weight > 0 else 5.0


def estimate_clarity_score(analysis: Dict[str, Any]) -> float:
    """Estimate clarity based on analysis metadata"""
    # Simple heuristic based on summary quality
    summary = analysis.get("summary", "")
    if len(summary) > 50 and len(summary) < 500:
        return 8.0
    return 6.0


def estimate_completeness_score(analysis: Dict[str, Any]) -> float:
    """Estimate completeness based on key points"""
    key_points = analysis.get("key_points", [])
    if len(key_points) >= 5:
        return 9.0
    elif len(key_points) >= 3:
        return 7.0
    return 5.0


def estimate_example_score(analysis: Dict[str, Any]) -> float:
    """Estimate quality of examples"""
    content_type = analysis.get("content_type", "")
    if content_type in ["tutorial", "code"]:
        return 8.0
    elif "example" in str(analysis.get("topics", [])).lower():
        return 7.0
    return 5.0


def estimate_depth_score(analysis: Dict[str, Any]) -> float:
    """Estimate depth of coverage"""
    # Based on technical level and key points
    level = analysis.get("technical_level", "intermediate")
    key_points = len(analysis.get("key_points", []))

    if level == "advanced" and key_points >= 4:
        return 9.0
    elif level == "intermediate" and key_points >= 3:
        return 7.0
    return 5.0

======= analyzer.py ======
# Copyright 2025 Franz und Franz GmbH
#
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#     http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.

"""
Content analysis using LLMs for m1f-research
"""
import asyncio
import logging
from typing import List, Dict, Any, Optional
from datetime import datetime
import re
import json

from .models import ScrapedContent, AnalyzedContent
from .llm_interface import LLMProvider
from .config import AnalysisConfig
from .analysis_templates import get_template, apply_template_scoring
from .prompt_utils import get_analysis_prompt, get_synthesis_prompt

logger = logging.getLogger(__name__)


class ContentAnalyzer:
    """
    LLM-powered content analysis with:
    - Relevance scoring (0-10)
    - Key points extraction
    - Content summarization
    - Content type detection
    - Topic extraction
    """

    def __init__(
        self,
        llm_provider: LLMProvider,
        config: AnalysisConfig,
        template_name: str = "general",
    ):
        self.llm = llm_provider
        self.config = config
        self.template = get_template(template_name)

    async def analyze_content(
        self,
        content_list: List[ScrapedContent],
        research_query: str,
        batch_size: int = 5,
    ) -> List[AnalyzedContent]:
        """
        Analyze scraped content for relevance and insights

        Args:
            content_list: List of scraped content to analyze
            research_query: Original research query for context
            batch_size: Number of items to analyze concurrently

        Returns:
            List of analyzed content with scores and insights
        """
        analyzed = []

        # Process in batches to avoid overwhelming the LLM
        for i in range(0, len(content_list), batch_size):
            batch = content_list[i : i + batch_size]

            # Analyze batch concurrently
            tasks = [
                self._analyze_single_content(item, research_query) for item in batch
            ]

            batch_results = await asyncio.gather(*tasks, return_exceptions=True)

            # Process results
            for item, result in zip(batch, batch_results):
                if isinstance(result, Exception):
                    logger.error(f"Analysis failed for {item.url}: {result}")
                    # Create fallback analyzed content
                    analyzed.append(self._create_fallback_analysis(item))
                else:
                    analyzed.append(result)

        return analyzed

    async def _analyze_single_content(
        self, content: ScrapedContent, research_query: str
    ) -> AnalyzedContent:
        """Analyze a single piece of content"""
        try:
            # Prepare content for analysis (truncate if needed)
            content_for_analysis = self._prepare_content(content.content)

            # Get comprehensive analysis from LLM
            analysis = await self._get_llm_analysis(
                content_for_analysis, research_query, content.url
            )

            # Create analyzed content
            return AnalyzedContent(
                url=content.url,
                title=content.title,
                content=content.content,
                relevance_score=analysis.get("relevance_score", 5.0),
                key_points=analysis.get("key_points", []),
                summary=analysis.get("summary", ""),
                content_type=analysis.get("content_type"),
                analysis_metadata=analysis,
            )

        except Exception as e:
            logger.error(f"Error analyzing {content.url}: {e}")
            return self._create_fallback_analysis(content)

    async def _get_llm_analysis(
        self, content: str, research_query: str, url: str
    ) -> Dict[str, Any]:
        """Get comprehensive analysis from LLM"""
        # Get template-specific or default analysis prompt
        prompt = get_analysis_prompt(
            template_name=self.template.name,
            prompt_type="relevance",
            query=research_query,
            url=url,
            content=content,
        )

        # Get analysis from LLM
        response = await self.llm.query(prompt)

        if response.error:
            raise Exception(f"LLM error: {response.error}")

        # Parse JSON response
        try:
            # Extract JSON from response
            json_str = self._extract_json(response.content)
            analysis = json.loads(json_str)

            # Validate and normalize the analysis
            analysis = self._validate_analysis(analysis)

            # Apply template-based scoring adjustments
            if self.template.name != "general":
                original_score = analysis["relevance_score"]
                template_adjusted_score = apply_template_scoring(
                    self.template, analysis
                )
                # Blend original and template scores
                analysis["relevance_score"] = (
                    original_score * 0.6 + template_adjusted_score * 0.4
                )
                analysis["template_score"] = template_adjusted_score

            return analysis

        except json.JSONDecodeError as e:
            logger.error(f"Failed to parse LLM response as JSON: {e}")
            # Try to extract what we can
            return self._extract_partial_analysis(response.content)

    def _prepare_content(self, content: str, max_length: int = 3000) -> str:
        """Prepare content for LLM analysis"""
        # Clean up content
        content = content.strip()

        # Remove excessive whitespace
        content = re.sub(r"\n{3,}", "\n\n", content)
        content = re.sub(r" {2,}", " ", content)

        # Truncate if too long
        if len(content) > max_length:
            # Try to truncate at a reasonable boundary
            truncated = content[:max_length]

            # Find last complete sentence
            last_period = truncated.rfind(".")
            if last_period > max_length * 0.8:
                content = truncated[: last_period + 1]
            else:
                content = truncated + "..."

        return content

    def _extract_json(self, text: str) -> str:
        """Extract JSON from LLM response"""
        # Remove markdown code blocks if present
        if "```json" in text:
            match = re.search(r"```json\s*(.*?)\s*```", text, re.DOTALL)
            if match:
                return match.group(1)

        # Try to find JSON object
        match = re.search(r"\{.*\}", text, re.DOTALL)
        if match:
            return match.group(0)

        return text

    def _validate_analysis(self, analysis: Dict[str, Any]) -> Dict[str, Any]:
        """Validate and normalize analysis results"""
        # Ensure required fields
        validated = {
            "relevance_score": float(analysis.get("relevance_score", 5.0)),
            "summary": str(analysis.get("summary", "")),
            "key_points": list(analysis.get("key_points", [])),
            "content_type": analysis.get("content_type", "unknown"),
            "topics": list(analysis.get("topics", [])),
            "technical_level": analysis.get("technical_level", "intermediate"),
            "strengths": analysis.get("strengths", ""),
            "limitations": analysis.get("limitations", ""),
        }

        # Clamp relevance score
        validated["relevance_score"] = max(0.0, min(10.0, validated["relevance_score"]))

        # Ensure key_points is a list of strings
        validated["key_points"] = [str(point) for point in validated["key_points"][:5]]

        # Validate content type
        valid_types = [
            "tutorial",
            "documentation",
            "blog",
            "discussion",
            "code",
            "reference",
            "news",
            "technical",
            "academic",
            "unknown",
        ]
        if validated["content_type"] not in valid_types:
            validated["content_type"] = "unknown"

        # Preserve any additional fields from the original analysis
        for key, value in analysis.items():
            if key not in validated:
                validated[key] = value

        return validated

    def _extract_partial_analysis(self, text: str) -> Dict[str, Any]:
        """Try to extract partial analysis from non-JSON response"""
        analysis = {
            "relevance_score": 5.0,
            "summary": "",
            "key_points": [],
            "content_type": "unknown",
        }

        # Try to extract relevance score
        score_match = re.search(r"relevance.*?(\d+(?:\.\d+)?)", text, re.IGNORECASE)
        if score_match:
            try:
                analysis["relevance_score"] = float(score_match.group(1))
            except:
                pass

        # Try to extract summary
        summary_match = re.search(r"summary[:\s]+(.*?)(?:\n|$)", text, re.IGNORECASE)
        if summary_match:
            analysis["summary"] = summary_match.group(1).strip()

        # Try to extract bullet points as key points
        bullets = re.findall(r"[-•*]\s+(.+?)(?:\n|$)", text)
        if bullets:
            analysis["key_points"] = bullets[:5]

        return analysis

    def _create_fallback_analysis(self, content: ScrapedContent) -> AnalyzedContent:
        """Create fallback analysis when LLM analysis fails"""
        # Basic heuristic analysis
        word_count = len(content.content.split())
        has_code = bool(re.search(r"```|`[^`]+`", content.content))

        # Estimate relevance based on title
        relevance = 5.0

        # Extract first paragraph as summary
        paragraphs = content.content.split("\n\n")
        summary = paragraphs[0][:200] + "..." if paragraphs else "No summary available"

        return AnalyzedContent(
            url=content.url,
            title=content.title,
            content=content.content,
            relevance_score=relevance,
            key_points=[],
            summary=summary,
            content_type="code" if has_code else "unknown",
            analysis_metadata={"fallback": True, "word_count": word_count},
        )

    async def extract_topics(
        self, analyzed_content: List[AnalyzedContent]
    ) -> Dict[str, List[str]]:
        """Extract and group topics from analyzed content"""
        all_topics = []

        for item in analyzed_content:
            topics = item.analysis_metadata.get("topics", [])
            all_topics.extend(topics)

        # Count topic frequency
        from collections import Counter

        topic_counts = Counter(all_topics)

        # Group by frequency
        grouped = {
            "primary": [t for t, c in topic_counts.items() if c >= 3],
            "secondary": [t for t, c in topic_counts.items() if c == 2],
            "mentioned": [t for t, c in topic_counts.items() if c == 1],
        }

        return grouped

    async def generate_synthesis(
        self, analyzed_content: List[AnalyzedContent], research_query: str
    ) -> str:
        """Generate a synthesis of all analyzed content"""
        if not analyzed_content:
            return "No content available for synthesis."

        # Prepare content summaries
        summaries = []
        for item in analyzed_content[:10]:  # Limit to top 10
            summaries.append(
                f"- {item.title} (Relevance: {item.relevance_score}): {item.summary}"
            )

        prompt = get_synthesis_prompt(
            query=research_query, summaries=chr(10).join(summaries)
        )

        response = await self.llm.query(prompt)

        if response.error:
            return "Unable to generate synthesis."

        return response.content

======= bundle_creator.py ======
# Copyright 2025 Franz und Franz GmbH
#
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#     http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.

"""
Smart bundle creation with intelligent content organization for m1f-research
"""
import re
import logging
from pathlib import Path
from typing import List, Dict, Any, Optional, Tuple
from datetime import datetime
import json
from collections import defaultdict, Counter

from ..m1f.file_operations import (
    safe_open,
)

from .models import AnalyzedContent
from .config import OutputConfig, ResearchConfig
from .llm_interface import LLMProvider
from .readme_generator import ReadmeGenerator
from .prompt_utils import get_subtopic_grouping_prompt, get_topic_summary_prompt

logger = logging.getLogger(__name__)


class SmartBundleCreator:
    """
    Intelligent bundle creation with:
    - Subtopic grouping and organization
    - Hierarchical content structuring
    - Smart navigation generation
    - Cross-reference linking
    - Summary synthesis per topic
    """

    def __init__(
        self,
        llm_provider: Optional[LLMProvider] = None,
        config: Optional[OutputConfig] = None,
        research_config: Optional[ResearchConfig] = None,
    ):
        self.llm = llm_provider
        self.config = config or OutputConfig()
        self.research_config = research_config

    async def create_bundle(
        self,
        content_list: List[AnalyzedContent],
        research_query: str,
        output_dir: Path,
        synthesis: Optional[str] = None,
    ) -> Path:
        """
        Create an intelligently organized research bundle

        Args:
            content_list: List of analyzed content to include
            research_query: Original research query
            output_dir: Directory to save bundle
            synthesis: Optional pre-generated synthesis

        Returns:
            Path to the created bundle file
        """
        # Group content by subtopics
        topic_groups = await self._group_by_subtopics(content_list, research_query)

        # Generate bundle structure
        bundle_content = await self._generate_bundle_content(
            topic_groups, research_query, synthesis
        )

        # Write bundle file
        bundle_path = output_dir / f"{self.config.bundle_prefix}-bundle.md"
        with safe_open(bundle_path, "w", encoding="utf-8") as f:
            if f:
                f.write(bundle_content)

        # Create supplementary files if enabled
        if self.config.create_index:
            await self._create_index_file(topic_groups, output_dir)

        if self.config.include_metadata:
            await self._create_metadata_file(content_list, research_query, output_dir)

        # Generate README if we have research config
        if self.research_config:
            readme_gen = ReadmeGenerator(self.research_config)
            await readme_gen.generate_readme(
                content_list=content_list,
                research_query=research_query,
                output_dir=output_dir,
                topic_groups=topic_groups,
                synthesis=synthesis,
            )

            # Also generate citations file
            await readme_gen.generate_citation_file(
                content_list, research_query, output_dir
            )

        logger.info(f"Created smart bundle at: {bundle_path}")
        return bundle_path

    async def _group_by_subtopics(
        self, content_list: List[AnalyzedContent], research_query: str
    ) -> Dict[str, List[AnalyzedContent]]:
        """Group content by subtopics using content analysis"""
        if not self.llm:
            # Fallback to simple grouping by content type
            return self._simple_grouping(content_list)

        # Extract topics from all content
        all_topics = []
        for item in content_list:
            topics = item.analysis_metadata.get("topics", [])
            all_topics.extend([(topic, item) for topic in topics])

        # If we have topics from analysis, use them
        if all_topics:
            return self._group_by_extracted_topics(all_topics, content_list)

        # Otherwise, use LLM to identify subtopics
        return await self._llm_group_by_subtopics(content_list, research_query)

    def _simple_grouping(
        self, content_list: List[AnalyzedContent]
    ) -> Dict[str, List[AnalyzedContent]]:
        """Simple grouping by content type"""
        groups = defaultdict(list)

        for item in content_list:
            content_type = item.content_type or "general"
            groups[content_type].append(item)

        # Sort items within each group by relevance
        for group in groups.values():
            group.sort(key=lambda x: x.relevance_score, reverse=True)

        return dict(groups)

    def _group_by_extracted_topics(
        self,
        topic_items: List[Tuple[str, AnalyzedContent]],
        all_content: List[AnalyzedContent],
    ) -> Dict[str, List[AnalyzedContent]]:
        """Group content by extracted topics"""
        # Count topic frequencies
        topic_counts = Counter(topic for topic, _ in topic_items)

        # Get top topics (those appearing in multiple documents)
        top_topics = [
            topic for topic, count in topic_counts.most_common(10) if count >= 2
        ]

        # Create groups
        groups = defaultdict(list)
        assigned = set()

        # Assign content to most relevant topic
        for item in all_content:
            item_topics = item.analysis_metadata.get("topics", [])

            # Find best matching top topic
            best_topic = None
            for topic in top_topics:
                if topic in item_topics:
                    best_topic = topic
                    break

            if best_topic:
                groups[best_topic].append(item)
                assigned.add(item.url)

        # Add ungrouped items to "Other" category
        other_items = [item for item in all_content if item.url not in assigned]
        if other_items:
            groups["Other Resources"].extend(other_items)

        # Sort items within each group by relevance
        for group in groups.values():
            group.sort(key=lambda x: x.relevance_score, reverse=True)

        return dict(groups)

    async def _llm_group_by_subtopics(
        self, content_list: List[AnalyzedContent], research_query: str
    ) -> Dict[str, List[AnalyzedContent]]:
        """Use LLM to identify and group by subtopics"""
        # Prepare content summaries for LLM
        summaries = []
        for i, item in enumerate(content_list):
            summaries.append(f"{i}. {item.title}: {item.summary[:100]}...")

        prompt = get_subtopic_grouping_prompt(
            query=research_query, summaries=chr(10).join(summaries)
        )

        try:
            response = await self.llm.query(prompt)

            if response.error:
                logger.error(f"LLM error during grouping: {response.error}")
                return self._simple_grouping(content_list)

            # Parse JSON response
            import re

            json_match = re.search(r"\{.*\}", response.content, re.DOTALL)
            if json_match:
                grouping_data = json.loads(json_match.group(0))

                # Create groups based on LLM response
                groups = {}
                used_indices = set()

                for subtopic in grouping_data.get("subtopics", []):
                    name = subtopic["name"]
                    indices = subtopic.get("item_indices", [])

                    groups[name] = []
                    for idx in indices:
                        if 0 <= idx < len(content_list) and idx not in used_indices:
                            groups[name].append(content_list[idx])
                            used_indices.add(idx)

                # Add any ungrouped items
                ungrouped = [
                    item for i, item in enumerate(content_list) if i not in used_indices
                ]
                if ungrouped:
                    groups["Other Resources"] = ungrouped

                # Sort items within each group
                for group in groups.values():
                    group.sort(key=lambda x: x.relevance_score, reverse=True)

                return groups

        except Exception as e:
            logger.error(f"Error in LLM grouping: {e}")

        # Fallback to simple grouping
        return self._simple_grouping(content_list)

    async def _generate_bundle_content(
        self,
        topic_groups: Dict[str, List[AnalyzedContent]],
        research_query: str,
        synthesis: Optional[str] = None,
    ) -> str:
        """Generate the bundle content with smart organization"""
        lines = []

        # Header
        lines.append(f"# Research: {research_query}")
        lines.append(f"\nGenerated on: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")

        # Statistics
        total_sources = sum(len(items) for items in topic_groups.values())
        lines.append(f"Total sources: {total_sources}")
        lines.append(f"Topics covered: {len(topic_groups)}")
        lines.append("\n---\n")

        # Executive Summary
        if self.config.create_summary:
            lines.append("## Executive Summary\n")

            if synthesis:
                lines.append(synthesis)
                lines.append("\n")

            # Topic overview
            lines.append("### Topics Covered:\n")
            for topic, items in topic_groups.items():
                avg_relevance = sum(item.relevance_score for item in items) / len(items)
                lines.append(
                    f"- **{topic}** ({len(items)} sources, avg relevance: {avg_relevance:.1f}/10)"
                )
            lines.append("\n---\n")

        # Table of Contents
        if self.config.create_index:
            lines.append("## Table of Contents\n")

            # Topic-based navigation
            for i, (topic, items) in enumerate(topic_groups.items(), 1):
                topic_anchor = self._create_anchor(topic)
                lines.append(f"{i}. [{topic}](#{topic_anchor}) ({len(items)} sources)")

                # Show top items under each topic
                if len(items) > 0:
                    for j, item in enumerate(items[:3], 1):  # Show top 3
                        item_anchor = self._create_anchor(f"{topic}-{j}-{item.title}")
                        lines.append(f"   - [{item.title[:60]}...](#{item_anchor})")
                    if len(items) > 3:
                        lines.append(f"   - ...and {len(items) - 3} more")

            lines.append("\n---\n")

        # Content sections by topic
        for topic_idx, (topic, items) in enumerate(topic_groups.items(), 1):
            topic_anchor = self._create_anchor(topic)
            lines.append(f"## {topic_idx}. {topic}\n")

            # Topic summary if we have LLM
            if self.llm and len(items) > 2:
                topic_summary = await self._generate_topic_summary(topic, items)
                if topic_summary:
                    lines.append(f"*{topic_summary}*\n")

            # Items in this topic
            for item_idx, item in enumerate(items, 1):
                item_anchor = self._create_anchor(f"{topic}-{item_idx}-{item.title}")
                lines.append(f"### {topic_idx}.{item_idx}. {item.title}\n")

                # Metadata
                lines.append(f"**Source:** {item.url}")
                lines.append(f"**Relevance:** {item.relevance_score}/10")
                if item.content_type:
                    lines.append(f"**Type:** {item.content_type}")
                lines.append("")

                # Key points
                if item.key_points:
                    lines.append("**Key Points:**")
                    for point in item.key_points:
                        lines.append(f"- {point}")
                    lines.append("")

                # Summary
                if item.summary:
                    lines.append("**Summary:**")
                    lines.append(item.summary)
                    lines.append("")

                # Content
                lines.append("**Content:**")
                lines.append(item.content)
                lines.append("\n---\n")

        # Cross-references section
        if self.config.create_index:
            cross_refs = self._identify_cross_references(topic_groups)
            if cross_refs:
                lines.append("## Cross-References\n")
                lines.append("Topics that appear across multiple sources:\n")
                for term, locations in cross_refs.items():
                    if len(locations) > 1:
                        lines.append(
                            f"- **{term}**: appears in {len(locations)} sources"
                        )
                lines.append("\n")

        return "\n".join(lines)

    async def _generate_topic_summary(
        self, topic: str, items: List[AnalyzedContent]
    ) -> Optional[str]:
        """Generate a summary for a specific topic"""
        if not self.llm or not items:
            return None

        # Prepare item summaries
        summaries = [f"- {item.title}: {item.summary[:100]}..." for item in items[:5]]

        prompt = get_topic_summary_prompt(
            topic=topic, summaries=chr(10).join(summaries)
        )

        try:
            response = await self.llm.query(prompt)
            if not response.error:
                return response.content.strip()
        except Exception as e:
            logger.error(f"Error generating topic summary: {e}")

        return None

    def _create_anchor(self, text: str) -> str:
        """Create a valid markdown anchor from text"""
        # Remove special characters and convert to lowercase
        anchor = re.sub(r"[^\w\s-]", "", text.lower())
        # Replace spaces with hyphens
        anchor = re.sub(r"\s+", "-", anchor)
        # Remove duplicate hyphens
        anchor = re.sub(r"-+", "-", anchor)
        # Trim hyphens from ends
        return anchor.strip("-")

    def _identify_cross_references(
        self, topic_groups: Dict[str, List[AnalyzedContent]]
    ) -> Dict[str, List[str]]:
        """Identify terms that appear across multiple topics"""
        term_locations = defaultdict(set)

        # Extract key terms from each topic group
        for topic, items in topic_groups.items():
            for item in items:
                # Extract from key points
                for point in item.key_points:
                    # Simple term extraction (in production, use NLP)
                    terms = re.findall(r"\b[A-Z][a-z]+(?:\s+[A-Z][a-z]+)*\b", point)
                    for term in terms:
                        if len(term) > 3:  # Skip short terms
                            term_locations[term].add(topic)

        # Filter to terms appearing in multiple topics
        cross_refs = {
            term: list(locations)
            for term, locations in term_locations.items()
            if len(locations) > 1
        }

        return cross_refs

    async def _create_index_file(
        self, topic_groups: Dict[str, List[AnalyzedContent]], output_dir: Path
    ):
        """Create a separate index file for navigation"""
        index_path = output_dir / "index.md"

        lines = []
        lines.append("# Research Index\n")
        lines.append("## By Topic\n")

        for topic, items in topic_groups.items():
            lines.append(f"### {topic}")
            for item in items:
                lines.append(
                    f"- [{item.title}]({item.url}) (Relevance: {item.relevance_score}/10)"
                )
            lines.append("")

        lines.append("## By Relevance\n")
        all_items = []
        for items in topic_groups.values():
            all_items.extend(items)
        all_items.sort(key=lambda x: x.relevance_score, reverse=True)

        for item in all_items[:20]:  # Top 20
            lines.append(f"- {item.relevance_score}/10: [{item.title}]({item.url})")

        with safe_open(index_path, "w", encoding="utf-8") as f:
            if f:
                f.write("\n".join(lines))

    async def _create_metadata_file(
        self, content_list: List[AnalyzedContent], research_query: str, output_dir: Path
    ):
        """Create metadata JSON file"""
        metadata = {
            "query": research_query,
            "generated_at": datetime.now().isoformat(),
            "statistics": {
                "total_sources": len(content_list),
                "average_relevance": (
                    sum(item.relevance_score for item in content_list)
                    / len(content_list)
                    if content_list
                    else 0
                ),
                "content_types": dict(
                    Counter(item.content_type or "unknown" for item in content_list)
                ),
            },
            "sources": [
                {
                    "url": item.url,
                    "title": item.title,
                    "relevance_score": item.relevance_score,
                    "content_type": item.content_type,
                    "key_points": item.key_points,
                    "topics": item.analysis_metadata.get("topics", []),
                }
                for item in content_list
            ],
        }

        metadata_path = output_dir / "metadata.json"
        with safe_open(metadata_path, "w", encoding="utf-8") as f:
            if f:
                f.write(json.dumps(metadata, indent=2))

======= cli.py ======
# Copyright 2025 Franz und Franz GmbH
#
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#     http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.

"""
Enhanced CLI interface for m1f-research with improved UX
"""

import argparse
import sys
import os
from pathlib import Path
from typing import Optional, List, Dict, Any
import asyncio
import logging
from datetime import datetime

from ..m1f.file_operations import (
    safe_exists,
)

from .config import ResearchConfig
from .orchestrator import EnhancedResearchOrchestrator
from .output import OutputFormatter, ProgressTracker

# Use unified colorama module
try:
    from ..shared.colors import Colors, ColoredHelpFormatter, COLORAMA_AVAILABLE, info
except ImportError:
    # Fallback to local implementation
    from .output import Colors, COLORAMA_AVAILABLE

    def info(msg):
        print(msg)

    class ColoredHelpFormatter(argparse.RawDescriptionHelpFormatter):
        """Fallback help formatter with colors if available."""

        def _format_action_invocation(self, action: argparse.Action) -> str:
            """Format action with colors."""
            parts = super()._format_action_invocation(action)

            if COLORAMA_AVAILABLE:
                # Color the option names
                parts = parts.replace("-", f"{Colors.CYAN}-")
                parts = f"{parts}{Colors.RESET}"

            return parts

        def _format_usage(
            self, usage: str, actions, groups, prefix: Optional[str]
        ) -> str:
            """Format usage line with colors."""
            result = super()._format_usage(usage, actions, groups, prefix)

            if COLORAMA_AVAILABLE and result:
                # Highlight the program name
                prog_name = self._prog
                colored_prog = f"{Colors.GREEN}{prog_name}{Colors.RESET}"
                result = result.replace(prog_name, colored_prog, 1)

            return result


# Import version
try:
    from .._version import __version__
except ImportError:
    __version__ = "3.8.0"


class EnhancedResearchCommand:
    """Enhanced CLI with better user experience"""

    def __init__(self):
        self.parser = self._create_parser()
        self.formatter: Optional[OutputFormatter] = None

    def _create_parser(self) -> argparse.ArgumentParser:
        """Create enhanced argument parser"""
        parser = argparse.ArgumentParser(
            prog="m1f-research",
            description="AI-powered research tool with advanced job management",
            formatter_class=ColoredHelpFormatter,
            epilog=f"""
{Colors.BOLD}Examples:{Colors.RESET}
  {Colors.CYAN}# Start new research{Colors.RESET}
  m1f-research "microservices best practices"
  
  {Colors.CYAN}# List jobs with filters{Colors.RESET}
  m1f-research --list-jobs --search "python" --limit 10
  
  {Colors.CYAN}# Resume with progress tracking{Colors.RESET}
  m1f-research --resume abc123 --verbose
  
  {Colors.CYAN}# JSON output for automation{Colors.RESET}
  m1f-research --list-jobs --format json | jq '.[] | select(.status=="completed")'
  
  {Colors.CYAN}# Clean up old data{Colors.RESET}
  m1f-research --clean-raw abc123

{Colors.BOLD}For more help:{Colors.RESET}
  m1f-research --help-examples    # More usage examples
  m1f-research --help-filters     # Filtering guide
  m1f-research --help-providers   # LLM provider setup
""",
        )

        # Main query
        parser.add_argument(
            "query", nargs="?", help="Research query (required for new jobs)"
        )

        # Output format
        output_group = parser.add_argument_group("output options")
        output_group.add_argument(
            "--format",
            choices=["text", "json"],
            default="text",
            help="Output format (default: text)",
        )

        output_group.add_argument(
            "--quiet", "-q", action="store_true", help="Suppress non-error output"
        )

        output_group.add_argument(
            "--verbose",
            "-v",
            action="count",
            default=0,
            help="Increase verbosity (-vv for debug)",
        )

        output_group.add_argument(
            "--no-color", action="store_true", help="Disable colored output"
        )

        # Help extensions
        help_group = parser.add_argument_group("extended help")
        help_group.add_argument(
            "--help-examples", action="store_true", help="Show extended examples"
        )

        help_group.add_argument(
            "--help-filters", action="store_true", help="Show filtering guide"
        )

        help_group.add_argument(
            "--help-providers",
            action="store_true",
            help="Show LLM provider setup guide",
        )

        # Job management
        job_group = parser.add_argument_group("job management")
        job_group.add_argument(
            "--resume", metavar="JOB_ID", help="Resume an existing research job"
        )

        job_group.add_argument(
            "--list-jobs", action="store_true", help="List all research jobs"
        )

        job_group.add_argument(
            "--status", metavar="JOB_ID", help="Show detailed job status"
        )

        job_group.add_argument(
            "--watch", metavar="JOB_ID", help="Watch job progress in real-time"
        )

        job_group.add_argument(
            "--urls-file", type=Path, help="File containing URLs to add (one per line)"
        )

        # List filters
        filter_group = parser.add_argument_group("filtering options")
        filter_group.add_argument("--limit", type=int, help="Limit number of results")

        filter_group.add_argument(
            "--offset", type=int, default=0, help="Offset for pagination"
        )

        filter_group.add_argument("--date", help="Filter by date (Y-M-D, Y-M, or Y)")

        filter_group.add_argument("--search", help="Search jobs by query term")

        filter_group.add_argument(
            "--status-filter",
            choices=["active", "completed", "failed"],
            help="Filter by job status",
        )

        # Data management
        data_group = parser.add_argument_group("data management")
        data_group.add_argument(
            "--clean-raw", metavar="JOB_ID", help="Clean raw HTML data for a job"
        )

        data_group.add_argument(
            "--clean-all-raw",
            action="store_true",
            help="Clean raw HTML data for all jobs",
        )

        data_group.add_argument(
            "--export", metavar="JOB_ID", help="Export job data to JSON"
        )

        # Research options
        research_group = parser.add_argument_group("research options")
        research_group.add_argument(
            "--urls",
            type=int,
            default=20,
            help="Number of URLs to search for (default: 20)",
        )

        research_group.add_argument(
            "--scrape",
            type=int,
            default=10,
            help="Maximum URLs to scrape (default: 10)",
        )

        research_group.add_argument(
            "--provider",
            "-p",
            choices=["claude", "claude-code", "gemini", "gemini-cli"],
            default="claude",
            help="LLM provider to use",
        )

        research_group.add_argument("--model", "-m", help="Specific model to use")

        research_group.add_argument(
            "--template",
            "-t",
            choices=["general", "technical", "academic", "tutorial", "reference"],
            default="general",
            help="Analysis template",
        )

        # Behavior options
        behavior_group = parser.add_argument_group("behavior options")
        behavior_group.add_argument(
            "--output",
            "-o",
            type=Path,
            default=Path("./research-data"),
            help="Output directory",
        )

        behavior_group.add_argument(
            "--name", "-n", help="Custom name for research bundle"
        )

        behavior_group.add_argument(
            "--config", "-c", type=Path, help="Configuration file path"
        )

        behavior_group.add_argument(
            "--interactive", "-i", action="store_true", help="Start in interactive mode"
        )

        behavior_group.add_argument(
            "--no-filter", action="store_true", help="Disable content filtering"
        )

        behavior_group.add_argument(
            "--no-analysis", action="store_true", help="Skip AI analysis"
        )

        behavior_group.add_argument(
            "--concurrent", type=int, default=5, help="Max concurrent operations"
        )

        behavior_group.add_argument(
            "--dry-run", action="store_true", help="Preview without executing"
        )

        behavior_group.add_argument(
            "--yes", "-y", action="store_true", help="Answer yes to all prompts"
        )

        # Version
        parser.add_argument(
            "--version", action="version", version=f"%(prog)s {__version__}"
        )

        return parser

    def _validate_args(self, args) -> Optional[str]:
        """Validate arguments and return error message if invalid"""
        # Check for conflicting options
        if args.resume and args.query:
            return "Cannot specify both query and --resume"

        # Check required args for operations
        if not any(
            [
                args.query,
                args.resume,
                args.list_jobs,
                args.status,
                args.clean_raw,
                args.clean_all_raw,
                args.export,
                args.watch,
                args.help_examples,
                args.help_filters,
                args.help_providers,
                args.interactive,
            ]
        ):
            # Default to interactive mode when no operation specified
            args.interactive = True

        # Validate URLs file if provided
        if args.urls_file and not safe_exists(args.urls_file):
            return f"URLs file not found: {args.urls_file}"

        # Validate numeric ranges
        if args.urls < 0:
            return "--urls must be non-negative"

        if args.scrape < 0:
            return "--scrape must be non-negative"

        if args.concurrent < 1:
            return "--concurrent must be at least 1"

        if args.limit and args.limit < 1:
            return "--limit must be positive"

        if args.offset < 0:
            return "--offset must be non-negative"

        # Validate date format
        if args.date:
            if not self._validate_date_format(args.date):
                return f"Invalid date format: {args.date}. Use Y-M-D, Y-M, or Y"

        return None

    def _validate_date_format(self, date_str: str) -> bool:
        """Validate date format"""
        try:
            if len(date_str) == 10:  # Y-M-D
                datetime.strptime(date_str, "%Y-%m-%d")
            elif len(date_str) == 7:  # Y-M
                datetime.strptime(date_str, "%Y-%m")
            elif len(date_str) == 4:  # Y
                datetime.strptime(date_str, "%Y")
            else:
                return False
            return True
        except ValueError:
            return False

    async def run(self, args=None):
        """Run the CLI with enhanced output"""
        args = self.parser.parse_args(args)

        # Handle extended help
        if args.help_examples:
            self._show_examples()
            return 0

        if args.help_filters:
            self._show_filters_guide()
            return 0

        if args.help_providers:
            self._show_providers_guide()
            return 0

        # Setup formatter
        if args.no_color:
            Colors.disable()

        self.formatter = OutputFormatter(
            format=args.format, verbose=args.verbose, quiet=args.quiet
        )

        # Validate arguments
        error = self._validate_args(args)
        if error:
            self.formatter.error(error)
            return 1

        # Setup logging
        self._setup_logging(args)

        try:
            # Route to appropriate handler
            if args.list_jobs:
                return await self._list_jobs(args)
            elif args.status:
                return await self._show_status(args)
            elif args.watch:
                return await self._watch_job(args)
            elif args.clean_raw:
                return await self._clean_raw(args)
            elif args.clean_all_raw:
                return await self._clean_all_raw(args)
            elif args.export:
                return await self._export_job(args)
            elif args.interactive:
                return await self._interactive_mode(args)
            else:
                return await self._run_research(args)

        except KeyboardInterrupt:
            self.formatter.warning("Interrupted by user")
            return 130
        except Exception as e:
            self.formatter.error(str(e))
            if args.verbose > 0:
                import traceback

                traceback.print_exc()
            return 1
        finally:
            self.formatter.cleanup()

    def _setup_logging(self, args):
        """Setup logging based on verbosity"""
        if args.format == "json":
            # Suppress all logging in JSON mode
            logging.disable(logging.CRITICAL)
        else:
            level = logging.WARNING
            if args.verbose == 1:
                level = logging.INFO
            elif args.verbose >= 2:
                level = logging.DEBUG

            logging.basicConfig(
                level=level,
                format="%(asctime)s - %(name)s - %(levelname)s - %(message)s",
            )

    def _show_examples(self):
        """Show extended examples"""
        examples = """
# Research Workflows

## Basic Research
m1f-research "python async programming"

## Research with Custom Settings
m1f-research "react hooks" \\
  --urls 50 \\
  --scrape 25 \\
  --template technical \\
  --output ~/research

## Using Manual URLs
# Create URL list
cat > urls.txt << EOF
https://docs.python.org/3/library/asyncio.html
https://realpython.com/async-io-python/
EOF

# Use in research
m1f-research "python async" --urls-file urls.txt

## Job Management

# List recent jobs
m1f-research --list-jobs --limit 10

# Find specific research
m1f-research --list-jobs --search "react" --date 2025-07

# Resume interrupted job
m1f-research --resume abc123

# Add more URLs to existing job
m1f-research --resume abc123 --urls-file more-urls.txt

## Automation

# Export job data
m1f-research --export abc123 > job-data.json

# List completed jobs as JSON
m1f-research --list-jobs --status-filter completed --format json

# Batch processing
for topic in "react hooks" "vue composition" "angular signals"; do
  m1f-research "$topic" --quiet
done

## Interactive Research
m1f-research --interactive
"""
        info(examples)

    def _show_filters_guide(self):
        """Show filtering guide"""
        guide = """
# Filtering Guide

## Date Filtering

Filter jobs by creation date:

  # Specific day
  m1f-research --list-jobs --date 2025-07-23
  
  # Specific month
  m1f-research --list-jobs --date 2025-07
  
  # Specific year
  m1f-research --list-jobs --date 2025

## Search Filtering

Find jobs by query content:

  # Simple search
  m1f-research --list-jobs --search "python"
  
  # Case-insensitive
  m1f-research --list-jobs --search "REACT"
  
  # Partial matches
  m1f-research --list-jobs --search "async"

## Status Filtering

Filter by job status:

  # Only completed jobs
  m1f-research --list-jobs --status-filter completed
  
  # Only failed jobs
  m1f-research --list-jobs --status-filter failed
  
  # Active jobs
  m1f-research --list-jobs --status-filter active

## Pagination

Handle large result sets:

  # First page (10 items)
  m1f-research --list-jobs --limit 10
  
  # Second page
  m1f-research --list-jobs --limit 10 --offset 10
  
  # Large page
  m1f-research --list-jobs --limit 50

## Combined Filters

Combine multiple filters:

  # Python jobs from July 2025
  m1f-research --list-jobs \\
    --search "python" \\
    --date 2025-07 \\
    --limit 20
  
  # Completed React jobs
  m1f-research --list-jobs \\
    --search "react" \\
    --status-filter completed \\
    --limit 10
"""
        info(guide)

    def _show_providers_guide(self):
        """Show providers setup guide"""
        guide = """
# LLM Provider Setup Guide

## Claude (Anthropic)

1. Get API key from https://console.anthropic.com/
2. Set environment variable:
   export ANTHROPIC_API_KEY="your-key-here"
3. Use in research:
   m1f-research "topic" --provider claude --model claude-3-opus-20240229

## Gemini (Google)

1. Get API key from https://makersuite.google.com/app/apikey
2. Set environment variable:
   export GOOGLE_API_KEY="your-key-here"
3. Use in research:
   m1f-research "topic" --provider gemini --model gemini-1.5-pro

## OpenAI

1. Get API key from https://platform.openai.com/api-keys
2. Set environment variable:
   export OPENAI_API_KEY="your-key-here"
3. Use in research:
   m1f-research "topic" --provider openai --model gpt-4

## CLI Providers

For enhanced integration:

# Claude Code SDK (uses proper Claude Code SDK integration)
m1f-research "topic" --provider claude-cli

# Gemini CLI (requires gemini-cli installed) 
m1f-research "topic" --provider gemini-cli

## Configuration File

Set default provider in .m1f.config.yml:

```yaml
research:
  llm:
    provider: claude
    model: claude-3-opus-20240229
```
"""
        info(guide)

    async def _run_research(self, args):
        """Run research with progress tracking"""
        config = self._create_config(args)
        orchestrator = EnhancedResearchOrchestrator(config)

        # Show research plan
        self.formatter.header(
            f"🔍 Research: {args.query or 'Resuming job'}",
            f"Provider: {config.llm.provider} | URLs: {config.scraping.search_limit} | Scrape: {config.scraping.scrape_limit}",
        )

        # Add progress callback
        def progress_callback(phase: str, current: int, total: int):
            if phase == "searching":
                self.formatter.info(f"Searching for URLs... ({current}/{total})")
            elif phase == "scraping":
                self.formatter.progress(current, total, "Scraping URLs")
            elif phase == "analyzing":
                self.formatter.progress(current, total, "Analyzing content")

        # Set callback if orchestrator supports it
        if hasattr(orchestrator, "set_progress_callback"):
            orchestrator.set_progress_callback(progress_callback)

        # Run research
        result = await orchestrator.research(
            query=args.query, job_id=args.resume, urls_file=args.urls_file
        )

        # Show results
        self.formatter.success("Research completed!")

        if self.formatter.format == "json":
            self.formatter._json_buffer.append(
                {
                    "type": "result",
                    "job_id": result.job_id,
                    "output_dir": str(result.output_dir),
                    "urls_found": result.urls_found,
                    "pages_scraped": len(result.scraped_content),
                    "pages_analyzed": len(result.analyzed_content),
                    "bundle_created": result.bundle_created,
                }
            )
        else:
            self.formatter.info(f"Job ID: {result.job_id}")
            self.formatter.info(f"Output: {result.output_dir}")
            self.formatter.list_item(f"URLs found: {result.urls_found}")
            self.formatter.list_item(f"Pages scraped: {len(result.scraped_content)}")
            self.formatter.list_item(f"Pages analyzed: {len(result.analyzed_content)}")

            if result.bundle_created:
                bundle_path = result.output_dir / "RESEARCH_BUNDLE.md"
                self.formatter.success(f"Research bundle: {bundle_path}")

        return 0

    async def _list_jobs(self, args):
        """List jobs with enhanced formatting"""
        from .job_manager import JobManager

        job_manager = JobManager(args.output)

        # Get total count
        total_count = job_manager.count_jobs(
            status=args.status_filter, date_filter=args.date, search_term=args.search
        )

        # Get jobs
        jobs = job_manager.list_jobs(
            status=args.status_filter,
            limit=args.limit,
            offset=args.offset,
            date_filter=args.date,
            search_term=args.search,
        )

        if not jobs:
            self.formatter.info("No jobs found matching criteria")
            return 0

        # Format for display
        if self.formatter.format == "json":
            self.formatter._json_buffer = jobs
        else:
            # Build filter description
            filters = []
            if args.search:
                filters.append(f"search: '{args.search}'")
            if args.date:
                filters.append(f"date: {args.date}")
            if args.status_filter:
                filters.append(f"status: {args.status_filter}")

            filter_str = f" (filtered by {', '.join(filters)})" if filters else ""

            # Show header
            if args.limit:
                page = (args.offset // args.limit) + 1 if args.limit else 1
                total_pages = (
                    (total_count + args.limit - 1) // args.limit if args.limit else 1
                )
                self.formatter.header(
                    f"📋 Research Jobs - Page {page}/{total_pages}",
                    f"Showing {len(jobs)} of {total_count}{filter_str}",
                )
            else:
                self.formatter.header(
                    f"📋 Research Jobs ({total_count} total{filter_str})"
                )

            # Prepare table data
            headers = ["ID", "Status", "Query", "Created", "Stats"]
            rows = []

            for job in jobs:
                stats = job["stats"]
                stats_str = f"{stats['scraped_urls']}/{stats['total_urls']}"
                if stats["analyzed_urls"]:
                    stats_str += f" ({stats['analyzed_urls']})"

                # Format status with color
                status = job["status"]
                if status == "completed":
                    status_display = f"{Colors.GREEN}{status}{Colors.RESET}"
                elif status == "active":
                    status_display = f"{Colors.YELLOW}{status}{Colors.RESET}"
                else:
                    status_display = f"{Colors.RED}{status}{Colors.RESET}"

                rows.append(
                    [
                        job["job_id"][:8],
                        status_display,
                        job["query"][:40],
                        job["created_at"][:16],
                        stats_str,
                    ]
                )

            self.formatter.table(headers, rows, highlight_search=args.search)

            # Pagination hints
            if args.limit and total_count > args.limit:
                self.formatter.info("")
                if args.offset + args.limit < total_count:
                    next_offset = args.offset + args.limit
                    self.formatter.info(f"Next page: --offset {next_offset}")
                if args.offset > 0:
                    prev_offset = max(0, args.offset - args.limit)
                    self.formatter.info(f"Previous page: --offset {prev_offset}")

        return 0

    async def _show_status(self, args):
        """Show job status with enhanced formatting"""
        from .job_manager import JobManager

        job_manager = JobManager(args.output)

        job = job_manager.get_job(args.status)
        if not job:
            self.formatter.error(f"Job not found: {args.status}")
            return 1

        info = job_manager.get_job_info(job)

        if self.formatter.format == "json":
            self.formatter._json_buffer.append(info)
        else:
            self.formatter.job_status(info)

        return 0

    async def _clean_raw(self, args):
        """Clean raw data with confirmation"""
        from .job_manager import JobManager

        job_manager = JobManager(args.output)

        if not args.yes:
            if not self.formatter.confirm(f"Clean raw data for job {args.clean_raw}?"):
                self.formatter.info("Cancelled")
                return 0

        self.formatter.info(f"Cleaning raw data for job {args.clean_raw}...")

        stats = await job_manager.cleanup_job_raw_data(args.clean_raw)

        if "error" in stats:
            self.formatter.error(stats["error"])
            return 1

        self.formatter.success(
            f"Cleaned {stats.get('html_files_deleted', 0)} files, "
            f"freed {stats.get('space_freed_mb', 0)} MB"
        )

        return 0

    async def _clean_all_raw(self, args):
        """Clean all raw data with confirmation"""
        from .job_manager import JobManager

        job_manager = JobManager(args.output)

        if not args.yes:
            if not self.formatter.confirm(
                "⚠️  This will delete ALL raw HTML data. Continue?", default=False
            ):
                self.formatter.info("Cancelled")
                return 0

        self.formatter.info("Cleaning raw data for all jobs...")

        # Show progress
        all_jobs = job_manager.list_jobs()
        progress = ProgressTracker(self.formatter, len(all_jobs), "Cleaning jobs")

        stats = {"jobs_cleaned": 0, "files_deleted": 0, "space_freed_mb": 0}

        for i, job_info in enumerate(all_jobs):
            try:
                job_stats = await job_manager.cleanup_job_raw_data(job_info["job_id"])
                if "error" not in job_stats:
                    stats["jobs_cleaned"] += 1
                    stats["files_deleted"] += job_stats.get("html_files_deleted", 0)
                    stats["space_freed_mb"] += job_stats.get("space_freed_mb", 0)
            except Exception as e:
                self.formatter.debug(f"Error cleaning {job_info['job_id']}: {e}")

            progress.update()

        progress.complete("Cleanup complete")

        self.formatter.success(
            f"Cleaned {stats['jobs_cleaned']} jobs, "
            f"{stats['files_deleted']} files, "
            f"freed {stats['space_freed_mb']:.1f} MB"
        )

        return 0

    async def _export_job(self, args):
        """Export job data to JSON"""
        from .job_manager import JobManager

        job_manager = JobManager(args.output)

        job = job_manager.get_job(args.export)
        if not job:
            self.formatter.error(f"Job not found: {args.export}")
            return 1

        info = job_manager.get_job_info(job)

        # Add content if available
        job_db = job_manager.get_job_database(job)
        content = job_db.get_content_for_bundle()
        info["content"] = content

        # Output as JSON
        import json

        print(json.dumps(info, indent=2, default=str))

        return 0

    async def _watch_job(self, args):
        """Watch job progress in real-time"""
        from .job_manager import JobManager
        import time

        job_manager = JobManager(args.output)

        self.formatter.info(f"Watching job {args.watch}... (Ctrl+C to stop)")

        last_stats = None
        while True:
            try:
                job = job_manager.get_job(args.watch)
                if not job:
                    self.formatter.error(f"Job not found: {args.watch}")
                    return 1

                info = job_manager.get_job_info(job)
                stats = info["stats"]

                # Check if stats changed
                if stats != last_stats:
                    # Clear screen and show status
                    os.system("clear" if os.name == "posix" else "cls")
                    self.formatter.job_status(info)
                    last_stats = stats

                # Check if job is complete
                if info["status"] in ["completed", "failed"]:
                    break

                # Wait before next check
                await asyncio.sleep(2)

            except KeyboardInterrupt:
                break

        return 0

    async def _interactive_mode(self, args):
        """Run in interactive mode"""
        self.formatter.header("🔍 m1f-research Interactive Mode")
        self.formatter.info("Type 'help' for commands, 'exit' to quit\n")

        while True:
            try:
                command = input(f"{Colors.CYAN}research> {Colors.RESET}").strip()

                if not command:
                    continue

                if command.lower() in ["exit", "quit"]:
                    break

                if command.lower() == "help":
                    self._show_interactive_help()
                    continue

                # Parse command
                parts = command.split()
                if parts[0] == "research":
                    # New research
                    query = " ".join(parts[1:])
                    # Create new args with the query, avoiding duplicate 'query' key
                    research_args = vars(args).copy()
                    research_args["query"] = query
                    research_args["resume"] = None
                    research_args["urls_file"] = None
                    await self._run_research(argparse.Namespace(**research_args))
                elif parts[0] == "list":
                    # List jobs
                    await self._list_jobs(args)
                elif parts[0] == "status" and len(parts) > 1:
                    # Show status
                    args.status = parts[1]
                    await self._show_status(args)
                elif parts[0] == "resume" and len(parts) > 1:
                    # Resume job
                    args.resume = parts[1]
                    args.query = None
                    await self._run_research(args)
                else:
                    self.formatter.warning(f"Unknown command: {command}")
                    self.formatter.info("Type 'help' for available commands")

            except KeyboardInterrupt:
                info("")  # New line after ^C
                continue
            except EOFError:
                break

        self.formatter.info("\n👋 Goodbye!")
        return 0

    def _show_interactive_help(self):
        """Show interactive mode help"""
        help_text = """
Available commands:
  research <query>     Start new research
  list                 List all jobs
  status <job_id>      Show job status
  resume <job_id>      Resume a job
  help                 Show this help
  exit/quit           Exit interactive mode

Examples:
  research python async programming
  list
  status abc123
  resume abc123
"""
        info(help_text)

    def _create_config(self, args) -> ResearchConfig:
        """Create configuration from arguments"""
        # Load base config
        if args.config and safe_exists(args.config):
            config = ResearchConfig.from_yaml(args.config)
        else:
            config = ResearchConfig()

        # Apply CLI overrides
        config.llm.provider = args.provider
        if args.model:
            config.llm.model = args.model

        # Set scraping config properly
        config.url_count = args.urls
        config.scrape_count = args.scrape
        config.scraping.max_concurrent = args.concurrent
        config.scraping.search_limit = args.urls  # Add for compatibility
        config.scraping.scrape_limit = args.scrape  # Add for compatibility

        config.output.directory = args.output
        if args.name:
            config.output.name = args.name

        # Set template in analysis config
        config.template = args.template

        config.interactive = args.interactive
        config.no_filter = args.no_filter
        config.no_analysis = args.no_analysis
        config.dry_run = args.dry_run
        config.verbose = args.verbose

        return config


def main():
    """Main entry point"""
    cli = EnhancedResearchCommand()
    sys.exit(asyncio.run(cli.run()))


if __name__ == "__main__":
    main()

======= config.py ======
# Copyright 2025 Franz und Franz GmbH
#
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#     http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.

"""
Configuration management for m1f-research
"""

from dataclasses import dataclass, field
from typing import Dict, List, Optional, Any
from pathlib import Path
import yaml
import os
from argparse import Namespace

from ..m1f.file_operations import (
    safe_read_text,
)


@dataclass
class LLMConfig:
    """LLM provider configuration"""

    provider: str = "claude"
    model: Optional[str] = None
    api_key_env: Optional[str] = None
    temperature: float = 0.7
    max_tokens: int = 4096
    cli_command: Optional[str] = None  # For CLI providers
    cli_args: List[str] = field(default_factory=list)


@dataclass
class ScrapingConfig:
    """Web scraping configuration"""

    search_limit: int = 20  # Number of URLs to search for
    scrape_limit: int = 10  # Maximum URLs to scrape
    timeout_range: str = "1-3"  # seconds
    timeout: int = 30  # Total timeout for requests in seconds
    delay: List[float] = field(
        default_factory=lambda: [1.0, 3.0]
    )  # delay range in seconds
    max_concurrent: int = 5
    retry_attempts: int = 2
    retries: int = 2  # Number of retries for failed requests
    user_agents: List[str] = field(
        default_factory=lambda: [
            "Mozilla/5.0 (m1f-research/0.1.0) AppleWebKit/537.36",
            "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36",
        ]
    )
    respect_robots_txt: bool = True
    headers: Dict[str, str] = field(default_factory=dict)


@dataclass
class OutputConfig:
    """Output configuration"""

    directory: Path = Path("./m1f/research")
    create_summary: bool = True
    create_index: bool = True
    bundle_prefix: str = "research"
    format: str = "markdown"
    include_metadata: bool = True


@dataclass
class AnalysisConfig:
    """Content analysis configuration"""

    relevance_threshold: float = 7.0
    duplicate_threshold: float = 0.8
    min_content_length: int = 100
    max_content_length: Optional[int] = None
    prefer_code_examples: bool = False
    prioritize_recent: bool = True
    language: str = "en"


@dataclass
class ResearchTemplate:
    """Research template configuration"""

    name: str
    description: str
    sources: List[str] = field(default_factory=list)
    analysis_focus: str = "general"
    url_count: int = 20
    scrape_count: int = 10
    analysis_config: Optional[AnalysisConfig] = None


@dataclass
class ResearchConfig:
    """Main research configuration"""

    # Core settings
    query: Optional[str] = None
    url_count: int = 20
    scrape_count: int = 10

    # Component configs
    llm: LLMConfig = field(default_factory=LLMConfig)
    scraping: ScrapingConfig = field(default_factory=ScrapingConfig)
    output: OutputConfig = field(default_factory=OutputConfig)
    analysis: AnalysisConfig = field(default_factory=AnalysisConfig)
    filtering: AnalysisConfig = field(
        default_factory=AnalysisConfig
    )  # For content filtering

    # Behavior settings
    interactive: bool = False
    no_filter: bool = False
    no_analysis: bool = False
    dry_run: bool = False
    verbose: int = 0

    # Templates
    template: str = "general"
    templates: Dict[str, ResearchTemplate] = field(default_factory=dict)

    @classmethod
    def from_yaml(cls, path: Path) -> "ResearchConfig":
        """Load configuration from YAML file"""
        content = safe_read_text(path)
        data = yaml.safe_load(content)

        # Extract research section
        research_data = data.get("research", {})

        # Parse LLM config
        llm_data = research_data.get("llm", {})
        llm_config = LLMConfig(
            provider=llm_data.get("provider", "claude"),
            model=llm_data.get("model"),
            api_key_env=llm_data.get("api_key_env"),
            temperature=llm_data.get("temperature", 0.7),
            max_tokens=llm_data.get("max_tokens", 4096),
            cli_command=llm_data.get("cli_command"),
            cli_args=llm_data.get("cli_args", []),
        )

        # Parse CLI tools config
        if "cli_tools" in research_data:
            cli_tools = research_data["cli_tools"]
            if llm_config.provider in cli_tools:
                tool_config = cli_tools[llm_config.provider]
                llm_config.cli_command = tool_config.get("command", llm_config.provider)
                llm_config.cli_args = tool_config.get("args", [])

        # Parse scraping config
        scraping_data = research_data.get("scraping", {})
        scraping_config = ScrapingConfig(
            timeout_range=scraping_data.get("timeout_range", "1-3"),
            max_concurrent=scraping_data.get("max_concurrent", 5),
            retry_attempts=scraping_data.get("retry_attempts", 2),
            user_agents=scraping_data.get("user_agents", ScrapingConfig().user_agents),
            respect_robots_txt=scraping_data.get("respect_robots_txt", True),
            headers=scraping_data.get("headers", {}),
        )

        # Parse output config
        output_data = research_data.get("output", {})
        output_config = OutputConfig(
            directory=Path(output_data.get("directory", "./research-data")),
            create_summary=output_data.get("create_summary", True),
            create_index=output_data.get("create_index", True),
            bundle_prefix=output_data.get("bundle_prefix", "research"),
            format=output_data.get("format", "markdown"),
            include_metadata=output_data.get("include_metadata", True),
        )

        # Parse analysis config
        analysis_data = research_data.get("analysis", {})
        analysis_config = AnalysisConfig(
            relevance_threshold=analysis_data.get("relevance_threshold", 7.0),
            duplicate_threshold=analysis_data.get("duplicate_threshold", 0.8),
            min_content_length=analysis_data.get("min_content_length", 100),
            max_content_length=analysis_data.get("max_content_length"),
            prefer_code_examples=analysis_data.get("prefer_code_examples", False),
            prioritize_recent=analysis_data.get("prioritize_recent", True),
            language=analysis_data.get("language", "en"),
        )

        # Parse templates
        templates = {}
        templates_data = research_data.get("templates", {})
        for name, template_data in templates_data.items():
            # Create template-specific analysis config if provided
            template_analysis = None
            if "analysis" in template_data:
                ta = template_data["analysis"]
                template_analysis = AnalysisConfig(
                    relevance_threshold=ta.get(
                        "relevance_threshold", analysis_config.relevance_threshold
                    ),
                    duplicate_threshold=ta.get(
                        "duplicate_threshold", analysis_config.duplicate_threshold
                    ),
                    min_content_length=ta.get(
                        "min_content_length", analysis_config.min_content_length
                    ),
                    max_content_length=ta.get(
                        "max_content_length", analysis_config.max_content_length
                    ),
                    prefer_code_examples=ta.get(
                        "prefer_code_examples", analysis_config.prefer_code_examples
                    ),
                    prioritize_recent=ta.get(
                        "prioritize_recent", analysis_config.prioritize_recent
                    ),
                    language=ta.get("language", analysis_config.language),
                )

            templates[name] = ResearchTemplate(
                name=name,
                description=template_data.get("description", ""),
                sources=template_data.get("sources", ["web"]),
                analysis_focus=template_data.get("analysis_focus", "general"),
                url_count=template_data.get("url_count", 20),
                scrape_count=template_data.get("scrape_count", 10),
                analysis_config=template_analysis,
            )

        # Get defaults
        defaults = research_data.get("defaults", {})

        return cls(
            url_count=defaults.get("url_count", 20),
            scrape_count=defaults.get("scrape_count", 10),
            llm=llm_config,
            scraping=scraping_config,
            output=output_config,
            analysis=analysis_config,
            templates=templates,
        )

    @classmethod
    def from_args(cls, args: Namespace) -> "ResearchConfig":
        """Create configuration from command line arguments"""
        config = cls()

        # Basic settings
        config.query = args.query
        config.url_count = args.urls
        config.scrape_count = args.scrape
        config.interactive = args.interactive
        config.no_filter = args.no_filter
        config.no_analysis = args.no_analysis
        config.dry_run = args.dry_run
        config.verbose = args.verbose
        config.template = args.template

        # Load from config file if provided
        if args.config:
            base_config = cls.from_yaml(args.config)
            # Merge with base config
            config.llm = base_config.llm
            config.scraping = base_config.scraping
            config.output = base_config.output
            config.analysis = base_config.analysis
            config.templates = base_config.templates

        # Override with command line args
        config.llm.provider = args.provider
        if args.model:
            config.llm.model = args.model

        # Output settings
        config.output.directory = args.output
        if args.name:
            config.output.bundle_prefix = args.name

        # Scraping settings
        config.scraping.max_concurrent = args.concurrent

        # Apply template if specified
        if config.template and config.template in config.templates:
            template = config.templates[config.template]
            config.url_count = template.url_count
            config.scrape_count = template.scrape_count
            if template.analysis_config:
                config.analysis = template.analysis_config

        # Set API key from environment if not set
        if not config.llm.api_key_env:
            if config.llm.provider == "claude":
                config.llm.api_key_env = "ANTHROPIC_API_KEY"
            elif config.llm.provider == "gemini":
                config.llm.api_key_env = "GOOGLE_API_KEY"

        return config

    def to_yaml(self) -> str:
        """Convert configuration to YAML string"""
        data = {
            "research": {
                "defaults": {
                    "url_count": self.url_count,
                    "scrape_count": self.scrape_count,
                },
                "llm": {
                    "provider": self.llm.provider,
                    "model": self.llm.model,
                    "api_key_env": self.llm.api_key_env,
                    "temperature": self.llm.temperature,
                    "max_tokens": self.llm.max_tokens,
                },
                "scraping": {
                    "timeout_range": self.scraping.timeout_range,
                    "max_concurrent": self.scraping.max_concurrent,
                    "retry_attempts": self.scraping.retry_attempts,
                    "user_agents": self.scraping.user_agents,
                    "respect_robots_txt": self.scraping.respect_robots_txt,
                },
                "output": {
                    "directory": str(self.output.directory),
                    "create_summary": self.output.create_summary,
                    "create_index": self.output.create_index,
                    "bundle_prefix": self.output.bundle_prefix,
                    "format": self.output.format,
                },
                "analysis": {
                    "relevance_threshold": self.analysis.relevance_threshold,
                    "duplicate_threshold": self.analysis.duplicate_threshold,
                    "min_content_length": self.analysis.min_content_length,
                    "max_content_length": self.analysis.max_content_length,
                    "prefer_code_examples": self.analysis.prefer_code_examples,
                    "prioritize_recent": self.analysis.prioritize_recent,
                    "language": self.analysis.language,
                },
                "templates": {
                    name: {
                        "description": template.description,
                        "sources": template.sources,
                        "analysis_focus": template.analysis_focus,
                        "url_count": template.url_count,
                        "scrape_count": template.scrape_count,
                    }
                    for name, template in self.templates.items()
                },
            }
        }

        return yaml.dump(data, default_flow_style=False, sort_keys=False)

    def get_timeout_range(self) -> tuple[float, float]:
        """Parse timeout range string to min/max values"""
        parts = self.scraping.timeout_range.split("-")
        if len(parts) == 2:
            return float(parts[0]), float(parts[1])
        else:
            val = float(parts[0])
            return val, val

======= content_filter.py ======
# Copyright 2025 Franz und Franz GmbH
#
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#     http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.

"""
Content filtering and quality assessment for m1f-research
"""
import re
import hashlib
from typing import List, Dict, Optional, Tuple
from collections import Counter
import logging

from .models import ScrapedContent, AnalyzedContent
from .config import AnalysisConfig

logger = logging.getLogger(__name__)


class ContentFilter:
    """
    Advanced content filtering with:
    - Content length validation
    - Language detection
    - Spam/ad detection
    - Code-to-text ratio analysis
    - Duplicate detection
    - Quality scoring
    """

    def __init__(self, config: AnalysisConfig):
        self.config = config
        self.seen_hashes = set()
        self.spam_patterns = self._load_spam_patterns()
        self.quality_indicators = self._load_quality_indicators()

    def filter_scraped_content(
        self, content_list: List[ScrapedContent]
    ) -> List[ScrapedContent]:
        """Filter scraped content based on quality criteria"""
        filtered = []

        for content in content_list:
            # Check if content passes all filters
            if self._passes_filters(content):
                filtered.append(content)
            else:
                logger.debug(f"Filtered out: {content.url}")

        logger.info(f"Filtered {len(content_list)} to {len(filtered)} items")
        return filtered

    def filter_analyzed_content(
        self, content_list: List[AnalyzedContent]
    ) -> List[AnalyzedContent]:
        """Filter analyzed content based on relevance and quality"""
        filtered = []

        for content in content_list:
            # Check relevance threshold
            if content.relevance_score < self.config.relevance_threshold:
                logger.debug(
                    f"Below relevance threshold: {content.url} ({content.relevance_score})"
                )
                continue

            # Check content length
            if not self._check_content_length(content.content):
                continue

            # Check for duplicates
            if self._is_duplicate(content.content):
                logger.debug(f"Duplicate content: {content.url}")
                continue

            filtered.append(content)

        return filtered

    def filter_content(self, content: str) -> Tuple[bool, str]:
        """
        Filter a single content item and return pass/fail with reason.

        Args:
            content: Content string to check

        Returns:
            Tuple of (passed: bool, reason: str)
        """
        # Check content length
        if not self._check_content_length(content):
            return (
                False,
                f"Content too short (min: {self.config.min_content_length} chars)",
            )

        # Check for spam/ads
        if self._is_spam(content):
            return False, "Spam/advertising content detected"

        # Check quality score
        quality_score = self._calculate_quality_score(content)
        if quality_score < 0.3:  # Minimum quality threshold
            return False, f"Low quality content (score: {quality_score:.2f})"

        # Check for duplicates
        if self._is_duplicate(content):
            return False, "Duplicate content"

        return True, "Passed all filters"

    def _passes_filters(self, content: ScrapedContent) -> bool:
        """Check if content passes all quality filters"""
        # Check content length
        if not self._check_content_length(content.content):
            return False

        # Check language (if configured)
        if self.config.language != "any":
            detected_lang = self._detect_language(content.content)
            if detected_lang != self.config.language:
                logger.debug(
                    f"Wrong language: {content.url} (detected: {detected_lang})"
                )
                return False

        # Check for spam/ads
        if self._is_spam(content.content):
            logger.debug(f"Spam detected: {content.url}")
            return False

        # Check quality score
        quality_score = self._calculate_quality_score(content.content)
        logger.debug(f"Quality score for {content.url}: {quality_score:.2f}")
        if quality_score < 0.3:  # Minimum quality threshold
            logger.debug(f"Low quality: {content.url} (score: {quality_score:.2f})")
            return False

        # Check for duplicates
        if self._is_duplicate(content.content):
            logger.debug(f"Duplicate: {content.url}")
            return False

        return True

    def _check_content_length(self, content: str) -> bool:
        """Check if content length is within acceptable range"""
        length = len(content)

        if length < self.config.min_content_length:
            return False

        if self.config.max_content_length and length > self.config.max_content_length:
            return False

        return True

    def _detect_language(self, content: str) -> str:
        """Simple language detection based on common words"""
        # This is a simplified implementation
        # In production, use langdetect or similar library

        english_words = {"the", "and", "is", "in", "to", "of", "a", "for", "with", "on"}
        spanish_words = {"el", "la", "de", "en", "y", "a", "los", "las", "un", "una"}
        french_words = {"le", "de", "la", "et", "à", "les", "un", "une", "dans", "pour"}
        german_words = {
            "der",
            "die",
            "das",
            "und",
            "in",
            "von",
            "zu",
            "mit",
            "den",
            "ein",
        }

        # Extract words
        words = re.findall(r"\b\w+\b", content.lower())[:200]  # Check first 200 words
        word_set = set(words)

        # Count matches
        scores = {
            "en": len(word_set.intersection(english_words)),
            "es": len(word_set.intersection(spanish_words)),
            "fr": len(word_set.intersection(french_words)),
            "de": len(word_set.intersection(german_words)),
        }

        # Return language with highest score
        if max(scores.values()) > 0:
            return max(scores, key=scores.get)

        return "unknown"

    def _is_spam(self, content: str) -> bool:
        """Detect spam/ad content using patterns and heuristics"""
        content_lower = content.lower()

        # Check spam patterns
        spam_score = 0
        for pattern in self.spam_patterns:
            if pattern in content_lower:
                spam_score += 1

        # Check for excessive links
        links = re.findall(r"https?://[^\s]+", content)
        if len(links) > 20:  # Too many links
            spam_score += 2

        # Check for excessive capitalization
        if len(re.findall(r"[A-Z]{5,}", content)) > 10:
            spam_score += 1

        # Check for repeated phrases
        phrases = re.findall(r"\b\w+\s+\w+\s+\w+\b", content_lower)
        phrase_counts = Counter(phrases)
        if any(count >= 5 for count in phrase_counts.values()):
            spam_score += 1

        # Check for common spam indicators
        spam_indicators = [
            r"click here now",
            r"limited time offer",
            r"act now",
            r"100% free",
            r"no credit card",
            r"make money fast",
            r"work from home",
            r"congratulations you",
            r"you have been selected",
        ]

        for indicator in spam_indicators:
            if re.search(indicator, content_lower):
                spam_score += 2

        return spam_score >= 3

    def _calculate_quality_score(self, content: str) -> float:
        """Calculate overall quality score (0-1)"""
        scores = []

        # Content structure score
        structure_score = self._score_structure(content)
        scores.append(structure_score)

        # Readability score
        readability_score = self._score_readability(content)
        scores.append(readability_score)

        # Information density score
        density_score = self._score_information_density(content)
        scores.append(density_score)

        # Code quality score (for technical content)
        if self.config.prefer_code_examples:
            code_score = self._score_code_content(content)
            scores.append(code_score)

        return sum(scores) / len(scores)

    def _score_structure(self, content: str) -> float:
        """Score content structure (headings, paragraphs, lists)"""
        score = 0.5  # Base score

        # Check for headings
        headings = re.findall(r"^#{1,6}\s+.+", content, re.MULTILINE)
        if headings:
            score += min(len(headings) * 0.05, 0.2)

        # Check for lists
        lists = re.findall(r"^[\*\-]\s+.+", content, re.MULTILINE)
        if lists:
            score += min(len(lists) * 0.02, 0.1)

        # Check for code blocks
        code_blocks = re.findall(r"```[\s\S]*?```", content)
        if code_blocks:
            score += min(len(code_blocks) * 0.05, 0.2)

        return min(score, 1.0)

    def _score_readability(self, content: str) -> float:
        """Score content readability"""
        # Simple readability metrics
        sentences = re.split(r"[.!?]+", content)
        words = re.findall(r"\b\w+\b", content)

        if not sentences or not words:
            return 0.0

        # Average sentence length
        avg_sentence_length = len(words) / len(sentences)

        # Ideal range is 15-25 words per sentence
        if 15 <= avg_sentence_length <= 25:
            sentence_score = 1.0
        elif 10 <= avg_sentence_length <= 30:
            sentence_score = 0.7
        else:
            sentence_score = 0.4

        # Check for paragraph breaks
        paragraphs = re.split(r"\n\n+", content)
        if len(paragraphs) > 3:
            paragraph_score = 1.0
        else:
            paragraph_score = 0.5

        return (sentence_score + paragraph_score) / 2

    def _score_information_density(self, content: str) -> float:
        """Score information density and uniqueness"""
        words = re.findall(r"\b\w+\b", content.lower())

        if not words:
            return 0.0

        # Vocabulary richness
        unique_words = set(words)
        vocabulary_ratio = len(unique_words) / len(words)

        # Good range is 0.3-0.7
        if 0.3 <= vocabulary_ratio <= 0.7:
            vocab_score = 1.0
        elif 0.2 <= vocabulary_ratio <= 0.8:
            vocab_score = 0.7
        else:
            vocab_score = 0.4

        # Check for meaningful content (not just filler)
        meaningful_words = [w for w in words if len(w) > 3]
        meaningful_ratio = len(meaningful_words) / len(words)

        content_score = min(meaningful_ratio * 1.5, 1.0)

        return (vocab_score + content_score) / 2

    def _score_code_content(self, content: str) -> float:
        """Score code content quality and ratio"""
        # Find code blocks
        code_blocks = re.findall(r"```[\s\S]*?```", content)
        inline_code = re.findall(r"`[^`]+`", content)

        total_length = len(content)
        code_length = sum(len(block) for block in code_blocks) + sum(
            len(code) for code in inline_code
        )

        if total_length == 0:
            return 0.0

        code_ratio = code_length / total_length

        # For technical content, ideal code ratio is 0.2-0.5
        if 0.2 <= code_ratio <= 0.5:
            return 1.0
        elif 0.1 <= code_ratio <= 0.6:
            return 0.7
        elif code_ratio > 0:
            return 0.5
        else:
            return 0.2  # No code in technical content

    def _is_duplicate(self, content: str) -> bool:
        """Check if content is duplicate using content hashing"""
        # Normalize content for comparison
        normalized = self._normalize_content(content)

        # Create content hash
        content_hash = hashlib.sha256(normalized.encode()).hexdigest()

        if content_hash in self.seen_hashes:
            return True

        self.seen_hashes.add(content_hash)

        # Also check for near-duplicates using similarity
        # This is a simplified check - in production use more sophisticated methods
        for seen_hash in list(self.seen_hashes)[-10:]:  # Check last 10
            # Would implement similarity comparison here
            pass

        return False

    def _normalize_content(self, content: str) -> str:
        """Normalize content for duplicate detection"""
        # Remove extra whitespace
        normalized = re.sub(r"\s+", " ", content)

        # Remove punctuation for comparison
        normalized = re.sub(r"[^\w\s]", "", normalized)

        # Convert to lowercase
        normalized = normalized.lower().strip()

        return normalized

    def _load_spam_patterns(self) -> List[str]:
        """Load common spam patterns"""
        return [
            "viagra",
            "cialis",
            "casino",
            "poker",
            "lottery",
            "weight loss",
            "diet pills",
            "forex",
            "binary options",
            "get rich quick",
            "mlm",
            "work from home",
            "click here now",
            "buy now",
            "order now",
            "unsubscribe",
            "opt out",
            "remove me",
        ]

    def _load_quality_indicators(self) -> Dict[str, float]:
        """Load positive quality indicators"""
        return {
            "tutorial": 0.2,
            "guide": 0.2,
            "documentation": 0.3,
            "example": 0.2,
            "implementation": 0.2,
            "best practices": 0.3,
            "how to": 0.2,
            "reference": 0.2,
            "api": 0.1,
            "framework": 0.1,
        }

    def get_filter_stats(self) -> Dict[str, int]:
        """Get filtering statistics"""
        return {
            "total_seen": len(self.seen_hashes),
            "duplicate_checks": len(self.seen_hashes),
        }

======= job_manager.py ======
# Copyright 2025 Franz und Franz GmbH
#
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#     http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.

"""
Job management for m1f-research with persistence and resume support
"""
import logging
from pathlib import Path
from typing import Optional, Dict, Any, List
from datetime import datetime

from ..m1f.file_operations import (
    safe_exists,
    safe_mkdir,
)

from .research_db import ResearchDatabase, JobDatabase, ResearchJob
from .config import ResearchConfig

logger = logging.getLogger(__name__)


class JobManager:
    """Manages research jobs with persistence"""

    def __init__(self, base_dir: Path):
        self.base_dir = base_dir
        # Note: We'll need to call async init separately
        # For now, keep sync behavior for compatibility

        # Main research database
        self.main_db = ResearchDatabase(self.base_dir / "research_jobs.db")

    def create_job(self, query: str, config: ResearchConfig) -> ResearchJob:
        """Create a new research job"""
        # Create output directory with hierarchical structure
        now = datetime.now()
        output_dir = self.base_dir / now.strftime("%Y/%m/%d")

        # Create job with serializable config
        config_dict = self._serialize_config(config)
        job = ResearchJob.create_new(
            query=query, config=config_dict, output_dir=str(output_dir)
        )

        # Update output directory to include job ID
        job.output_dir = str(
            output_dir / f"{job.job_id}_{self._sanitize_query(query)[:30]}"
        )

        # Save to database
        self.main_db.create_job(job)

        # Create job directory
        job_path = Path(job.output_dir)
        # Note: We'll need to call safe_mkdir separately for full async support
        job_path.mkdir(parents=True, exist_ok=True)

        # Create job-specific database
        job_db = JobDatabase(job_path / "research.db")

        logger.info(f"Created job {job.job_id}: {query}")
        logger.info(f"Output directory: {job.output_dir}")

        return job

    def get_job(self, job_id: str) -> Optional[ResearchJob]:
        """Get an existing job by ID"""
        job = self.main_db.get_job(job_id)
        if not job:
            logger.error(f"Job {job_id} not found")
        return job

    def get_job_database(self, job: ResearchJob) -> JobDatabase:
        """Get the database for a specific job"""
        job_path = Path(job.output_dir)
        return JobDatabase(job_path / "research.db")

    def update_job_status(self, job_id: str, status: str):
        """Update job status"""
        self.main_db.update_job_status(job_id, status)
        logger.info(f"Updated job {job_id} status to: {status}")

    def update_job_stats(self, job: ResearchJob, **additional_stats):
        """Update job statistics from job database"""
        job_db = self.get_job_database(job)
        stats = job_db.get_stats()
        stats.update(additional_stats)
        self.main_db.update_job_stats(job.job_id, **stats)

    def list_jobs(
        self,
        status: Optional[str] = None,
        limit: Optional[int] = None,
        offset: int = 0,
        date_filter: Optional[str] = None,
        search_term: Optional[str] = None,
    ) -> List[Dict[str, Any]]:
        """List jobs with advanced filtering"""
        return self.main_db.list_jobs(status, limit, offset, date_filter, search_term)

    def count_jobs(
        self,
        status: Optional[str] = None,
        date_filter: Optional[str] = None,
        search_term: Optional[str] = None,
    ) -> int:
        """Count jobs matching filters"""
        return self.main_db.count_jobs(status, date_filter, search_term)

    def find_recent_job(self, query: str) -> Optional[ResearchJob]:
        """Find the most recent job for a similar query"""
        jobs = self.list_jobs(status="active")

        # Simple similarity check (can be improved)
        query_lower = query.lower()
        for job_data in jobs:
            if query_lower in job_data["query"].lower():
                return self.get_job(job_data["job_id"])

        return None

    async def create_symlink_to_latest(self, job: ResearchJob):
        """Create a symlink to the latest research bundle"""
        job_path = Path(job.output_dir)
        bundle_path = job_path / "RESEARCH_BUNDLE.md"

        if safe_exists(bundle_path):
            # Create symlink in base directory
            latest_link = self.base_dir / "latest_research.md"

            # Remove old symlink if exists
            if safe_exists(latest_link) or latest_link.is_symlink():
                latest_link.unlink()

            # Create relative symlink
            try:
                relative_path = Path("..") / bundle_path.relative_to(
                    self.base_dir.parent
                )
                latest_link.symlink_to(relative_path)
                logger.info(f"Created symlink: {latest_link} -> {relative_path}")
            except Exception as e:
                logger.warning(f"Could not create symlink: {e}")
                # Fallback: create absolute symlink
                try:
                    latest_link.symlink_to(bundle_path.absolute())
                except Exception as e2:
                    logger.error(f"Failed to create symlink: {e2}")

    def _sanitize_query(self, query: str) -> str:
        """Sanitize query for directory name"""
        safe_name = "".join(c if c.isalnum() or c in "- " else "_" for c in query)
        return safe_name.replace(" ", "-").lower()

    def get_job_info(self, job: ResearchJob) -> Dict[str, Any]:
        """Get comprehensive job information"""
        job_db = self.get_job_database(job)
        stats = job_db.get_stats()

        bundle_exists = safe_exists(Path(job.output_dir) / "RESEARCH_BUNDLE.md")

        return {
            "job_id": job.job_id,
            "query": job.query,
            "status": job.status,
            "created_at": job.created_at.isoformat(),
            "updated_at": job.updated_at.isoformat(),
            "output_dir": job.output_dir,
            "stats": stats,
            "bundle_exists": bundle_exists,
        }

    def cleanup_old_jobs(self, days: int = 30):
        """Clean up jobs older than specified days"""
        # TODO: Implement cleanup logic
        pass

    async def cleanup_job_raw_data(self, job_id: str) -> Dict[str, Any]:
        """
        Clean up raw data for a specific job while preserving aggregated data

        Returns:
            Dict with cleanup statistics
        """
        job = self.get_job(job_id)
        if not job:
            return {"error": f"Job {job_id} not found"}

        job_db = self.get_job_database(job)
        cleanup_stats = job_db.cleanup_raw_content()

        # Also clean up any HTML files in the job directory
        job_dir = Path(job.output_dir)
        html_files_deleted = 0
        space_freed = 0

        if safe_exists(job_dir):
            # Look for HTML files (if any were saved)
            for html_file in job_dir.glob("**/*.html"):
                try:
                    file_size = html_file.stat().st_size
                    html_file.unlink()
                    html_files_deleted += 1
                    space_freed += file_size
                except Exception as e:
                    logger.error(f"Error deleting {html_file}: {e}")

        cleanup_stats["html_files_deleted"] = html_files_deleted
        cleanup_stats["space_freed_mb"] = round(space_freed / (1024 * 1024), 2)

        logger.info(f"Cleaned up job {job_id}: {cleanup_stats}")
        return cleanup_stats

    async def cleanup_all_raw_data(self) -> Dict[str, Any]:
        """Clean up raw data for all jobs"""
        all_jobs = self.list_jobs()
        total_stats = {
            "jobs_cleaned": 0,
            "files_deleted": 0,
            "space_freed_mb": 0,
            "errors": [],
        }

        for job_info in all_jobs:
            try:
                stats = await self.cleanup_job_raw_data(job_info["job_id"])
                if "error" not in stats:
                    total_stats["jobs_cleaned"] += 1
                    total_stats["files_deleted"] += stats.get("html_files_deleted", 0)
                    total_stats["space_freed_mb"] += stats.get("space_freed_mb", 0)
                else:
                    total_stats["errors"].append(stats["error"])
            except Exception as e:
                total_stats["errors"].append(
                    f"Error cleaning job {job_info['job_id']}: {e}"
                )

        return total_stats

    def _serialize_config(self, config: ResearchConfig) -> Dict[str, Any]:
        """Convert ResearchConfig to serializable dict"""

        def serialize_value(val):
            if hasattr(val, "__dict__"):
                return {k: serialize_value(v) for k, v in val.__dict__.items()}
            elif isinstance(val, Path):
                return str(val)
            elif isinstance(val, (list, tuple)):
                return [serialize_value(v) for v in val]
            elif isinstance(val, dict):
                return {k: serialize_value(v) for k, v in val.items()}
            else:
                return val

        return serialize_value(config)

======= llm_interface.py ======
# Copyright 2025 Franz und Franz GmbH
#
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#     http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.

"""
LLM Provider interface and implementations for m1f-research
"""
from abc import ABC, abstractmethod
from typing import Dict, Any, List, Optional, Union
import os
import json
import subprocess
import aiohttp
import asyncio
from dataclasses import dataclass
from .prompt_utils import get_web_search_prompt
import anyio

# Claude SDK removed - using direct subprocess instead

# Import shared Claude utilities
from ..shared.claude_utils import (
    ClaudeConfig,
    ClaudeHTTPClient,
    ClaudeSessionManager,
    ClaudeErrorHandler,
)


@dataclass
class LLMResponse:
    """Standard response format from LLM providers"""

    content: str
    raw_response: Optional[Dict[str, Any]] = None
    usage: Optional[Dict[str, int]] = None
    error: Optional[str] = None


class LLMProvider(ABC):
    """Base class for LLM providers"""

    def __init__(self, api_key: Optional[str] = None, model: Optional[str] = None):
        self.api_key = api_key
        self.model = model or self.default_model

    @property
    @abstractmethod
    def default_model(self) -> str:
        """Default model for this provider"""
        pass

    @abstractmethod
    async def query(
        self, prompt: str, system: Optional[str] = None, **kwargs
    ) -> LLMResponse:
        """
        Query the LLM with a prompt

        Args:
            prompt: User prompt
            system: System prompt (optional)
            **kwargs: Provider-specific options

        Returns:
            LLMResponse object
        """
        pass

    @abstractmethod
    async def search_web(
        self, query: str, num_results: int = 20
    ) -> List[Dict[str, str]]:
        """
        Use LLM to search the web for URLs

        Args:
            query: Search query
            num_results: Number of results to return

        Returns:
            List of dicts with 'url', 'title', 'description'
        """
        pass

    @abstractmethod
    async def analyze_content(
        self, content: str, analysis_type: str = "relevance"
    ) -> Dict[str, Any]:
        """
        Analyze content using the LLM

        Args:
            content: Content to analyze
            analysis_type: Type of analysis (relevance, summary, key_points, etc.)

        Returns:
            Analysis results as dict
        """
        pass


class ClaudeProvider(LLMProvider):
    """Claude API provider via Anthropic"""

    def __init__(self, api_key: Optional[str] = None, model: Optional[str] = None):
        api_key = api_key or os.getenv("ANTHROPIC_API_KEY")
        super().__init__(api_key, model)

        # Use shared configuration and HTTP client
        self.config = ClaudeConfig(api_key=api_key, model=self.model)
        self.client = ClaudeHTTPClient(self.config)
        self.error_handler = ClaudeErrorHandler()

    @property
    def default_model(self) -> str:
        return "claude-3-opus-20240229"

    async def query(
        self, prompt: str, system: Optional[str] = None, **kwargs
    ) -> LLMResponse:
        """Query Claude API using shared HTTP client"""
        try:
            response = await self.client.send_request(
                prompt=prompt, system=system, **kwargs
            )

            return LLMResponse(
                content=response["content"][0]["text"],
                raw_response=response,
                usage=response.get("usage"),
            )
        except Exception as e:
            self.error_handler.handle_api_error(e, operation="Claude API query")
            return LLMResponse(content="", error=str(e))

    async def search_web(
        self, query: str, num_results: int = 20
    ) -> List[Dict[str, str]]:
        """Use Claude to generate search URLs"""
        prompt = f"""As a research assistant, help me find {num_results} relevant web resources about: "{query}"

Please suggest real, existing websites and resources that would be helpful for researching this topic. Return your suggestions as a JSON array where each entry has:
- url: A real website URL that likely contains information on this topic
- title: The expected page/site title
- description: What kind of information this resource likely contains

Focus on well-known, authoritative sources in this domain such as:
- Official documentation and guides
- Industry-leading blogs and publications
- Educational resources and tutorials
- Professional forums and communities

Example format:
[
  {{"url": "https://example.com/article", "title": "Article Title", "description": "Brief description"}}
]

Return ONLY the JSON array, no other text."""

        response = await self.query(prompt)

        if response.error:
            raise Exception(f"LLM error: {response.error}")

        try:
            # Extract JSON from response
            content = response.content.strip()

            # Handle various formats Claude might return
            if "```json" in content:
                start = content.find("```json") + 7
                end = content.find("```", start)
                if end != -1:
                    content = content[start:end].strip()
            elif "```" in content:
                start = content.find("```") + 3
                end = content.find("```", start)
                if end != -1:
                    content = content[start:end].strip()

            # Try to find JSON array brackets if not already present
            if not content.startswith("["):
                start_idx = content.find("[")
                if start_idx != -1:
                    end_idx = content.rfind("]") + 1
                    if end_idx > start_idx:
                        content = content[start_idx:end_idx]

            results = json.loads(content)

            # Validate and ensure required fields
            if not isinstance(results, list):
                raise ValueError("Response is not a JSON array")

            valid_results = []
            for result in results:
                if isinstance(result, dict) and "url" in result:
                    if "title" not in result:
                        result["title"] = "Untitled"
                    if "description" not in result:
                        result["description"] = ""
                    valid_results.append(result)

            return valid_results[:num_results]

        except (json.JSONDecodeError, ValueError) as e:
            raise Exception(f"Failed to parse LLM response as JSON: {str(e)}")

    async def analyze_content(
        self, content: str, analysis_type: str = "relevance"
    ) -> Dict[str, Any]:
        """Analyze content with Claude"""
        prompts = {
            "relevance": """Analyze this content and rate its relevance (0-10) for the research topic.
Return JSON with: relevance_score, reason, key_topics""",
            "summary": """Provide a concise summary of this content.
Return JSON with: summary, main_points (array), content_type""",
            "key_points": """Extract the key points from this content.
Return JSON with: key_points (array), technical_level, recommended_reading_order""",
        }

        prompt = prompts.get(analysis_type, prompts["relevance"])
        prompt = f"{prompt}\n\nContent:\n{content[:4000]}..."  # Limit content length

        response = await self.query(prompt)

        if response.error:
            raise Exception(f"Analysis error: {response.error}")

        try:
            content = response.content.strip()
            if content.startswith("```json"):
                content = content[7:]
            if content.endswith("```"):
                content = content[:-3]

            return json.loads(content)

        except json.JSONDecodeError:
            # Return basic analysis if JSON parsing fails
            return {
                "relevance_score": 5,
                "reason": "Could not parse analysis",
                "raw_response": response.content,
            }


class GeminiProvider(LLMProvider):
    """Google Gemini API provider"""

    def __init__(self, api_key: Optional[str] = None, model: Optional[str] = None):
        api_key = api_key or os.getenv("GOOGLE_API_KEY")
        super().__init__(api_key, model)
        self.base_url = "https://generativelanguage.googleapis.com/v1beta"

    @property
    def default_model(self) -> str:
        return "gemini-pro"

    def _validate_api_key(self):
        """Validate that API key is present"""
        if not self.api_key:
            raise ValueError("GOOGLE_API_KEY is required for Gemini provider")

    async def query(
        self, prompt: str, system: Optional[str] = None, **kwargs
    ) -> LLMResponse:
        """Query Gemini API"""
        self._validate_api_key()

        # Combine system and user prompts for Gemini
        full_prompt = prompt
        if system:
            full_prompt = f"{system}\n\n{prompt}"

        data = {
            "contents": [{"parts": [{"text": full_prompt}]}],
            "generationConfig": {
                "temperature": kwargs.get("temperature", 0.7),
                "topK": kwargs.get("top_k", 40),
                "topP": kwargs.get("top_p", 0.95),
                "maxOutputTokens": kwargs.get("max_tokens", 2048),
            },
        }

        try:
            async with aiohttp.ClientSession() as session:
                async with session.post(
                    f"{self.base_url}/models/{self.model}:generateContent?key={self.api_key}",
                    json=data,
                ) as response:
                    result = await response.json()

                    if response.status != 200:
                        return LLMResponse(
                            content="",
                            error=f"API error: {result.get('error', {}).get('message', 'Unknown error')}",
                        )

                    content = result["candidates"][0]["content"]["parts"][0]["text"]

                    return LLMResponse(
                        content=content,
                        raw_response=result,
                        usage=result.get("usageMetadata"),
                    )

        except Exception as e:
            return LLMResponse(content="", error=str(e))

    async def search_web(
        self, query: str, num_results: int = 20
    ) -> List[Dict[str, str]]:
        """Use Gemini to generate search URLs"""
        prompt = f"""As a research assistant, help me find {num_results} relevant web resources about: "{query}"

Please suggest real, existing websites and resources that would be helpful for researching this topic. Return your suggestions as a JSON array where each entry has:
- url: A real website URL that likely contains information on this topic
- title: The expected page/site title
- description: What kind of information this resource likely contains

Focus on well-known, authoritative sources.

Example format:
[
  {{"url": "https://example.com/article", "title": "Article Title", "description": "Brief description"}}
]

Return ONLY the JSON array."""

        response = await self.query(prompt)

        if response.error:
            raise Exception(f"LLM error: {response.error}")

        try:
            content = response.content.strip()

            # Handle various formats Gemini might return
            if "```json" in content:
                start = content.find("```json") + 7
                end = content.find("```", start)
                if end != -1:
                    content = content[start:end].strip()
            elif "```" in content:
                start = content.find("```") + 3
                end = content.find("```", start)
                if end != -1:
                    content = content[start:end].strip()

            # Try to find JSON array brackets if not already present
            if not content.startswith("["):
                start_idx = content.find("[")
                if start_idx != -1:
                    end_idx = content.rfind("]") + 1
                    if end_idx > start_idx:
                        content = content[start_idx:end_idx]

            results = json.loads(content)

            # Validate and ensure required fields
            if not isinstance(results, list):
                raise ValueError("Response is not a JSON array")

            valid_results = []
            for result in results:
                if isinstance(result, dict) and "url" in result:
                    if "title" not in result:
                        result["title"] = "Untitled"
                    if "description" not in result:
                        result["description"] = ""
                    valid_results.append(result)

            return valid_results[:num_results]

        except (json.JSONDecodeError, ValueError) as e:
            raise Exception(f"Failed to parse Gemini response as JSON: {str(e)}")

    async def analyze_content(
        self, content: str, analysis_type: str = "relevance"
    ) -> Dict[str, Any]:
        """Analyze content with Gemini"""
        # Similar implementation to Claude
        prompts = {
            "relevance": """Analyze this content and rate its relevance (0-10).
Return JSON with: relevance_score, reason, key_topics""",
            "summary": """Summarize this content.
Return JSON with: summary, main_points (array), content_type""",
            "key_points": """Extract key points.
Return JSON with: key_points (array), technical_level""",
        }

        prompt = prompts.get(analysis_type, prompts["relevance"])
        prompt = f"{prompt}\n\nContent:\n{content[:4000]}..."

        response = await self.query(prompt)

        if response.error:
            raise Exception(f"Analysis error: {response.error}")

        try:
            content = response.content.strip()
            if content.startswith("```json"):
                content = content[7:]
            if content.endswith("```"):
                content = content[:-3]

            return json.loads(content)

        except json.JSONDecodeError:
            return {
                "relevance_score": 5,
                "reason": "Could not parse analysis",
                "raw_response": response.content,
            }


class ClaudeCodeProvider(LLMProvider):
    """Claude Code provider using subprocess for direct CLI control"""

    def __init__(self, api_key: Optional[str] = None, model: Optional[str] = None):
        super().__init__(api_key="claude-code", model=model)
        self.error_handler = ClaudeErrorHandler()
        self.binary_path = self._find_claude_binary()

    def _find_claude_binary(self) -> str:
        """Find Claude binary in system"""
        # Try default command first
        try:
            result = subprocess.run(
                ["claude", "--version"], capture_output=True, text=True, timeout=5
            )
            if result.returncode == 0:
                return "claude"
        except (subprocess.CalledProcessError, FileNotFoundError):
            pass

        # Try known paths
        from ..shared.claude_utils import ClaudeBinaryFinder

        return ClaudeBinaryFinder.find()

    @property
    def default_model(self) -> str:
        return None  # Let Claude CLI use its default model

    async def query(
        self, prompt: str, system: Optional[str] = None, **kwargs
    ) -> LLMResponse:
        """Query Claude using direct subprocess call with streaming"""
        # Combine system and user prompts
        full_prompt = prompt
        if system:
            full_prompt = f"{system}\n\n{prompt}"

        try:
            # Build command with WebSearch tool enabled for URL discovery
            # Use stream-json format for real-time feedback
            cmd = [
                self.binary_path,
                "-p",
                "--allowedTools",
                "WebSearch",
                "--output-format",
                "stream-json",
                "--verbose",
            ]

            # Add model if specified and not None
            if self.model and self.model not in [None, "default"]:
                cmd.extend(["--model", self.model])

            # Show progress if requested
            show_progress = kwargs.get("show_progress", True)

            # Run subprocess in executor to avoid blocking
            loop = asyncio.get_event_loop()

            def run_claude():
                import time
                import sys
                import json

                # Try to import colors for better output
                try:
                    from ..shared.colors import info, dim
                except ImportError:
                    # Fallback if colors not available
                    def info(msg):
                        print(f"  {msg}", flush=True)

                    def dim(msg):
                        return msg

                # Use Popen for streaming like m1f_claude_runner
                process = subprocess.Popen(
                    cmd,
                    stdin=subprocess.PIPE,
                    stdout=subprocess.PIPE,
                    stderr=subprocess.PIPE,
                    text=True,
                    bufsize=1,
                    universal_newlines=True,
                )

                # Send prompt via stdin and close it
                process.stdin.write(full_prompt)
                process.stdin.close()

                # Collect output
                stdout_lines = []
                result_content = []
                start_time = time.time()
                last_progress_time = 0
                spinner_chars = ["⠋", "⠙", "⠹", "⠸", "⠼", "⠴", "⠦", "⠧", "⠇", "⠏"]
                spinner_idx = 0

                # Show initial message
                if show_progress:
                    info("  🤖 Claude is processing your request...")

                # Read stdout line by line for real-time feedback
                while True:
                    line = process.stdout.readline()
                    if not line and process.poll() is not None:
                        break

                    if line:
                        line = line.rstrip()  # Keep internal spacing
                        stdout_lines.append(line)

                        # Parse JSON stream for progress updates
                        if show_progress:
                            current_time = time.time()
                            elapsed = current_time - start_time

                            try:
                                # Try to parse as JSON for stream-json format
                                json_obj = json.loads(line)
                                msg_type = json_obj.get("type", "")

                                # Show progress based on message type
                                if msg_type == "assistant":
                                    # Check for tool_use and text in assistant message
                                    message = json_obj.get("message", {})
                                    if isinstance(message, dict):
                                        content_parts = message.get("content", [])
                                        for part in content_parts:
                                            if isinstance(part, dict):
                                                if (
                                                    part.get("type") == "tool_use"
                                                    and part.get("name") == "WebSearch"
                                                ):
                                                    query_info = part.get(
                                                        "input", {}
                                                    ).get("query", "")
                                                    sys.stdout.write(
                                                        "\r" + " " * 80 + "\r"
                                                    )
                                                    info(
                                                        f'  🔍 WebSearch: "{query_info}"'
                                                    )
                                                elif part.get("type") == "text":
                                                    text = part.get("text", "")
                                                    # Check if this is the final JSON output
                                                    if "```json" in text:
                                                        sys.stdout.write(
                                                            "\r" + " " * 80 + "\r"
                                                        )
                                                        # Try to count URLs in the JSON
                                                        try:
                                                            import re

                                                            urls_found = len(
                                                                re.findall(
                                                                    r'"url":', text
                                                                )
                                                            )
                                                            if urls_found > 0:
                                                                info(
                                                                    f"  📝 Formatting {urls_found} URLs as JSON..."
                                                                )
                                                        except:
                                                            info(
                                                                f"  📝 Formatting results as JSON..."
                                                            )
                                elif msg_type == "user":
                                    # Check for tool_result in user message (WebSearch response)
                                    message = json_obj.get("message", {})
                                    if isinstance(message, dict):
                                        content_parts = message.get("content", [])
                                        for part in content_parts:
                                            if (
                                                isinstance(part, dict)
                                                and part.get("type") == "tool_result"
                                            ):
                                                content = part.get("content", "")
                                                if "Web search results" in content:
                                                    sys.stdout.write(
                                                        "\r" + " " * 80 + "\r"
                                                    )
                                                    # Try to count links in the response
                                                    try:
                                                        import re

                                                        links_match = re.search(
                                                            r"Links: \[(.*?)\]", content
                                                        )
                                                        if links_match:
                                                            links_str = (
                                                                links_match.group(1)
                                                            )
                                                            links_count = (
                                                                links_str.count(
                                                                    '"url":'
                                                                )
                                                            )
                                                            info(
                                                                f"  ✅ WebSearch returned {links_count} results"
                                                            )
                                                        else:
                                                            info(
                                                                f"  ✅ WebSearch results received"
                                                            )
                                                    except:
                                                        info(
                                                            f"  ✅ WebSearch results received"
                                                        )
                            except json.JSONDecodeError:
                                # Not JSON, show spinner for regular output
                                if current_time - last_progress_time > 0.3:
                                    sys.stdout.write("\r")
                                    sys.stdout.write(
                                        f"  {spinner_chars[spinner_idx]} Processing... [{elapsed:.1f}s]"
                                    )
                                    sys.stdout.flush()
                                    spinner_idx = (spinner_idx + 1) % len(spinner_chars)
                                    last_progress_time = current_time

                # Clear the spinner line
                if show_progress:
                    sys.stdout.write("\r")
                    sys.stdout.write(" " * 50)  # Clear the line
                    sys.stdout.write("\r")
                    sys.stdout.flush()
                    elapsed = time.time() - start_time
                    info(f"  ✅ Claude completed in {elapsed:.1f}s")

                # Get any stderr
                stderr = process.stderr.read()

                # Wait for process to finish
                process.wait(timeout=5)

                # Extract actual content from JSON stream
                final_content = ""
                for line in stdout_lines:
                    try:
                        json_obj = json.loads(line)
                        # Look for assistant messages with text content
                        if json_obj.get("type") == "assistant":
                            message = json_obj.get("message", {})
                            if isinstance(message, dict):
                                content_parts = message.get("content", [])
                                for part in content_parts:
                                    if (
                                        isinstance(part, dict)
                                        and part.get("type") == "text"
                                    ):
                                        final_content += part.get("text", "")
                    except json.JSONDecodeError:
                        # If not JSON, include as-is (shouldn't happen with stream-json)
                        pass

                # If no JSON content found, fall back to raw output
                if not final_content:
                    final_content = "\n".join(stdout_lines)

                return process.returncode, final_content, stderr

            returncode, stdout, stderr = await loop.run_in_executor(None, run_claude)

            if returncode != 0:
                error_msg = stderr.strip() if stderr else ""
                if not error_msg:
                    error_msg = f"Process exited with code {returncode}"
                return LLMResponse(content="", error=f"Claude error: {error_msg}")

            return LLMResponse(
                content=stdout.strip(),
                raw_response={"command": cmd, "returncode": returncode},
            )

        except subprocess.TimeoutExpired:
            return LLMResponse(content="", error="Claude request timed out")
        except Exception as e:
            self.error_handler.handle_api_error(e, operation="Claude CLI query")
            return LLMResponse(content="", error=str(e))

    async def search_web(
        self, query: str, num_results: int = 20
    ) -> List[Dict[str, str]]:
        """Use Claude direct to generate search URLs"""
        prompt = f"""List {num_results} URLs of authoritative websites about: {query}

Return as JSON array with url, title, description. No comments, no explanation, just the JSON:

[
  {{"url": "https://example.com", "title": "Site Title", "description": "Brief description"}}
]"""

        response = await self.query(prompt)

        if response.error:
            raise Exception(f"Claude error: {response.error}")

        try:
            # Extract JSON from response
            content = response.content.strip()

            # Debug: Log the raw response content
            import logging

            logger = logging.getLogger(__name__)
            logger.debug(f"Raw Claude response: {content[:500]}...")

            # Try to find JSON array in the content
            # Handle various formats Claude might return
            if "```json" in content:
                # Extract content between ```json and ```
                start = content.find("```json") + 7
                end = content.find("```", start)
                if end != -1:
                    content = content[start:end].strip()
            elif "```" in content:
                # Extract content between ``` and ```
                start = content.find("```") + 3
                end = content.find("```", start)
                if end != -1:
                    content = content[start:end].strip()

            # Try to find JSON array brackets if not already present
            if not content.startswith("["):
                # Look for JSON array in the content
                start_idx = content.find("[")
                if start_idx != -1:
                    end_idx = content.rfind("]") + 1
                    if end_idx > start_idx:
                        content = content[start_idx:end_idx]

            results = json.loads(content)

            # Validate the structure
            if not isinstance(results, list):
                raise ValueError("Response is not a JSON array")

            # Ensure each result has required fields
            valid_results = []
            for result in results:
                if isinstance(result, dict) and "url" in result:
                    # Add default values for missing fields
                    if "title" not in result:
                        result["title"] = "Untitled"
                    if "description" not in result:
                        result["description"] = ""
                    valid_results.append(result)

            return valid_results[:num_results]

        except (json.JSONDecodeError, ValueError) as e:
            # Log the full response for debugging
            import logging

            logger = logging.getLogger(__name__)
            logger.error(f"Failed to parse JSON. Response content: {response.content}")
            raise Exception(f"Failed to parse Claude response as JSON: {str(e)}")

    async def analyze_content(
        self, content: str, analysis_type: str = "relevance"
    ) -> Dict[str, Any]:
        """Analyze content with Claude Code"""

        prompts = {
            "relevance": """Analyze this content and rate its relevance (0-10) for the research topic.
Return JSON with: relevance_score, reason, key_topics""",
            "summary": """Provide a concise summary of this content.
Return JSON with: summary, main_points (array), content_type""",
            "key_points": """Extract the key points from this content.
Return JSON with: key_points (array), technical_level, recommended_reading_order""",
        }

        prompt = prompts.get(analysis_type, prompts["relevance"])
        prompt = f"{prompt}\n\nContent:\n{content[:4000]}..."  # Limit content length

        response = await self.query(prompt)

        if response.error:
            raise Exception(f"Analysis error: {response.error}")

        try:
            content = response.content.strip()
            if content.startswith("```json"):
                content = content[7:]
            if content.endswith("```"):
                content = content[:-3]

            return json.loads(content)

        except json.JSONDecodeError:
            # Return basic analysis if JSON parsing fails
            return {
                "relevance_score": 5,
                "reason": "Could not parse analysis",
                "raw_response": response.content,
            }


class CLIProvider(LLMProvider):
    """Provider for CLI-based LLM tools like gemini-cli"""

    def __init__(self, command: str = "gemini", model: Optional[str] = None):
        super().__init__(api_key="cli", model=model)
        self.command = command

    @property
    def default_model(self) -> str:
        return "default"

    async def query(
        self, prompt: str, system: Optional[str] = None, **kwargs
    ) -> LLMResponse:
        """Query via CLI command"""
        # Combine system and user prompts
        full_prompt = prompt
        if system:
            full_prompt = f"{system}\n\n{prompt}"

        try:
            # Only handle non-Claude CLI tools
            if self.command == "claude":
                # Redirect to ClaudeCodeProvider instead
                provider = ClaudeCodeProvider(model=self.model)
                return await provider.query(prompt, system, **kwargs)

            # Other CLI tools (like gemini-cli) use stdin
            cmd = [self.command]

            # Add model if specified
            if self.model != "default":
                cmd.extend(["--model", self.model])

            # Add any additional CLI args
            if "cli_args" in kwargs:
                cmd.extend(kwargs["cli_args"])

            # Run command asynchronously
            proc = await asyncio.create_subprocess_exec(
                *cmd,
                stdin=asyncio.subprocess.PIPE,
                stdout=asyncio.subprocess.PIPE,
                stderr=asyncio.subprocess.PIPE,
            )

            stdout, stderr = await proc.communicate(full_prompt.encode())

            if proc.returncode != 0:
                return LLMResponse(content="", error=f"CLI error: {stderr.decode()}")

            return LLMResponse(
                content=stdout.decode().strip(),
                raw_response={"command": cmd, "returncode": proc.returncode},
            )

        except Exception as e:
            return LLMResponse(content="", error=str(e))

    async def search_web(
        self, query: str, num_results: int = 20
    ) -> List[Dict[str, str]]:
        """Use CLI tool to generate search URLs"""
        # Redirect claude commands to ClaudeCodeProvider
        if self.command == "claude":
            provider = ClaudeCodeProvider(model=self.model)
            return await provider.search_web(query, num_results)

        prompt = f"""As a research assistant, help me find {num_results} relevant web resources about: "{query}"

Please suggest real, existing websites and resources that would be helpful for researching this topic. Return your suggestions as a JSON array where each entry has:
- url: A real website URL that likely contains information on this topic
- title: The expected page/site title
- description: What kind of information this resource likely contains

Example format:
[
  {{"url": "https://example.com/article", "title": "Article Title", "description": "Brief description"}}
]

Return ONLY the JSON array."""

        response = await self.query(prompt)

        if response.error:
            raise Exception(f"CLI error: {response.error}")

        try:
            content = response.content.strip()

            # Handle various formats CLI might return
            if "```json" in content:
                start = content.find("```json") + 7
                end = content.find("```", start)
                if end != -1:
                    content = content[start:end].strip()
            elif "```" in content:
                start = content.find("```") + 3
                end = content.find("```", start)
                if end != -1:
                    content = content[start:end].strip()

            # Try to find JSON array brackets if not already present
            if not content.startswith("["):
                start_idx = content.find("[")
                if start_idx != -1:
                    end_idx = content.rfind("]") + 1
                    if end_idx > start_idx:
                        content = content[start_idx:end_idx]

            results = json.loads(content)

            # Validate and ensure required fields
            if not isinstance(results, list):
                raise ValueError("Response is not a JSON array")

            valid_results = []
            for result in results:
                if isinstance(result, dict) and "url" in result:
                    if "title" not in result:
                        result["title"] = "Untitled"
                    if "description" not in result:
                        result["description"] = ""
                    valid_results.append(result)

            return valid_results[:num_results]

        except (json.JSONDecodeError, ValueError) as e:
            raise Exception(f"Failed to parse CLI response as JSON: {str(e)}")

    async def analyze_content(
        self, content: str, analysis_type: str = "relevance"
    ) -> Dict[str, Any]:
        """Analyze content via CLI"""
        # Redirect claude commands to ClaudeCodeProvider
        if self.command == "claude":
            provider = ClaudeCodeProvider(model=self.model)
            return await provider.analyze_content(content, analysis_type)

        prompts = {
            "relevance": "Rate relevance 0-10. Return JSON: relevance_score, reason",
            "summary": "Summarize. Return JSON: summary, main_points",
            "key_points": "Extract key points. Return JSON: key_points",
        }

        prompt = prompts.get(analysis_type, prompts["relevance"])
        prompt = f"{prompt}\n\nContent:\n{content[:2000]}..."

        response = await self.query(prompt)

        if response.error:
            raise Exception(f"Analysis error: {response.error}")

        try:
            content = response.content.strip()
            if content.startswith("```json"):
                content = content[7:]
            if content.endswith("```"):
                content = content[:-3]

            return json.loads(content)

        except json.JSONDecodeError:
            return {
                "relevance_score": 5,
                "reason": "Could not parse analysis",
                "raw_response": response.content,
            }


def get_provider(provider_name: str, **kwargs) -> LLMProvider:
    """Factory function to get LLM provider instance"""
    providers = {
        "claude": ClaudeProvider,  # Anthropic API
        "claude-code": ClaudeCodeProvider,  # Direct Claude CLI
        "gemini": GeminiProvider,
        "gemini-cli": lambda **kw: CLIProvider(command="gemini", model=kw.get("model")),
    }

    provider_class = providers.get(provider_name.lower())
    if not provider_class:
        raise ValueError(f"Unknown provider: {provider_name}")

    return provider_class(**kwargs)

======= models.py ======
# Copyright 2025 Franz und Franz GmbH
#
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#     http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.

"""
Data models for m1f-research
"""
from dataclasses import dataclass, field
from datetime import datetime
from typing import List, Dict, Any, Optional
from pathlib import Path


@dataclass
class ResearchResult:
    """Complete research result"""

    query: str
    job_id: str
    urls_found: int
    scraped_content: List["ScrapedContent"]
    analyzed_content: List["AnalyzedContent"]
    bundle_path: Optional["Path"] = None
    bundle_created: bool = False
    output_dir: Optional["Path"] = None
    generated_at: datetime = field(default_factory=datetime.now)
    config: Dict[str, Any] = field(default_factory=dict)


@dataclass
class ScrapedContent:
    """Scraped web content"""

    url: str
    title: str
    content: str  # HTML or markdown content
    content_type: str = ""
    scraped_at: datetime = field(default_factory=datetime.now)
    error: Optional[str] = None
    metadata: Dict[str, Any] = field(default_factory=dict)


@dataclass
class AnalyzedContent:
    """Analyzed content with relevance and insights"""

    url: str
    title: str
    content: str  # markdown content
    relevance_score: float  # 0-10
    key_points: List[str]
    summary: str
    content_type: Optional[str] = None  # tutorial, reference, blog, etc.
    analysis_metadata: Dict[str, Any] = field(default_factory=dict)

    # Compatibility with old API
    @property
    def metadata(self) -> Dict[str, Any]:
        return self.analysis_metadata


@dataclass
class ResearchSource:
    """A source for research (web, github, arxiv, etc.)"""

    name: str
    type: str
    weight: float = 1.0
    config: Dict[str, Any] = field(default_factory=dict)

======= orchestrator.py ======
# Copyright 2025 Franz und Franz GmbH
#
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#     http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.

"""
Enhanced research orchestrator with job management and persistence
"""

import asyncio
import os
from pathlib import Path
from typing import List, Dict, Any, Optional
from datetime import datetime
import json
import logging

from ..m1f.file_operations import (
    safe_open,
)

from .config import ResearchConfig
from .llm_interface import get_provider, LLMProvider
from .models import ResearchResult, ScrapedContent, AnalyzedContent
from .job_manager import JobManager
from .research_db import ResearchJob, JobDatabase
from .url_manager import URLManager
from .smart_scraper import EnhancedSmartScraper
from .content_filter import ContentFilter
from .analyzer import ContentAnalyzer
from .bundle_creator import SmartBundleCreator
from .readme_generator import ReadmeGenerator

logger = logging.getLogger(__name__)

try:
    from ..scrape_tool.scrapers.base import WebScraperBase as WebScraper
except ImportError:
    logger.warning("Could not import WebScraperBase from scrape_tool")
    WebScraper = None

try:
    from ..html2md_tool import HTML2MDConverter as HTMLToMarkdownConverter
except ImportError:
    logger.warning("Could not import HTML2MDConverter from html2md_tool")
    HTMLToMarkdownConverter = None


class EnhancedResearchOrchestrator:
    """Enhanced orchestrator with job persistence and resume support"""

    def __init__(self, config: ResearchConfig):
        self.config = config
        self.llm = self._init_llm()
        self.job_manager = JobManager(config.output.directory)
        self.current_job: Optional[ResearchJob] = None
        self.job_db: Optional[JobDatabase] = None
        self.url_manager: Optional[URLManager] = None
        self.progress_callback = None

    def _init_llm(self) -> Optional[LLMProvider]:
        """Initialize LLM provider from config"""
        if self.config.dry_run:
            return None

        try:
            # Determine effective provider with sensible defaults
            provider_name = (self.config.llm.provider or "claude").lower()

            # If user selected Claude but no API key is present, use Claude Code
            if provider_name == "claude" and not os.getenv("ANTHROPIC_API_KEY"):
                provider_name = "claude-code"
                logger.info("No ANTHROPIC_API_KEY found, using claude-code provider")

            return get_provider(
                provider_name,
                api_key=None,  # Providers will read from environment when needed
                model=self.config.llm.model,
            )
        except Exception as e:
            logger.error(f"Failed to initialize LLM provider: {e}")
            if not self.config.no_analysis:
                raise
            return None

    def set_progress_callback(self, callback):
        """Set callback for progress updates"""
        self.progress_callback = callback

    async def research(
        self, query: str, job_id: Optional[str] = None, urls_file: Optional[Path] = None
    ) -> ResearchResult:
        """
        Run research workflow with job management

        Args:
            query: Research query
            job_id: Existing job ID to resume
            urls_file: Optional file with additional URLs

        Returns:
            ResearchResult with all findings
        """
        logger.info(f"Starting research for: {query}")

        try:
            # Initialize or resume job
            if job_id:
                self.current_job = self.job_manager.get_job(job_id)
                if not self.current_job:
                    raise ValueError(f"Job {job_id} not found")
                logger.info(f"Resuming job {job_id}")
            else:
                self.current_job = self.job_manager.create_job(query, self.config)
                logger.info(f"Created new job {self.current_job.job_id}")

            # Setup job database and URL manager
            self.job_db = self.job_manager.get_job_database(self.current_job)
            self.url_manager = URLManager(self.job_db)

            # Phase 1: URL Collection
            urls = await self._collect_urls(query, urls_file, resume=bool(job_id))

            if not urls:
                logger.warning("No URLs to scrape")
                self.job_manager.update_job_status(self.current_job.job_id, "completed")
                return self._create_empty_result()

            # Phase 2: Smart Scraping
            scraped_content = await self._scrape_urls(urls)

            # Phase 3: Content Filtering
            filtered_content = await self._filter_content(scraped_content)

            # Phase 4: Content Analysis (optional)
            if not self.config.no_analysis and self.llm:
                analyzed_content = await self._analyze_content(filtered_content)
            else:
                # Convert to AnalyzedContent with defaults
                analyzed_content = [
                    self._scraped_to_analyzed(s) for s in filtered_content
                ]

            # Phase 5: Bundle Creation
            bundle_path = await self._create_bundle(analyzed_content, query)

            # Update job status
            self.job_manager.update_job_stats(self.current_job)
            self.job_manager.update_job_status(self.current_job.job_id, "completed")

            # Create symlink to latest research
            await self.job_manager.create_symlink_to_latest(self.current_job)

            return ResearchResult(
                query=query,
                job_id=self.current_job.job_id,
                urls_found=len(urls),
                scraped_content=scraped_content,
                analyzed_content=analyzed_content,
                bundle_path=bundle_path,
                bundle_created=True,
                output_dir=Path(self.current_job.output_dir),
            )

        except Exception as e:
            logger.error(f"Research failed: {e}")
            if self.current_job:
                self.job_manager.update_job_status(self.current_job.job_id, "failed")
            raise

    async def _collect_urls(
        self, query: str, urls_file: Optional[Path], resume: bool
    ) -> List[str]:
        """Collect URLs from LLM and/or file"""
        all_urls = []

        # Add URLs from file if provided
        if urls_file:
            added = await self.url_manager.add_urls_from_file(urls_file)
            logger.info(f"Added {added} URLs from file")

        # Get URLs from LLM if not resuming
        if not resume and not self.config.dry_run:
            logger.info("Searching for URLs using LLM...")
            if self.progress_callback:
                self.progress_callback(
                    "searching", 0, self.config.scraping.search_limit
                )
            try:
                llm_urls = await self.llm.search_web(
                    query, self.config.scraping.search_limit
                )
                added = self.url_manager.add_urls_from_list(llm_urls, source="llm")
                logger.info(f"Added {added} URLs from LLM search")
                if self.progress_callback:
                    self.progress_callback(
                        "searching", added, self.config.scraping.search_limit
                    )
            except Exception as e:
                logger.error(f"Error searching for URLs: {e}")
                if not urls_file:  # If no manual URLs, this is fatal
                    # Provide helpful error message for common issues
                    if "Failed to parse" in str(e) and "JSON" in str(e):
                        error_msg = (
                            "LLM failed to generate URLs. This can happen when:\n"
                            "1. Using Claude without API key (falls back to Claude Code which may refuse URL generation)\n"
                            "2. Query contains sensitive topics\n\n"
                            "Solutions:\n"
                            "- Set ANTHROPIC_API_KEY environment variable to use Claude API\n"
                            "- Use --provider gemini with GOOGLE_API_KEY set\n"
                            "- Provide URLs manually with --urls-file\n"
                            "- Try rephrasing your query"
                        )
                        raise Exception(error_msg) from e
                    raise

        # Get unscraped URLs
        all_urls = self.url_manager.get_unscraped_urls()
        logger.info(f"Total URLs to scrape: {len(all_urls)}")

        # Update stats
        self.job_manager.update_job_stats(
            self.current_job, total_urls=self.job_db.get_stats()["total_urls"]
        )

        # Limit URLs if configured
        if (
            self.config.scraping.scrape_limit
            and len(all_urls) > self.config.scraping.scrape_limit
        ):
            all_urls = all_urls[: self.config.scraping.scrape_limit]
            logger.info(f"Limited to {len(all_urls)} URLs")

        return all_urls

    async def _scrape_urls(self, urls: List[str]) -> List[ScrapedContent]:
        """Scrape URLs with smart delay management"""
        if self.config.dry_run:
            logger.info("DRY RUN: Would scrape URLs")
            return []

        scraped_content = []

        async with EnhancedSmartScraper(
            self.config.scraping, self.job_db, self.url_manager
        ) as scraper:
            # Set progress callback
            def scraping_progress(completed, total, percentage):
                logger.info(
                    f"Scraping progress: {completed}/{total} ({percentage:.1f}%)"
                )
                if self.progress_callback:
                    self.progress_callback("scraping", completed, total)
                if completed % 5 == 0:  # Update stats every 5 URLs
                    self.job_manager.update_job_stats(
                        self.current_job,
                        scraped_urls=self.job_db.get_stats()["scraped_urls"],
                    )

            scraper.set_progress_callback(scraping_progress)

            # Scrape URLs
            raw_content = await scraper.scrape_urls(urls)

            # Convert HTML to Markdown
            for scraped in raw_content:
                try:
                    # Use html2md tool if available
                    if HTMLToMarkdownConverter:
                        converter = HTMLToMarkdownConverter()
                        markdown = converter.convert_html(scraped.content)
                    else:
                        # Fallback to basic conversion
                        markdown = self._basic_html_to_markdown(scraped.content)

                    # Save to database
                    self.job_db.save_content(
                        url=scraped.url,
                        title=scraped.title,
                        markdown=markdown,
                        metadata={
                            "scraped_at": scraped.scraped_at.isoformat(),
                            "content_type": scraped.content_type,
                        },
                    )

                    # Update scraped content
                    scraped.content = markdown
                    scraped_content.append(scraped)

                except Exception as e:
                    logger.error(f"Error converting {scraped.url}: {e}")

        # Final stats update
        stats = scraper.get_statistics()
        logger.info(
            f"Scraping complete: {stats['successful_urls']} successful, "
            f"{stats['failed_urls']} failed"
        )

        self.job_manager.update_job_stats(self.current_job)

        return scraped_content

    async def _filter_content(
        self, content: List[ScrapedContent]
    ) -> List[ScrapedContent]:
        """Filter content for quality"""
        if self.config.no_filter:
            logger.info("Content filtering disabled")
            return content

        filter = ContentFilter(self.config.filtering)
        filtered = []

        for item in content:
            passed, reason = filter.filter_content(item.content)

            # Update database
            self.job_db.save_content(
                url=item.url,
                title=item.title,
                markdown=item.content,
                metadata={"scraped_at": item.scraped_at.isoformat()},
                filtered=not passed,
                filter_reason=reason,
            )

            if passed:
                filtered.append(item)
            else:
                logger.debug(f"Filtered out {item.url}: {reason}")

        logger.info(f"Filtered {len(content)} to {len(filtered)} items")

        # Update stats
        self.job_manager.update_job_stats(
            self.current_job, filtered_urls=len(content) - len(filtered)
        )

        return filtered

    async def _analyze_content(
        self, content: List[ScrapedContent]
    ) -> List[AnalyzedContent]:
        """Analyze content with LLM"""
        if not content:
            return []

        if self.config.no_analysis:
            # Convert to AnalyzedContent with defaults
            return [self._scraped_to_analyzed(s) for s in content]

        analyzer = ContentAnalyzer(self.llm, self.config.analysis)

        # Call the proper analyze_content method with the research query
        try:
            analyzed = await analyzer.analyze_content(content, self.current_job.query)

            # Save analysis to database
            for result in analyzed:
                self.job_db.save_analysis(
                    url=result.url,
                    relevance_score=result.relevance_score,
                    key_points=result.key_points,
                    content_type=result.content_type,
                    analysis_data={
                        "summary": result.summary,
                        "metadata": result.analysis_metadata,
                    },
                )

            # Sort by relevance
            analyzed.sort(key=lambda x: x.relevance_score, reverse=True)

            # Update stats
            self.job_manager.update_job_stats(
                self.current_job,
                analyzed_urls=len(analyzed),
            )

            return analyzed

        except Exception as e:
            logger.error(f"Error analyzing content: {e}")
            # Fallback to basic conversion
            return [self._scraped_to_analyzed(s) for s in content]

    async def _create_bundle(self, content: List[AnalyzedContent], query: str) -> Path:
        """Create the final research bundle"""
        if self.config.dry_run:
            logger.info("DRY RUN: Would create bundle")
            return Path(self.current_job.output_dir)

        output_dir = Path(self.current_job.output_dir)

        # Create bundle
        bundle_creator = SmartBundleCreator(
            llm_provider=self.llm if not self.config.no_analysis else None,
            config=self.config.output,
            research_config=self.config,
        )

        bundle_path = await bundle_creator.create_bundle(
            content, query, output_dir, synthesis=None  # TODO: Add synthesis generation
        )

        # Create prominent bundle file
        await self._create_prominent_bundle(output_dir, content, query)

        logger.info(f"Bundle created at: {bundle_path}")
        return bundle_path

    async def _create_prominent_bundle(
        self, output_dir: Path, content: List[AnalyzedContent], query: str
    ):
        """Create the prominent RESEARCH_BUNDLE.md file"""
        bundle_path = output_dir / "RESEARCH_BUNDLE.md"

        # Create header
        bundle_content = f"""# Research Bundle: {query}

**Generated**: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}  
**Job ID**: {self.current_job.job_id}  
**Total Sources**: {len(content)}

---

## Executive Summary

This research bundle contains {len(content)} carefully selected sources about "{query}".

"""

        # Add table of contents
        bundle_content += "## Table of Contents\n\n"
        for i, item in enumerate(content, 1):
            title = item.title or f"Source {i}"
            bundle_content += f"{i}. [{title}](#{i}-{self._slugify(title)})\n"

        bundle_content += "\n---\n\n"

        # Add all content
        for i, item in enumerate(content, 1):
            title = item.title or f"Source {i}"
            bundle_content += f"## {i}. {title}\n\n"
            bundle_content += f"**Source**: {item.url}\n"

            if hasattr(item, "relevance_score"):
                bundle_content += f"**Relevance**: {item.relevance_score}/10\n"

            if hasattr(item, "key_points") and item.key_points:
                bundle_content += "\n### Key Points:\n"
                for point in item.key_points:
                    bundle_content += f"- {point}\n"

            bundle_content += f"\n### Content:\n\n{item.content}\n\n"
            bundle_content += "---\n\n"

        # Write bundle
        with safe_open(bundle_path, "w", encoding="utf-8") as f:
            if f:
                f.write(bundle_content)

        logger.info(f"Created prominent bundle: {bundle_path}")

        # Also create executive summary
        summary_path = output_dir / "EXECUTIVE_SUMMARY.md"
        summary_content = f"""# Executive Summary: {query}

**Job ID**: {self.current_job.job_id}  
**Date**: {datetime.now().strftime('%Y-%m-%d')}

## Overview

Research on "{query}" yielded {len(content)} high-quality sources.

## Top Sources

"""

        for i, item in enumerate(content[:5], 1):  # Top 5
            summary_content += f"{i}. **{item.title}**\n"
            if hasattr(item, "summary"):
                summary_content += f"   - {item.summary[:200]}...\n"
            summary_content += f"   - [Link]({item.url})\n\n"

        with safe_open(summary_path, "w", encoding="utf-8") as f:
            if f:
                f.write(summary_content)

    def _scraped_to_analyzed(self, scraped: ScrapedContent) -> AnalyzedContent:
        """Convert ScrapedContent to AnalyzedContent"""
        return AnalyzedContent(
            url=scraped.url,
            title=scraped.title,
            content=scraped.content,
            relevance_score=5.0,  # Default
            key_points=[],
            summary="",
            content_type="unknown",
            analysis_metadata={},
        )

    def _basic_html_to_markdown(self, html: str) -> str:
        """Basic HTML to Markdown conversion"""
        import re

        # Remove script and style tags
        html = re.sub(
            r"<script[^>]*>.*?</script>", "", html, flags=re.DOTALL | re.IGNORECASE
        )
        html = re.sub(
            r"<style[^>]*>.*?</style>", "", html, flags=re.DOTALL | re.IGNORECASE
        )

        # Basic conversions
        conversions = [
            (r"<h1[^>]*>(.*?)</h1>", r"# \1\n"),
            (r"<h2[^>]*>(.*?)</h2>", r"## \1\n"),
            (r"<h3[^>]*>(.*?)</h3>", r"### \1\n"),
            (r"<p[^>]*>(.*?)</p>", r"\1\n\n"),
            (r"<strong[^>]*>(.*?)</strong>", r"**\1**"),
            (r"<b[^>]*>(.*?)</b>", r"**\1**"),
            (r"<em[^>]*>(.*?)</em>", r"*\1*"),
            (r"<i[^>]*>(.*?)</i>", r"*\1*"),
            (r'<a[^>]+href="([^"]+)"[^>]*>(.*?)</a>', r"[\2](\1)"),
            (r"<br[^>]*>", "\n"),
            (r"<[^>]+>", ""),  # Remove remaining tags
        ]

        for pattern, replacement in conversions:
            html = re.sub(pattern, replacement, html, flags=re.IGNORECASE | re.DOTALL)

        # Clean up
        html = re.sub(r"\n{3,}", "\n\n", html)
        return html.strip()

    def _slugify(self, text: str) -> str:
        """Create URL-safe slug from text"""
        import re

        text = re.sub(r"[^\w\s-]", "", text.lower())
        text = re.sub(r"[-\s]+", "-", text)
        return text[:50]

    def _create_empty_result(self) -> ResearchResult:
        """Create empty result when no URLs found"""
        return ResearchResult(
            query=self.current_job.query if self.current_job else "",
            job_id=self.current_job.job_id if self.current_job else "",
            urls_found=0,
            scraped_content=[],
            analyzed_content=[],
            bundle_path=(
                Path(self.current_job.output_dir) if self.current_job else Path()
            ),
            bundle_created=False,
            output_dir=(
                Path(self.current_job.output_dir) if self.current_job else Path()
            ),
        )

    async def get_job_status(self, job_id: str) -> Dict[str, Any]:
        """Get status of a research job"""
        job = self.job_manager.get_job(job_id)
        if not job:
            return {"error": f"Job {job_id} not found"}

        return self.job_manager.get_job_info(job)

    async def list_jobs(self, status: Optional[str] = None) -> List[Dict[str, Any]]:
        """List all research jobs"""
        return self.job_manager.list_jobs(status)

======= output.py ======
# Copyright 2025 Franz und Franz GmbH
#
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#     http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.

"""
Enhanced output formatting for m1f-research CLI
"""

import sys
import json
from typing import Any, Dict, List, Optional
from datetime import datetime
import shutil
from pathlib import Path

# Use unified colorama module
from ..shared.colors import Colors, COLORAMA_AVAILABLE


class OutputFormatter:
    """Handles formatted output for m1f-research"""

    def __init__(self, format: str = "text", verbose: int = 0, quiet: bool = False):
        self.format = format
        self.verbose = verbose
        self.quiet = quiet

        # Disable colors if not TTY or if requested
        if not sys.stdout.isatty() or format == "json":
            Colors.disable()

        # Track if we're in JSON mode
        self._json_buffer = [] if format == "json" else None

    def print(self, message: str = "", level: str = "info", end: str = "\n", **kwargs):
        """Print a message with appropriate formatting"""
        if self.quiet and level != "error":
            return

        if self.format == "json":
            self._json_buffer.append(
                {
                    "type": "message",
                    "level": level,
                    "message": message,
                    "timestamp": datetime.now().isoformat(),
                    **kwargs,
                }
            )
        else:
            print(message, end=end)

    def success(self, message: str, **kwargs):
        """Print success message"""
        if self.format == "json":
            self._json_buffer.append({"type": "success", "message": message, **kwargs})
        else:
            self.print(f"{Colors.GREEN}✅ {message}{Colors.RESET}")

    def error(self, message: str, suggestion: Optional[str] = None, **kwargs):
        """Print error message with optional suggestion"""
        if self.format == "json":
            self._json_buffer.append(
                {
                    "type": "error",
                    "message": message,
                    "suggestion": suggestion,
                    **kwargs,
                }
            )
        else:
            self.print(f"{Colors.RED}❌ Error: {message}{Colors.RESET}", level="error")
            if suggestion:
                self.print(f"{Colors.YELLOW}💡 Suggestion: {suggestion}{Colors.RESET}")

    def warning(self, message: str, **kwargs):
        """Print warning message"""
        if self.format == "json":
            self._json_buffer.append({"type": "warning", "message": message, **kwargs})
        else:
            self.print(f"{Colors.YELLOW}⚠️  {message}{Colors.RESET}")

    def info(self, message: str, **kwargs):
        """Print info message"""
        if self.format == "json":
            self._json_buffer.append({"type": "info", "message": message, **kwargs})
        else:
            self.print(f"{Colors.CYAN}ℹ️  {message}{Colors.RESET}")

    def debug(self, message: str, **kwargs):
        """Print debug message (only if verbose)"""
        if self.verbose < 2:
            return

        if self.format == "json":
            self._json_buffer.append({"type": "debug", "message": message, **kwargs})
        else:
            self.print(f"{Colors.BRIGHT_BLACK}🔍 {message}{Colors.RESET}")

    def header(self, title: str, subtitle: Optional[str] = None):
        """Print a section header"""
        if self.format == "json":
            self._json_buffer.append(
                {"type": "header", "title": title, "subtitle": subtitle}
            )
        else:
            self.print()
            self.print(f"{Colors.BOLD}{Colors.BLUE}{title}{Colors.RESET}")
            if subtitle:
                self.print(f"{Colors.DIM}{subtitle}{Colors.RESET}")
            self.print()

    def progress(self, current: int, total: int, message: str = ""):
        """Show progress bar"""
        if self.quiet or self.format == "json":
            return

        # Calculate percentage
        percentage = (current / total * 100) if total > 0 else 0

        # Terminal width
        term_width = shutil.get_terminal_size().columns
        bar_width = min(40, term_width - 30)

        # Build progress bar
        filled = int(bar_width * current / total) if total > 0 else 0
        bar = "█" * filled + "░" * (bar_width - filled)

        # Build message
        msg = f"\r{Colors.CYAN}[{bar}] {percentage:>5.1f}% {message}{Colors.RESET}"

        # Print without newline
        sys.stdout.write(msg)
        sys.stdout.flush()

        # Add newline when complete
        if current >= total:
            self.print()

    def table(
        self,
        headers: List[str],
        rows: List[List[str]],
        highlight_search: Optional[str] = None,
    ):
        """Print a formatted table"""
        if self.format == "json":
            self._json_buffer.append(
                {"type": "table", "headers": headers, "rows": rows}
            )
            return

        # Calculate column widths
        widths = [len(h) for h in headers]
        for row in rows:
            for i, cell in enumerate(row):
                widths[i] = max(widths[i], len(str(cell)))

        # Ensure we don't exceed terminal width
        term_width = shutil.get_terminal_size().columns
        total_width = sum(widths) + len(widths) * 3 - 1

        if total_width > term_width:
            # Scale down widths proportionally
            scale = term_width / total_width
            widths = [int(w * scale) for w in widths]

        # Print header
        header_line = " | ".join(h.ljust(w)[:w] for h, w in zip(headers, widths))
        self.print(f"{Colors.BOLD}{header_line}{Colors.RESET}")
        self.print("-" * len(header_line))

        # Print rows
        for row in rows:
            row_cells = []
            for cell, width in zip(row, widths):
                cell_str = str(cell)[:width].ljust(width)

                # Highlight search term if present
                if highlight_search and highlight_search.lower() in cell_str.lower():
                    cell_str = cell_str.replace(
                        highlight_search,
                        f"{Colors.YELLOW}{Colors.BOLD}{highlight_search}{Colors.RESET}",
                    )

                row_cells.append(cell_str)

            self.print(" | ".join(row_cells))

    def job_status(self, job: Dict[str, Any]):
        """Print formatted job status"""
        if self.format == "json":
            self._json_buffer.append({"type": "job_status", "job": job})
            return

        self.header(f"📋 Job Status: {job['job_id']}")

        # Basic info
        self.print(f"{Colors.BOLD}Query:{Colors.RESET} {job['query']}")

        # Color-code status
        status = job["status"]
        if status == "completed":
            status_colored = f"{Colors.GREEN}{status}{Colors.RESET}"
        elif status == "active":
            status_colored = f"{Colors.YELLOW}{status}{Colors.RESET}"
        else:
            status_colored = f"{Colors.RED}{status}{Colors.RESET}"

        self.print(f"{Colors.BOLD}Status:{Colors.RESET} {status_colored}")

        self.print(f"{Colors.BOLD}Created:{Colors.RESET} {job['created_at']}")
        self.print(f"{Colors.BOLD}Updated:{Colors.RESET} {job['updated_at']}")
        self.print(f"{Colors.BOLD}Output:{Colors.RESET} {job['output_dir']}")

        # Statistics
        self.print(f"\n{Colors.BOLD}Statistics:{Colors.RESET}")
        stats = job["stats"]
        self.print(f"  Total URLs: {stats['total_urls']}")
        self.print(f"  Scraped: {stats['scraped_urls']}")
        self.print(f"  Filtered: {stats['filtered_urls']}")
        self.print(f"  Analyzed: {stats['analyzed_urls']}")

        if job.get("bundle_exists"):
            self.print(f"\n{Colors.GREEN}✅ Research bundle available{Colors.RESET}")

    def list_item(self, item: str, indent: int = 0):
        """Print a list item"""
        if self.format == "json":
            self._json_buffer.append(
                {"type": "list_item", "item": item, "indent": indent}
            )
        else:
            prefix = "  " * indent + "• "
            self.print(f"{prefix}{item}")

    def confirm(self, prompt: str, default: bool = False) -> bool:
        """Ask for user confirmation"""
        if self.format == "json" or self.quiet:
            return default

        suffix = " [Y/n]" if default else " [y/N]"
        response = input(f"{Colors.YELLOW}{prompt}{suffix}: {Colors.RESET}").lower()

        if not response:
            return default

        return response in ("y", "yes")

    def get_json_output(self) -> str:
        """Get JSON output (for JSON format)"""
        if self.format != "json":
            return ""

        return json.dumps(self._json_buffer, indent=2)

    def cleanup(self):
        """Clean up and output JSON if needed"""
        if self.format == "json" and self._json_buffer:
            print(self.get_json_output())


class ProgressTracker:
    """Track and display progress for long operations"""

    def __init__(self, formatter: OutputFormatter, total: int, message: str = ""):
        self.formatter = formatter
        self.total = total
        self.current = 0
        self.message = message
        self.start_time = datetime.now()

    def update(self, increment: int = 1, message: Optional[str] = None):
        """Update progress"""
        self.current += increment
        if message:
            self.message = message

        # Calculate ETA
        if self.current > 0:
            elapsed = (datetime.now() - self.start_time).total_seconds()
            rate = self.current / elapsed
            remaining = (self.total - self.current) / rate if rate > 0 else 0
            eta = f" ETA: {int(remaining)}s" if remaining > 1 else ""
        else:
            eta = ""

        self.formatter.progress(self.current, self.total, f"{self.message}{eta}")

    def complete(self, message: Optional[str] = None):
        """Mark as complete"""
        self.current = self.total
        if message:
            self.message = message
        self.formatter.progress(self.current, self.total, self.message)

======= prompt_utils.py ======
# Copyright 2025 Franz und Franz GmbH
#
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#     http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.

"""
Prompt utilities for m1f-research using shared prompt loader
"""

from pathlib import Path
from tools.shared.prompts import PromptLoader, format_prompt

# Initialize loader with research-specific prompts
_loader = PromptLoader(
    [
        Path(__file__).parent.parent / "shared" / "prompts" / "research",
        Path(__file__).parent / "prompts",  # Fallback to local prompts if any
    ]
)


def get_web_search_prompt(query: str, num_results: int = 20) -> str:
    """Get formatted web search prompt."""
    return _loader.format("llm/web_search.md", query=query, num_results=num_results)


def get_analysis_prompt(
    template_name: str, prompt_type: str, query: str, url: str, content: str
) -> str:
    """Get formatted analysis prompt for a specific template."""
    # Try template-specific prompt first
    prompt_name = f"analysis/{template_name}_{prompt_type}.md"

    # Set appropriate fallback - always use general as fallback since it exists
    fallback_name = f"analysis/general_{prompt_type}.md"

    try:
        base_prompt = _loader.load_with_fallback(prompt_name, fallback_name)
    except FileNotFoundError:
        # Ultimate fallback
        base_prompt = _loader.load("analysis/default_analysis.md")

    # For template-specific prompts, we need to add the full analysis structure
    if "Return ONLY valid JSON" not in base_prompt:
        analysis_template = _loader.load("analysis/default_analysis.md")
        # Replace the focus section with template-specific content
        base_prompt = (
            f"{base_prompt}\n\nURL: {{url}}\n\nContent:\n{{content}}\n\n"
            + analysis_template.split("Content:")[1].strip()
        )

    # Only pass url if the template contains {url}
    if "{url}" in base_prompt:
        return format_prompt(base_prompt, query=query, url=url, content=content)
    else:
        return format_prompt(base_prompt, query=query, content=content)


def get_synthesis_prompt(query: str, summaries: str) -> str:
    """Get formatted synthesis prompt."""
    return _loader.format("analysis/synthesis.md", query=query, summaries=summaries)


def get_subtopic_grouping_prompt(query: str, summaries: str) -> str:
    """Get formatted subtopic grouping prompt."""
    return _loader.format(
        "bundle/subtopic_grouping.md", query=query, summaries=summaries
    )


def get_topic_summary_prompt(topic: str, summaries: str) -> str:
    """Get formatted topic summary prompt."""
    return _loader.format("bundle/topic_summary.md", topic=topic, summaries=summaries)

======= readme_generator.py ======
# Copyright 2025 Franz und Franz GmbH
#
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#     http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.

"""
README generator for research bundles
"""
import logging
from pathlib import Path
from typing import List, Dict, Any, Optional
from datetime import datetime
import json

from ..m1f.file_operations import (
    safe_open,
)

from .models import AnalyzedContent
from .config import ResearchConfig

logger = logging.getLogger(__name__)


class ReadmeGenerator:
    """
    Generate comprehensive README files for research bundles with:
    - Executive summary
    - Key findings
    - Source overview
    - Usage instructions
    - Citation information
    """

    def __init__(self, config: ResearchConfig):
        self.config = config

    async def generate_readme(
        self,
        content_list: List[AnalyzedContent],
        research_query: str,
        output_dir: Path,
        topic_groups: Optional[Dict[str, List[AnalyzedContent]]] = None,
        synthesis: Optional[str] = None,
    ) -> Path:
        """
        Generate a README.md file for the research bundle

        Args:
            content_list: List of analyzed content
            research_query: Original research query
            output_dir: Directory containing the bundle
            topic_groups: Optional topic groupings
            synthesis: Optional research synthesis

        Returns:
            Path to the generated README file
        """
        readme_path = output_dir / "README.md"

        lines = []

        # Title and description
        lines.append(f"# Research Bundle: {research_query}")
        lines.append("")
        lines.append(
            f"Generated on {datetime.now().strftime('%Y-%m-%d %H:%M:%S')} using m1f-research"
        )
        lines.append("")

        # Quick stats
        lines.append("## Quick Stats")
        lines.append("")
        lines.append(f"- **Total Sources**: {len(content_list)}")
        if topic_groups:
            lines.append(f"- **Topics Covered**: {len(topic_groups)}")

        avg_relevance = (
            sum(item.relevance_score for item in content_list) / len(content_list)
            if content_list
            else 0
        )
        lines.append(f"- **Average Relevance**: {avg_relevance:.1f}/10")

        # Content type distribution
        content_types = {}
        for item in content_list:
            ct = item.content_type or "unknown"
            content_types[ct] = content_types.get(ct, 0) + 1

        lines.append(
            f"- **Content Types**: {', '.join(f'{k} ({v})' for k, v in content_types.items())}"
        )
        lines.append("")

        # Executive summary
        lines.append("## Executive Summary")
        lines.append("")

        if synthesis:
            lines.append(synthesis)
        else:
            lines.append(
                f"This research bundle contains {len(content_list)} curated sources about '{research_query}'. "
            )
            lines.append(
                "The sources have been analyzed for relevance and organized for easy navigation."
            )
        lines.append("")

        # Key findings
        if content_list:
            lines.append("## Key Findings")
            lines.append("")

            # Top 3 most relevant sources
            top_sources = sorted(
                content_list, key=lambda x: x.relevance_score, reverse=True
            )[:3]
            lines.append("### Most Relevant Sources")
            lines.append("")
            for i, source in enumerate(top_sources, 1):
                lines.append(
                    f"{i}. **[{source.title}]({source.url})** (Relevance: {source.relevance_score}/10)"
                )
                if source.summary:
                    lines.append(f"   - {source.summary[:150]}...")
                lines.append("")

            # Common themes
            if topic_groups and len(topic_groups) > 1:
                lines.append("### Main Topics")
                lines.append("")
                for topic, items in list(topic_groups.items())[:5]:
                    lines.append(f"- **{topic}**: {len(items)} sources")
                lines.append("")

        # How to use this bundle
        lines.append("## How to Use This Bundle")
        lines.append("")
        lines.append("1. **Quick Overview**: Start with the executive summary above")
        lines.append("2. **Deep Dive**: Open `research-bundle.md` for the full content")
        lines.append(
            "3. **Navigation**: Use the table of contents to jump to specific sources"
        )
        lines.append(
            "4. **By Topic**: Sources are organized by subtopic for logical flow"
        )
        lines.append("5. **Metadata**: Check `metadata.json` for additional details")
        lines.append("")

        # Source overview
        lines.append("## Source Overview")
        lines.append("")

        if topic_groups:
            for topic, items in topic_groups.items():
                lines.append(f"### {topic}")
                lines.append("")
                for item in items[:3]:  # Show top 3 per topic
                    lines.append(
                        f"- [{item.title}]({item.url}) - {item.relevance_score}/10"
                    )
                if len(items) > 3:
                    lines.append(f"- ...and {len(items) - 3} more")
                lines.append("")
        else:
            # Simple list if no topic groups
            for item in content_list[:10]:
                lines.append(
                    f"- [{item.title}]({item.url}) - {item.relevance_score}/10"
                )
            if len(content_list) > 10:
                lines.append(f"- ...and {len(content_list) - 10} more sources")
            lines.append("")

        # Research methodology
        lines.append("## Research Methodology")
        lines.append("")
        lines.append("This research was conducted using the following approach:")
        lines.append("")
        lines.append(
            f"1. **Search**: Found {self.config.url_count} potential sources using {self.config.llm.provider}"
        )
        lines.append(
            f"2. **Scrape**: Downloaded content from top {self.config.scrape_count} URLs"
        )
        lines.append(f"3. **Filter**: Applied quality and relevance filters")
        if not self.config.no_analysis:
            lines.append(
                f"4. **Analyze**: Used LLM to score relevance and extract key points"
            )
            lines.append(f"5. **Organize**: Grouped content by topics for logical flow")
        lines.append("")

        # Configuration used
        lines.append("### Configuration")
        lines.append("")
        lines.append("```yaml")
        lines.append(f"provider: {self.config.llm.provider}")
        lines.append(f"relevance_threshold: {self.config.analysis.relevance_threshold}")
        lines.append(f"min_content_length: {self.config.analysis.min_content_length}")
        if hasattr(self.config, "template") and self.config.template:
            lines.append(f"template: {self.config.template}")
        lines.append("```")
        lines.append("")

        # Files in this bundle
        lines.append("## Files in This Bundle")
        lines.append("")
        lines.append("- `README.md` - This file")
        lines.append("- `research-bundle.md` - Complete research content")
        if self.config.output.create_index:
            lines.append("- `index.md` - Alternative navigation by topic and relevance")
        if self.config.output.include_metadata:
            lines.append("- `metadata.json` - Detailed source metadata")
            lines.append("- `search_results.json` - Original search results")
        lines.append("")

        # Citation information
        lines.append("## Citation")
        lines.append("")
        lines.append("If you use this research bundle, please cite:")
        lines.append("")
        lines.append("```")
        lines.append(f"Research Bundle: {research_query}")
        lines.append(
            f"Generated by m1f-research on {datetime.now().strftime('%Y-%m-%d')}"
        )
        lines.append(f"Sources: {len(content_list)} web resources")
        lines.append("```")
        lines.append("")

        # License and attribution
        lines.append("## License & Attribution")
        lines.append("")
        lines.append("This research bundle aggregates content from various sources. ")
        lines.append("Each source retains its original copyright and license. ")
        lines.append("Please refer to individual sources for their specific terms.")
        lines.append("")

        # Footer
        lines.append("---")
        lines.append("")
        lines.append(
            "*Generated by [m1f-research](https://github.com/m1f/m1f) - AI-powered research tool*"
        )

        # Write README
        readme_content = "\n".join(lines)
        with safe_open(readme_path, "w", encoding="utf-8") as f:
            if f:
                f.write(readme_content)

        logger.info(f"Generated README at: {readme_path}")
        return readme_path

    async def generate_citation_file(
        self, content_list: List[AnalyzedContent], research_query: str, output_dir: Path
    ):
        """Generate a CITATIONS.md file with proper citations for all sources"""
        citations_path = output_dir / "CITATIONS.md"

        lines = []
        lines.append("# Citations")
        lines.append("")
        lines.append(f"Sources used in research for: {research_query}")
        lines.append("")

        # Group by domain for organization
        by_domain = {}
        for item in content_list:
            from urllib.parse import urlparse

            domain = urlparse(item.url).netloc
            if domain not in by_domain:
                by_domain[domain] = []
            by_domain[domain].append(item)

        # Generate citations by domain
        for domain, items in sorted(by_domain.items()):
            lines.append(f"## {domain}")
            lines.append("")

            for item in sorted(items, key=lambda x: x.title):
                lines.append(f"- **{item.title}**")
                lines.append(f"  - URL: {item.url}")
                lines.append(f"  - Accessed: {datetime.now().strftime('%Y-%m-%d')}")
                lines.append(f"  - Relevance Score: {item.relevance_score}/10")
                lines.append("")

        # Write citations file
        with safe_open(citations_path, "w", encoding="utf-8") as f:
            if f:
                f.write("\n".join(lines))

        logger.info(f"Generated citations at: {citations_path}")

======= research_db.py ======
# Copyright 2025 Franz und Franz GmbH
#
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#     http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.

"""
Database management for m1f-research with dual DB system
"""
import sqlite3
import json
import uuid
from pathlib import Path
from datetime import datetime
from typing import Dict, List, Optional, Any, Tuple
from dataclasses import dataclass, asdict
import logging

logger = logging.getLogger(__name__)


@dataclass
class ResearchJob:
    """Research job data model"""

    job_id: str
    query: str
    created_at: datetime
    updated_at: datetime
    status: str  # active, completed, failed
    config: Dict[str, Any]
    output_dir: str

    @classmethod
    def create_new(
        cls, query: str, config: Dict[str, Any], output_dir: str
    ) -> "ResearchJob":
        """Create a new research job"""
        now = datetime.now()
        return cls(
            job_id=str(uuid.uuid4())[:8],  # Short ID for convenience
            query=query,
            created_at=now,
            updated_at=now,
            status="active",
            config=config,
            output_dir=output_dir,
        )


class ResearchDatabase:
    """Main research jobs database manager"""

    def __init__(self, db_path: Path):
        self.db_path = db_path
        self.db_path.parent.mkdir(parents=True, exist_ok=True)
        self._init_database()

    def _init_database(self):
        """Initialize the main research database"""
        with sqlite3.connect(str(self.db_path)) as conn:
            conn.execute(
                """
                CREATE TABLE IF NOT EXISTS jobs (
                    job_id TEXT PRIMARY KEY,
                    query TEXT NOT NULL,
                    created_at TIMESTAMP NOT NULL,
                    updated_at TIMESTAMP NOT NULL,
                    status TEXT NOT NULL,
                    config TEXT NOT NULL,
                    output_dir TEXT NOT NULL
                )
            """
            )

            conn.execute(
                """
                CREATE TABLE IF NOT EXISTS job_stats (
                    job_id TEXT PRIMARY KEY,
                    total_urls INTEGER DEFAULT 0,
                    scraped_urls INTEGER DEFAULT 0,
                    filtered_urls INTEGER DEFAULT 0,
                    analyzed_urls INTEGER DEFAULT 0,
                    FOREIGN KEY (job_id) REFERENCES jobs(job_id)
                )
            """
            )

            conn.commit()

    def create_job(self, job: ResearchJob) -> str:
        """Create a new research job"""
        with sqlite3.connect(str(self.db_path)) as conn:
            conn.execute(
                """INSERT INTO jobs (job_id, query, created_at, updated_at, status, config, output_dir)
                   VALUES (?, ?, ?, ?, ?, ?, ?)""",
                (
                    job.job_id,
                    job.query,
                    job.created_at.isoformat(),
                    job.updated_at.isoformat(),
                    job.status,
                    json.dumps(job.config),
                    job.output_dir,
                ),
            )

            # Initialize stats
            conn.execute("INSERT INTO job_stats (job_id) VALUES (?)", (job.job_id,))

            conn.commit()

        logger.info(f"Created new job: {job.job_id} for query: {job.query}")
        return job.job_id

    def get_job(self, job_id: str) -> Optional[ResearchJob]:
        """Get a research job by ID"""
        with sqlite3.connect(str(self.db_path)) as conn:
            conn.row_factory = sqlite3.Row
            cursor = conn.execute("SELECT * FROM jobs WHERE job_id = ?", (job_id,))
            row = cursor.fetchone()

            if row:
                return ResearchJob(
                    job_id=row["job_id"],
                    query=row["query"],
                    created_at=datetime.fromisoformat(row["created_at"]),
                    updated_at=datetime.fromisoformat(row["updated_at"]),
                    status=row["status"],
                    config=json.loads(row["config"]),
                    output_dir=row["output_dir"],
                )

        return None

    def update_job_status(self, job_id: str, status: str):
        """Update job status"""
        with sqlite3.connect(str(self.db_path)) as conn:
            conn.execute(
                "UPDATE jobs SET status = ?, updated_at = ? WHERE job_id = ?",
                (status, datetime.now().isoformat(), job_id),
            )
            conn.commit()

    def update_job_stats(self, job_id: str, **stats):
        """Update job statistics"""
        with sqlite3.connect(str(self.db_path)) as conn:
            # Build dynamic update query
            updates = []
            values = []
            for key, value in stats.items():
                if key in [
                    "total_urls",
                    "scraped_urls",
                    "filtered_urls",
                    "analyzed_urls",
                ]:
                    updates.append(f"{key} = ?")
                    values.append(value)

            if updates:
                values.append(job_id)
                query = f"UPDATE job_stats SET {', '.join(updates)} WHERE job_id = ?"
                conn.execute(query, values)
                conn.commit()

    def list_jobs(
        self,
        status: Optional[str] = None,
        limit: Optional[int] = None,
        offset: int = 0,
        date_filter: Optional[str] = None,
        search_term: Optional[str] = None,
    ) -> List[Dict[str, Any]]:
        """
        List jobs with advanced filtering options

        Args:
            status: Filter by job status
            limit: Maximum number of results
            offset: Number of results to skip (for pagination)
            date_filter: Date filter in Y-M-D or Y-M format
            search_term: Search term to filter queries
        """
        with sqlite3.connect(str(self.db_path)) as conn:
            conn.row_factory = sqlite3.Row

            query = """
                SELECT j.*, s.total_urls, s.scraped_urls, s.filtered_urls, s.analyzed_urls
                FROM jobs j
                LEFT JOIN job_stats s ON j.job_id = s.job_id
                WHERE 1=1
            """
            params = []

            # Status filter
            if status:
                query += " AND j.status = ?"
                params.append(status)

            # Search term filter
            if search_term:
                query += " AND j.query LIKE ?"
                params.append(f"%{search_term}%")

            # Date filter
            if date_filter:
                if len(date_filter) == 10:  # Y-M-D format
                    query += " AND DATE(j.created_at) = ?"
                    params.append(date_filter)
                elif len(date_filter) == 7:  # Y-M format
                    query += " AND strftime('%Y-%m', j.created_at) = ?"
                    params.append(date_filter)
                elif len(date_filter) == 4:  # Y format
                    query += " AND strftime('%Y', j.created_at) = ?"
                    params.append(date_filter)

            # Order by created_at
            query += " ORDER BY j.created_at DESC"

            # Pagination
            if limit:
                query += f" LIMIT {limit} OFFSET {offset}"

            cursor = conn.execute(query, params)

            jobs = []
            for row in cursor:
                jobs.append(
                    {
                        "job_id": row["job_id"],
                        "query": row["query"],
                        "created_at": row["created_at"],
                        "updated_at": row["updated_at"],
                        "status": row["status"],
                        "output_dir": row["output_dir"],
                        "stats": {
                            "total_urls": row["total_urls"] or 0,
                            "scraped_urls": row["scraped_urls"] or 0,
                            "filtered_urls": row["filtered_urls"] or 0,
                            "analyzed_urls": row["analyzed_urls"] or 0,
                        },
                    }
                )

            return jobs

    def count_jobs(
        self,
        status: Optional[str] = None,
        date_filter: Optional[str] = None,
        search_term: Optional[str] = None,
    ) -> int:
        """Count jobs matching filters (for pagination)"""
        with sqlite3.connect(str(self.db_path)) as conn:
            query = "SELECT COUNT(*) FROM jobs WHERE 1=1"
            params = []

            if status:
                query += " AND status = ?"
                params.append(status)

            if search_term:
                query += " AND query LIKE ?"
                params.append(f"%{search_term}%")

            if date_filter:
                if len(date_filter) == 10:  # Y-M-D format
                    query += " AND DATE(created_at) = ?"
                    params.append(date_filter)
                elif len(date_filter) == 7:  # Y-M format
                    query += " AND strftime('%Y-%m', created_at) = ?"
                    params.append(date_filter)
                elif len(date_filter) == 4:  # Y format
                    query += " AND strftime('%Y', created_at) = ?"
                    params.append(date_filter)

            cursor = conn.execute(query, params)
            return cursor.fetchone()[0]


class JobDatabase:
    """Per-job database for URL and content tracking"""

    def __init__(self, db_path: Path):
        self.db_path = db_path
        self.db_path.parent.mkdir(parents=True, exist_ok=True)
        self._init_database()

    def _init_database(self):
        """Initialize the job-specific database"""
        with sqlite3.connect(str(self.db_path)) as conn:
            # URL tracking table
            conn.execute(
                """
                CREATE TABLE IF NOT EXISTS urls (
                    url TEXT PRIMARY KEY,
                    normalized_url TEXT,
                    host TEXT,
                    added_by TEXT NOT NULL,
                    added_at TIMESTAMP NOT NULL,
                    scraped_at TIMESTAMP,
                    status_code INTEGER,
                    content_checksum TEXT,
                    error_message TEXT
                )
            """
            )

            # Content storage
            conn.execute(
                """
                CREATE TABLE IF NOT EXISTS content (
                    url TEXT PRIMARY KEY,
                    title TEXT,
                    markdown TEXT NOT NULL,
                    metadata TEXT,
                    word_count INTEGER,
                    filtered BOOLEAN DEFAULT 0,
                    filter_reason TEXT,
                    FOREIGN KEY (url) REFERENCES urls(url)
                )
            """
            )

            # Analysis results
            conn.execute(
                """
                CREATE TABLE IF NOT EXISTS analysis (
                    url TEXT PRIMARY KEY,
                    relevance_score REAL,
                    key_points TEXT,
                    content_type TEXT,
                    analysis_data TEXT,
                    analyzed_at TIMESTAMP,
                    FOREIGN KEY (url) REFERENCES urls(url)
                )
            """
            )

            # Create indexes
            conn.execute("CREATE INDEX IF NOT EXISTS idx_urls_host ON urls(host)")
            conn.execute(
                "CREATE INDEX IF NOT EXISTS idx_urls_scraped ON urls(scraped_at)"
            )
            conn.execute(
                "CREATE INDEX IF NOT EXISTS idx_content_filtered ON content(filtered)"
            )

            conn.commit()

    def add_urls(self, urls: List[Dict[str, str]], added_by: str = "llm") -> int:
        """Add URLs to the database"""
        added_count = 0

        with sqlite3.connect(str(self.db_path)) as conn:
            for url_data in urls:
                url = url_data.get("url", "")
                if not url:
                    continue

                try:
                    # Normalize URL
                    from urllib.parse import urlparse, urlunparse

                    parsed = urlparse(url)
                    normalized = urlunparse(
                        (
                            parsed.scheme.lower(),
                            parsed.netloc.lower(),
                            parsed.path.rstrip("/"),
                            parsed.params,
                            parsed.query,
                            "",
                        )
                    )

                    conn.execute(
                        """INSERT OR IGNORE INTO urls 
                           (url, normalized_url, host, added_by, added_at)
                           VALUES (?, ?, ?, ?, ?)""",
                        (
                            url,
                            normalized,
                            parsed.netloc,
                            added_by,
                            datetime.now().isoformat(),
                        ),
                    )

                    if conn.total_changes > added_count:
                        added_count = conn.total_changes

                except Exception as e:
                    logger.error(f"Error adding URL {url}: {e}")

            conn.commit()

        return added_count

    def get_unscraped_urls(self) -> List[str]:
        """Get all URLs that haven't been scraped yet"""
        with sqlite3.connect(str(self.db_path)) as conn:
            cursor = conn.execute("SELECT url FROM urls WHERE scraped_at IS NULL")
            return [row[0] for row in cursor]

    def get_urls_by_host(self) -> Dict[str, List[str]]:
        """Get URLs grouped by host"""
        with sqlite3.connect(str(self.db_path)) as conn:
            cursor = conn.execute(
                "SELECT host, url FROM urls WHERE scraped_at IS NULL ORDER BY host"
            )

            urls_by_host = {}
            for host, url in cursor:
                if host not in urls_by_host:
                    urls_by_host[host] = []
                urls_by_host[host].append(url)

            return urls_by_host

    def mark_url_scraped(
        self,
        url: str,
        status_code: int,
        content_checksum: Optional[str] = None,
        error_message: Optional[str] = None,
    ):
        """Mark a URL as scraped"""
        with sqlite3.connect(str(self.db_path)) as conn:
            conn.execute(
                """UPDATE urls 
                   SET scraped_at = ?, status_code = ?, 
                       content_checksum = ?, error_message = ?
                   WHERE url = ?""",
                (
                    datetime.now().isoformat(),
                    status_code,
                    content_checksum,
                    error_message,
                    url,
                ),
            )
            conn.commit()

    def save_content(
        self,
        url: str,
        title: str,
        markdown: str,
        metadata: Dict[str, Any],
        filtered: bool = False,
        filter_reason: Optional[str] = None,
    ):
        """Save scraped content"""
        word_count = len(markdown.split())

        with sqlite3.connect(str(self.db_path)) as conn:
            conn.execute(
                """INSERT OR REPLACE INTO content 
                   (url, title, markdown, metadata, word_count, filtered, filter_reason)
                   VALUES (?, ?, ?, ?, ?, ?, ?)""",
                (
                    url,
                    title,
                    markdown,
                    json.dumps(metadata),
                    word_count,
                    filtered,
                    filter_reason,
                ),
            )
            conn.commit()

    def save_analysis(
        self,
        url: str,
        relevance_score: float,
        key_points: List[str],
        content_type: str,
        analysis_data: Dict[str, Any],
    ):
        """Save content analysis results"""
        with sqlite3.connect(str(self.db_path)) as conn:
            conn.execute(
                """INSERT OR REPLACE INTO analysis 
                   (url, relevance_score, key_points, content_type, 
                    analysis_data, analyzed_at)
                   VALUES (?, ?, ?, ?, ?, ?)""",
                (
                    url,
                    relevance_score,
                    json.dumps(key_points),
                    content_type,
                    json.dumps(analysis_data),
                    datetime.now().isoformat(),
                ),
            )
            conn.commit()

    def get_content_for_bundle(self) -> List[Dict[str, Any]]:
        """Get all non-filtered content for bundle creation"""
        with sqlite3.connect(str(self.db_path)) as conn:
            conn.row_factory = sqlite3.Row
            cursor = conn.execute(
                """
                SELECT c.*, a.relevance_score, a.key_points, a.content_type
                FROM content c
                LEFT JOIN analysis a ON c.url = a.url
                WHERE c.filtered = 0
                ORDER BY a.relevance_score DESC NULLS LAST
            """
            )

            content = []
            for row in cursor:
                content.append(
                    {
                        "url": row["url"],
                        "title": row["title"],
                        "markdown": row["markdown"],
                        "metadata": json.loads(row["metadata"]),
                        "word_count": row["word_count"],
                        "relevance_score": row["relevance_score"],
                        "key_points": (
                            json.loads(row["key_points"]) if row["key_points"] else []
                        ),
                        "content_type": row["content_type"],
                    }
                )

            return content

    def get_stats(self) -> Dict[str, int]:
        """Get job statistics"""
        with sqlite3.connect(str(self.db_path)) as conn:
            stats = {}

            # Total URLs
            cursor = conn.execute("SELECT COUNT(*) FROM urls")
            stats["total_urls"] = cursor.fetchone()[0]

            # Scraped URLs
            cursor = conn.execute(
                "SELECT COUNT(*) FROM urls WHERE scraped_at IS NOT NULL"
            )
            stats["scraped_urls"] = cursor.fetchone()[0]

            # Filtered URLs
            cursor = conn.execute("SELECT COUNT(*) FROM content WHERE filtered = 1")
            stats["filtered_urls"] = cursor.fetchone()[0]

            # Analyzed URLs
            cursor = conn.execute("SELECT COUNT(*) FROM analysis")
            stats["analyzed_urls"] = cursor.fetchone()[0]

            return stats

    def get_raw_content_files(self) -> List[Dict[str, Any]]:
        """Get information about raw HTML content that can be cleaned"""
        # Since we don't store raw HTML files anymore (we convert to markdown immediately),
        # this returns an empty list. In future, we could track original HTML if needed.
        return []

    def cleanup_raw_content(self) -> Dict[str, int]:
        """
        Clean up raw HTML data while preserving aggregated data
        Returns counts of cleaned items
        """
        # Currently, we don't store raw HTML separately
        # This is a placeholder for future implementation
        return {"files_deleted": 0, "space_freed": 0}

======= scraper.py ======
# Copyright 2025 Franz und Franz GmbH
#
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#     http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.

"""
Smart scraper with advanced features for m1f-research
"""
import asyncio
import random
import aiohttp
from typing import List, Dict, Optional, Any
from datetime import datetime
import logging
from urllib.parse import urlparse, urljoin
import re

from .models import ScrapedContent
from .config import ScrapingConfig

logger = logging.getLogger(__name__)


class SmartScraper:
    """
    Advanced web scraper with:
    - Random timeouts for politeness
    - Concurrent scraping with rate limiting
    - Auto-retry on failures
    - Progress tracking
    - Robots.txt respect
    """

    def __init__(self, config: ScrapingConfig):
        self.config = config
        self.session: Optional[aiohttp.ClientSession] = None
        self.semaphore = asyncio.Semaphore(config.max_concurrent)
        self.progress_callback = None
        self.total_urls = 0
        self.completed_urls = 0
        self.failed_urls = []

    async def __aenter__(self):
        """Async context manager entry"""
        self.session = aiohttp.ClientSession()
        return self

    async def __aexit__(self, exc_type, exc_val, exc_tb):
        """Async context manager exit"""
        if self.session:
            await self.session.close()

    def set_progress_callback(self, callback):
        """Set callback for progress updates"""
        self.progress_callback = callback

    async def scrape_urls(self, urls: List[Dict[str, str]]) -> List[ScrapedContent]:
        """
        Scrape multiple URLs concurrently

        Args:
            urls: List of dicts with 'url', 'title', 'description'

        Returns:
            List of successfully scraped content
        """
        self.total_urls = len(urls)
        self.completed_urls = 0
        self.failed_urls = []

        # Create scraping tasks
        tasks = [self._scrape_with_retry(url_info) for url_info in urls]

        # Execute concurrently
        results = await asyncio.gather(*tasks, return_exceptions=True)

        # Filter out failures and exceptions
        scraped_content = []
        for result in results:
            if isinstance(result, ScrapedContent):
                scraped_content.append(result)
            elif isinstance(result, Exception):
                logger.error(f"Scraping exception: {result}")

        logger.info(f"Scraped {len(scraped_content)}/{len(urls)} URLs successfully")
        return scraped_content

    async def _scrape_with_retry(
        self, url_info: Dict[str, str]
    ) -> Optional[ScrapedContent]:
        """Scrape a single URL with retry logic"""
        url = url_info["url"]

        for attempt in range(self.config.retry_attempts):
            try:
                result = await self._scrape_single_url(url_info)
                if result:
                    return result

            except Exception as e:
                logger.warning(f"Attempt {attempt + 1} failed for {url}: {e}")
                if attempt < self.config.retry_attempts - 1:
                    # Exponential backoff
                    await asyncio.sleep(2**attempt)

        # All attempts failed
        self.failed_urls.append(url)
        return None

    async def _scrape_single_url(
        self, url_info: Dict[str, str]
    ) -> Optional[ScrapedContent]:
        """Scrape a single URL with rate limiting"""
        async with self.semaphore:
            url = url_info["url"]

            # Random delay for politeness
            min_delay, max_delay = self._parse_timeout_range()
            delay = random.uniform(min_delay, max_delay)
            await asyncio.sleep(delay)

            # Check robots.txt if enabled
            if self.config.respect_robots_txt and not await self._check_robots_txt(url):
                logger.info(f"Skipping {url} due to robots.txt")
                return None

            # Prepare headers
            headers = {
                "User-Agent": random.choice(self.config.user_agents),
                **self.config.headers,
            }

            try:
                async with self.session.get(
                    url,
                    headers=headers,
                    timeout=aiohttp.ClientTimeout(total=30),
                    allow_redirects=True,
                ) as response:
                    # Update progress
                    self.completed_urls += 1
                    if self.progress_callback:
                        self.progress_callback(self.completed_urls, self.total_urls)

                    if response.status == 200:
                        html = await response.text()

                        # Convert to markdown
                        markdown = await self._html_to_markdown(html, url)

                        return ScrapedContent(
                            url=str(response.url),  # Use final URL after redirects
                            title=url_info.get("title", self._extract_title(html)),
                            content=markdown,
                            scraped_at=datetime.now(),
                            metadata={
                                "status_code": response.status,
                                "content_type": response.headers.get(
                                    "Content-Type", ""
                                ),
                                "content_length": len(html),
                                "final_url": str(response.url),
                            },
                        )
                    else:
                        logger.warning(f"HTTP {response.status} for {url}")
                        return None

            except asyncio.TimeoutError:
                logger.error(f"Timeout scraping {url}")
                return None
            except Exception as e:
                logger.error(f"Error scraping {url}: {e}")
                return None

    async def _html_to_markdown(self, html: str, base_url: str) -> str:
        """Convert HTML to Markdown"""
        try:
            # Try to import and use existing converters
            from markdownify import markdownify

            # Configure markdownify for better output
            markdown = markdownify(
                html,
                heading_style="ATX",
                bullets="-",
                code_language="python",
                wrap=True,
                wrap_width=80,
            )

            # Fix relative URLs
            markdown = self._fix_relative_urls(markdown, base_url)

            return markdown

        except ImportError:
            # Fallback to basic conversion
            return self._basic_html_to_markdown(html)

    def _basic_html_to_markdown(self, html: str) -> str:
        """Basic HTML to Markdown conversion"""
        # Remove script and style tags
        html = re.sub(
            r"<script[^>]*>.*?</script>", "", html, flags=re.DOTALL | re.IGNORECASE
        )
        html = re.sub(
            r"<style[^>]*>.*?</style>", "", html, flags=re.DOTALL | re.IGNORECASE
        )

        # Convert common tags
        conversions = [
            (r"<h1[^>]*>(.*?)</h1>", r"# \1\n"),
            (r"<h2[^>]*>(.*?)</h2>", r"## \1\n"),
            (r"<h3[^>]*>(.*?)</h3>", r"### \1\n"),
            (r"<h4[^>]*>(.*?)</h4>", r"#### \1\n"),
            (r"<h5[^>]*>(.*?)</h5>", r"##### \1\n"),
            (r"<h6[^>]*>(.*?)</h6>", r"###### \1\n"),
            (r"<p[^>]*>(.*?)</p>", r"\1\n\n"),
            (r"<br[^>]*>", "\n"),
            (r"<strong[^>]*>(.*?)</strong>", r"**\1**"),
            (r"<b[^>]*>(.*?)</b>", r"**\1**"),
            (r"<em[^>]*>(.*?)</em>", r"*\1*"),
            (r"<i[^>]*>(.*?)</i>", r"*\1*"),
            (r"<code[^>]*>(.*?)</code>", r"`\1`"),
            (r'<a[^>]+href="([^"]+)"[^>]*>(.*?)</a>', r"[\2](\1)"),
            (r"<li[^>]*>(.*?)</li>", r"- \1\n"),
            (r"<ul[^>]*>", "\n"),
            (r"</ul>", "\n"),
            (r"<ol[^>]*>", "\n"),
            (r"</ol>", "\n"),
        ]

        for pattern, replacement in conversions:
            html = re.sub(pattern, replacement, html, flags=re.DOTALL | re.IGNORECASE)

        # Remove remaining tags
        html = re.sub(r"<[^>]+>", "", html)

        # Clean up whitespace
        html = re.sub(r"\n\s*\n\s*\n", "\n\n", html)
        html = html.strip()

        return html

    def _fix_relative_urls(self, markdown: str, base_url: str) -> str:
        """Convert relative URLs to absolute URLs"""
        # Parse base URL
        parsed_base = urlparse(base_url)
        base_domain = f"{parsed_base.scheme}://{parsed_base.netloc}"

        # Fix markdown links
        def fix_link(match):
            text = match.group(1)
            url = match.group(2)

            # Skip if already absolute
            if url.startswith(("http://", "https://", "mailto:", "#")):
                return match.group(0)

            # Convert to absolute
            if url.startswith("/"):
                url = base_domain + url
            else:
                url = urljoin(base_url, url)

            return f"[{text}]({url})"

        markdown = re.sub(r"\[([^\]]+)\]\(([^)]+)\)", fix_link, markdown)

        return markdown

    def _extract_title(self, html: str) -> str:
        """Extract title from HTML"""
        title_match = re.search(
            r"<title[^>]*>(.*?)</title>", html, re.IGNORECASE | re.DOTALL
        )
        if title_match:
            title = title_match.group(1).strip()
            # Clean up title
            title = re.sub(r"\s+", " ", title)
            return title[:200]  # Limit length
        return "Untitled"

    async def _check_robots_txt(self, url: str) -> bool:
        """Check if URL is allowed by robots.txt"""
        # Simple implementation - in production would use robotparser
        parsed = urlparse(url)
        robots_url = f"{parsed.scheme}://{parsed.netloc}/robots.txt"

        try:
            async with self.session.get(
                robots_url, timeout=aiohttp.ClientTimeout(total=5)
            ) as response:
                if response.status == 200:
                    robots_txt = await response.text()
                    # Very basic check - just look for explicit disallow
                    path = parsed.path or "/"
                    if f"Disallow: {path}" in robots_txt:
                        return False
        except:
            # If we can't check robots.txt, assume it's OK
            pass

        return True

    def _parse_timeout_range(self) -> tuple[float, float]:
        """Parse timeout range string"""
        parts = self.config.timeout_range.split("-")
        if len(parts) == 2:
            return float(parts[0]), float(parts[1])
        else:
            val = float(parts[0])
            return val, val

    def get_stats(self) -> Dict[str, Any]:
        """Get scraping statistics"""
        return {
            "total_urls": self.total_urls,
            "completed_urls": self.completed_urls,
            "failed_urls": len(self.failed_urls),
            "success_rate": (
                self.completed_urls / self.total_urls if self.total_urls > 0 else 0
            ),
            "failed_url_list": self.failed_urls,
        }

======= smart_scraper.py ======
# Copyright 2025 Franz und Franz GmbH
#
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#     http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.

"""
Enhanced smart scraper with per-host delay management for m1f-research
"""
import asyncio
import random
import aiohttp
import hashlib
from typing import List, Dict, Optional, Any, Tuple
from datetime import datetime
from collections import defaultdict
import logging
from urllib.parse import urlparse, urljoin
import re

from .models import ScrapedContent
from .config import ScrapingConfig
from .url_manager import URLManager
from .research_db import JobDatabase

logger = logging.getLogger(__name__)


class HostDelayManager:
    """Manages delays per host to be polite to servers"""

    def __init__(
        self, delay_range: Tuple[float, float] = (1.0, 3.0), threshold: int = 3
    ):
        self.delay_range = delay_range
        self.threshold = threshold
        self.host_request_count = defaultdict(int)
        self.last_request_time = {}

    async def wait_if_needed(self, url: str):
        """Wait if we're making too many requests to the same host"""
        host = urlparse(url).netloc
        self.host_request_count[host] += 1

        # Only delay if we've made more than threshold requests to this host
        if self.host_request_count[host] > self.threshold:
            delay = random.uniform(*self.delay_range)
            logger.debug(
                f"Delaying {delay:.1f}s for host {host} (request #{self.host_request_count[host]})"
            )
            await asyncio.sleep(delay)
        else:
            logger.debug(
                f"No delay for host {host} (request #{self.host_request_count[host]})"
            )

    def get_host_stats(self) -> Dict[str, int]:
        """Get request counts per host"""
        return dict(self.host_request_count)


class EnhancedSmartScraper:
    """
    Enhanced web scraper with:
    - Per-host delay management
    - Database integration
    - Content checksum tracking
    - Better error handling
    """

    def __init__(
        self, config: ScrapingConfig, job_db: JobDatabase, url_manager: URLManager
    ):
        self.config = config
        self.job_db = job_db
        self.url_manager = url_manager
        self.session: Optional[aiohttp.ClientSession] = None
        self.semaphore = asyncio.Semaphore(config.max_concurrent)
        self.delay_manager = HostDelayManager(
            delay_range=(config.delay[0], config.delay[1]), threshold=3
        )

        # Progress tracking
        self.progress_callback = None
        self.total_urls = 0
        self.completed_urls = 0
        self.successful_urls = 0
        self.failed_urls = []

    async def __aenter__(self):
        """Async context manager entry"""
        headers = {"User-Agent": random.choice(self.config.user_agents)}
        timeout = aiohttp.ClientTimeout(total=self.config.timeout)
        self.session = aiohttp.ClientSession(headers=headers, timeout=timeout)
        return self

    async def __aexit__(self, exc_type, exc_val, exc_tb):
        """Async context manager exit"""
        if self.session:
            await self.session.close()

    def set_progress_callback(self, callback):
        """Set callback for progress updates"""
        self.progress_callback = callback

    async def scrape_urls(self, urls: List[str]) -> List[ScrapedContent]:
        """
        Scrape multiple URLs with smart host-based delays
        """
        self.total_urls = len(urls)
        self.completed_urls = 0
        self.successful_urls = 0
        self.failed_urls = []

        # Group URLs by host for smart scheduling
        urls_by_host = defaultdict(list)
        for url in urls:
            host = urlparse(url).netloc
            urls_by_host[host].append(url)

        logger.info(
            f"Scraping {len(urls)} URLs from {len(urls_by_host)} different hosts"
        )

        # Create tasks with mixed hosts for better parallelism
        tasks = []
        url_queue = []

        # Interleave URLs from different hosts
        max_urls = max(len(urls) for urls in urls_by_host.values())
        for i in range(max_urls):
            for host, host_urls in urls_by_host.items():
                if i < len(host_urls):
                    url_queue.append(host_urls[i])

        # Create scraping tasks
        for url in url_queue:
            task = self._scrape_with_semaphore(url)
            tasks.append(task)

        # Run all tasks
        results = await asyncio.gather(*tasks, return_exceptions=True)

        # Process results
        scraped_content = []
        for url, result in zip(url_queue, results):
            if isinstance(result, Exception):
                logger.error(f"Failed to scrape {url}: {result}")
                self.failed_urls.append(url)
            elif result:
                scraped_content.append(result)
                self.successful_urls += 1

        # Log statistics
        logger.info(
            f"Scraping complete: {self.successful_urls}/{self.total_urls} successful"
        )
        if self.failed_urls:
            logger.warning(f"Failed to scrape {len(self.failed_urls)} URLs")

        host_stats = self.delay_manager.get_host_stats()
        logger.info(
            f"Requests per host: {dict(list(host_stats.items())[:5])}..."
        )  # Show first 5

        return scraped_content

    async def _scrape_with_semaphore(self, url: str) -> Optional[ScrapedContent]:
        """Scrape a single URL with semaphore control"""
        async with self.semaphore:
            return await self._scrape_url(url)

    async def _scrape_url(self, url: str) -> Optional[ScrapedContent]:
        """Scrape a single URL with retries and smart delays"""
        # Apply per-host delay if needed
        await self.delay_manager.wait_if_needed(url)

        # Check robots.txt if configured
        if not await self._check_robots_txt(url):
            logger.info(f"Skipping {url} due to robots.txt")
            self.url_manager.mark_url_scraped(
                url, -1, error_message="Blocked by robots.txt"
            )
            self._update_progress()
            return None

        # Try scraping with retries
        for attempt in range(self.config.retries):
            try:
                async with self.session.get(url) as response:
                    if response.status == 200:
                        content = await response.text()

                        # Calculate content checksum
                        content_checksum = hashlib.sha256(content.encode()).hexdigest()

                        # Create scraped content object
                        scraped = ScrapedContent(
                            url=url,
                            title=self._extract_title(content),
                            content=content,
                            content_type=response.headers.get("Content-Type", ""),
                            scraped_at=datetime.now(),
                        )

                        # Mark as scraped in database
                        self.url_manager.mark_url_scraped(
                            url, response.status, content_checksum=content_checksum
                        )

                        self._update_progress()
                        return scraped

                    else:
                        error_msg = f"HTTP {response.status}"
                        if attempt < self.config.retries - 1:
                            logger.warning(f"{error_msg} for {url}, retrying...")
                            await asyncio.sleep(2**attempt)  # Exponential backoff
                        else:
                            logger.warning(f"{error_msg} for {url}")
                            self.url_manager.mark_url_scraped(
                                url, response.status, error_message=error_msg
                            )
                            self._update_progress()
                            return None

            except asyncio.TimeoutError:
                error_msg = "Timeout"
                if attempt < self.config.retries - 1:
                    logger.warning(f"Timeout for {url}, retrying...")
                    await asyncio.sleep(2**attempt)
                else:
                    logger.error(
                        f"Timeout for {url} after {self.config.retries} attempts"
                    )
                    self.url_manager.mark_url_scraped(url, -1, error_message=error_msg)
                    self._update_progress()
                    return None

            except Exception as e:
                error_msg = str(e)
                logger.error(f"Error scraping {url}: {e}")
                if attempt < self.config.retries - 1:
                    await asyncio.sleep(2**attempt)
                else:
                    self.url_manager.mark_url_scraped(url, -1, error_message=error_msg)
                    self._update_progress()
                    return None

        return None

    async def _check_robots_txt(self, url: str) -> bool:
        """Check if URL is allowed by robots.txt"""
        if not self.config.respect_robots_txt:
            return True

        try:
            parsed = urlparse(url)
            robots_url = f"{parsed.scheme}://{parsed.netloc}/robots.txt"

            # Simple check - just see if robots.txt mentions the path
            async with self.session.get(robots_url) as response:
                if response.status == 200:
                    content = await response.text()
                    path = parsed.path or "/"

                    # Very basic robots.txt parsing
                    lines = content.lower().split("\n")
                    user_agent_applies = False

                    for line in lines:
                        line = line.strip()
                        if line.startswith("user-agent:"):
                            user_agent_applies = "*" in line or "bot" in line
                        elif user_agent_applies and line.startswith("disallow:"):
                            disallowed = line.split(":", 1)[1].strip()
                            if disallowed and path.lower().startswith(disallowed):
                                return False

            return True

        except Exception as e:
            logger.debug(f"Could not check robots.txt for {url}: {e}")
            return True  # Allow if we can't check

    def _extract_title(self, html: str) -> str:
        """Extract title from HTML"""
        match = re.search(r"<title[^>]*>([^<]+)</title>", html, re.IGNORECASE)
        if match:
            return match.group(1).strip()

        # Try h1 as fallback
        match = re.search(r"<h1[^>]*>([^<]+)</h1>", html, re.IGNORECASE)
        if match:
            return match.group(1).strip()

        return "Untitled"

    def _update_progress(self):
        """Update progress and call callback if set"""
        self.completed_urls += 1
        if self.progress_callback:
            progress = (self.completed_urls / self.total_urls) * 100
            self.progress_callback(self.completed_urls, self.total_urls, progress)

    def get_statistics(self) -> Dict[str, Any]:
        """Get scraping statistics"""
        return {
            "total_urls": self.total_urls,
            "completed_urls": self.completed_urls,
            "successful_urls": self.successful_urls,
            "failed_urls": len(self.failed_urls),
            "host_stats": self.delay_manager.get_host_stats(),
        }

======= url_manager.py ======
# Copyright 2025 Franz und Franz GmbH
#
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#     http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.

"""
URL management for m1f-research with file support and deduplication
"""
import logging
from pathlib import Path
from typing import List, Dict, Any, Optional
from urllib.parse import urlparse, urlunparse

from ..m1f.file_operations import (
    safe_exists,
    safe_read_text,
)

from .research_db import JobDatabase

logger = logging.getLogger(__name__)


class URLManager:
    """Manages URL collection, deduplication, and tracking"""

    def __init__(self, job_db: JobDatabase):
        self.job_db = job_db

    def add_urls_from_list(
        self, urls: List[Dict[str, str]], source: str = "llm"
    ) -> int:
        """Add URLs from a list (LLM-generated or manual)"""
        return self.job_db.add_urls(urls, added_by=source)

    async def add_urls_from_file(self, file_path: Path) -> int:
        """Add URLs from a text file (one URL per line)"""
        if not safe_exists(file_path):
            logger.error(f"URL file not found: {file_path}")
            return 0

        urls = []
        try:
            content = safe_read_text(file_path)
            for line in content.splitlines():
                line = line.strip()
                if line and not line.startswith("#"):  # Skip comments
                    # Support optional title after URL
                    parts = line.split("\t", 1)
                    url = parts[0].strip()
                    title = parts[1].strip() if len(parts) > 1 else ""

                    if url.startswith(("http://", "https://")):
                        urls.append(
                            {
                                "url": url,
                                "title": title,
                                "description": f"From file: {file_path.name}",
                            }
                        )

        except Exception as e:
            logger.error(f"Error reading URL file {file_path}: {e}")
            return 0

        logger.info(f"Found {len(urls)} URLs in {file_path}")
        return self.add_urls_from_list(urls, source="manual")

    def get_unscraped_urls(self) -> List[str]:
        """Get all URLs that haven't been scraped yet"""
        return self.job_db.get_unscraped_urls()

    def get_urls_grouped_by_host(self) -> Dict[str, List[str]]:
        """Get unscraped URLs grouped by host for smart delay management"""
        return self.job_db.get_urls_by_host()

    def normalize_url(self, url: str) -> str:
        """Normalize a URL for deduplication"""
        try:
            parsed = urlparse(url)

            # Normalize components
            scheme = parsed.scheme.lower()
            netloc = parsed.netloc.lower()
            path = parsed.path.rstrip("/")

            # Remove default ports
            if netloc.endswith(":80") and scheme == "http":
                netloc = netloc[:-3]
            elif netloc.endswith(":443") and scheme == "https":
                netloc = netloc[:-4]

            # Reconstruct URL
            normalized = urlunparse(
                (
                    scheme,
                    netloc,
                    path,
                    parsed.params,
                    parsed.query,
                    "",  # Remove fragment
                )
            )

            return normalized

        except Exception as e:
            logger.warning(f"Could not normalize URL {url}: {e}")
            return url

    def deduplicate_urls(self, urls: List[str]) -> List[str]:
        """Remove duplicate URLs based on normalization"""
        seen = set()
        unique = []

        for url in urls:
            normalized = self.normalize_url(url)
            if normalized not in seen:
                seen.add(normalized)
                unique.append(url)

        if len(urls) != len(unique):
            logger.info(f"Deduplicated {len(urls)} URLs to {len(unique)} unique URLs")

        return unique

    def get_host_from_url(self, url: str) -> str:
        """Extract host from URL"""
        try:
            return urlparse(url).netloc
        except:
            return "unknown"

    def create_url_batches(self, max_per_host: int = 5) -> List[List[str]]:
        """Create URL batches for parallel scraping with host limits"""
        urls_by_host = self.get_urls_grouped_by_host()
        batches = []

        # First pass: Add up to max_per_host from each host
        current_batch = []
        host_counts = {}

        for host, urls in urls_by_host.items():
            for url in urls[:max_per_host]:
                current_batch.append(url)
                host_counts[host] = host_counts.get(host, 0) + 1

                # Create new batch when we have enough diversity
                if len(current_batch) >= 10:  # Batch size
                    batches.append(current_batch)
                    current_batch = []

        # Add remaining URLs
        if current_batch:
            batches.append(current_batch)

        # Second pass: Add remaining URLs from hosts with many URLs
        for host, urls in urls_by_host.items():
            if len(urls) > max_per_host:
                remaining = urls[max_per_host:]
                for i in range(0, len(remaining), max_per_host):
                    batch = remaining[i : i + max_per_host]
                    batches.append(batch)

        logger.info(f"Created {len(batches)} URL batches for scraping")
        return batches

    def mark_url_scraped(
        self,
        url: str,
        status_code: int,
        content_checksum: Optional[str] = None,
        error_message: Optional[str] = None,
    ):
        """Mark a URL as scraped"""
        self.job_db.mark_url_scraped(url, status_code, content_checksum, error_message)

    def get_stats(self) -> Dict[str, int]:
        """Get URL statistics"""
        return self.job_db.get_stats()
