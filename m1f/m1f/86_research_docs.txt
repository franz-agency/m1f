======= README.md ======
# m1f-research Documentation

> ‚ö†Ô∏è **Early Alpha Warning**: m1f-research is in early alpha and under heavy development. Expect significant changes to features, APIs, and workflows.

AI-powered research orchestrator that automates the entire workflow from finding sources to creating Claude-ready knowledge bundles.

## What is m1f-research?

While the m1f toolkit provides individual tools (m1f-scrape, m1f-html2md, m1f), **m1f-research orchestrates them all** into an intelligent research pipeline:

1. **AI finds sources** on any topic (using Claude/Gemini)
2. **You review and curate** the URLs (unlike black-box LLM research modes)
3. **Automatically processes** everything: scrape ‚Üí convert ‚Üí bundle
4. **Creates knowledge bundles** ready for Claude's Project Knowledge

**Key advantage**: Unlike LLM "research modes", YOU control which sources are used and can add your own trusted sources.

## üöÄ New: 7-Phase Workflow System

The m1f-research tool now features an advanced workflow system that transforms simple queries into comprehensive research:

1. **Query Expansion** - Automatically generate search variations
2. **URL Collection** - Gather URLs from multiple sources  
3. **Interactive Review** - Curate URLs before scraping
4. **Deep Crawling** - Follow links for comprehensive coverage
5. **Smart Bundling** - Organize content intelligently
6. **AI Analysis** - Generate insights and summaries
7. **Phase Management** - Resume from any phase

## Documentation Files

- [60_research_overview.md](60_research_overview.md) - Overview, workflow system, and quick start
- [62_job_management.md](62_job_management.md) - Job persistence and filtering
- [63_cli_reference.md](63_cli_reference.md) - Complete command reference with workflow options
- [64_api_reference.md](64_api_reference.md) - Developer documentation
- [65_architecture.md](65_architecture.md) - Technical architecture and workflow components
- [66_examples.md](66_examples.md) - Real-world usage examples with workflow scenarios
- [67_cli_improvements.md](67_cli_improvements.md) - Enhanced CLI features and UX
- [68_job_deletion_guide.md](68_job_deletion_guide.md) - Complete guide to job deletion

## Quick Start

```bash
# Basic research with automatic workflow
m1f-research "python async programming"

# Interactive research with URL review
m1f-research "machine learning frameworks" --expand-queries --max-queries 10

# Deep research with crawling
m1f-research "rust web frameworks" --crawl-depth 2 --max-pages-per-site 20

# Generate comparative analysis
m1f-research "cloud providers" --analysis-type comparative

# List all jobs with phase status
m1f-research --list-jobs

# Resume a job from specific phase
m1f-research --resume abc123
```

## Workflow Configuration

Create a `.m1f-research.yml` for custom workflows:

```yaml
workflow:
  expand_queries: true      # Generate search variations
  max_queries: 5           # Number of query variations
  skip_review: false       # Interactive URL review
  crawl_depth: 1          # Follow links one level deep
  max_pages_per_site: 10  # Limit per domain
  generate_analysis: true  # Create AI analysis
  analysis_type: summary   # summary/comparative/technical/trend
```

For detailed documentation, see the numbered files above.

======= README.md ======
# m1f-research

> **Note**: This documentation has been moved to
> [docs/06_research/](../../docs/06_research/)

AI-powered research tool that automatically finds, scrapes, and bundles
information on any topic.

## Overview

m1f-research extends the m1f toolkit with intelligent research capabilities. It
uses LLMs to:

- Find relevant URLs for any research topic
- Scrape and convert web content to clean Markdown
- Analyze content for relevance and extract key insights
- Create organized research bundles

## Quick Start

```bash
# Basic research
m1f-research "microservices best practices"

# Research with more sources
m1f-research "react state management" --urls 30 --scrape 15

# Research with manual URL list
m1f-research "python async" --urls-file my-links.txt

# Resume an existing job
m1f-research --resume abc123

# Add more URLs to existing job
m1f-research --resume abc123 --urls-file more-links.txt

# List all research jobs
m1f-research --list-jobs

# Check job status
m1f-research --status abc123

# Use different LLM provider
m1f-research "machine learning" --provider gemini

# Interactive mode
m1f-research --interactive
```

## Installation

m1f-research is included with the m1f toolkit. Ensure you have:

1. m1f installed with the research extension
2. An API key for your chosen LLM provider:
   - Claude: Set `ANTHROPIC_API_KEY`
   - Gemini: Set `GOOGLE_API_KEY`

## Features

### üóÑÔ∏è Job Management

- **Persistent Jobs**: All research jobs are tracked in a SQLite database
- **Resume Support**: Continue interrupted research or add more URLs
- **Job History**: View all past research with statistics
- **Per-Job Database**: Each job has its own database for URL/content tracking

### üîç Intelligent Search

- Uses LLMs to find high-quality, relevant URLs
- Focuses on authoritative sources
- Mixes different content types (tutorials, docs, discussions)
- **Manual URL Support**: Add your own URLs via --urls-file

### üì• Smart Scraping

- **Per-Host Delay Management**: Only delays after 3+ requests to same host
- Concurrent scraping with intelligent scheduling
- Automatic HTML to Markdown conversion
- Handles failures gracefully
- **Content Deduplication**: Tracks content by checksum

### üß† Content Analysis

- Relevance scoring (0-10 scale)
- Key points extraction
- Content summarization
- Duplicate detection

### üì¶ Organized Bundles

- **Hierarchical Output**: YYYY/MM/DD directory structure
- **Prominent Bundle Files**: üìö_RESEARCH_BUNDLE.md and üìä_EXECUTIVE_SUMMARY.md
- Clean Markdown output
- Table of contents
- Source metadata
- Relevance-based ordering

## Usage Examples

### Basic Research

```bash
# Research a programming topic
m1f-research "golang error handling"

# Output saved to: ./research-data/golang-error-handling-20240120-143022/
```

### Advanced Options

```bash
# Specify output location and name
m1f-research "kubernetes security" \
  --output ./research \
  --name k8s-security

# Use a specific template
m1f-research "react hooks" --template technical

# Skip analysis for faster results
m1f-research "python asyncio" --no-analysis

# Dry run to see what would happen
m1f-research "rust ownership" --dry-run
```

### Configuration File

```bash
# Use a custom configuration
m1f-research "database optimization" --config research.yml
```

## Configuration

### Command Line Options

| Option             | Description                   | Default          |
| ------------------ | ----------------------------- | ---------------- |
| `--urls`           | Number of URLs to find        | 20               |
| `--scrape`         | Number of URLs to scrape      | 10               |
| `--output`         | Output directory              | ./research-data  |
| `--name`           | Bundle name                   | auto-generated   |
| `--provider`       | LLM provider                  | claude           |
| `--model`          | Specific model                | provider default |
| `--template`       | Research template             | general          |
| `--concurrent`     | Max concurrent scrapes        | 5                |
| `--no-filter`      | Disable filtering             | false            |
| `--no-analysis`    | Skip analysis                 | false            |
| `--interactive`    | Interactive mode              | false            |
| `--dry-run`        | Preview only                  | false            |
| **Job Management** |                               |                  |
| `--resume`         | Resume existing job by ID     | None             |
| `--list-jobs`      | List all research jobs        | false            |
| `--status`         | Show job status by ID         | None             |
| `--urls-file`      | File with URLs (one per line) | None             |

### Configuration File (.m1f.config.yml)

```yaml
research:
  # LLM settings
  llm:
    provider: claude
    model: claude-3-opus-20240229
    temperature: 0.7

  # Default counts
  defaults:
    url_count: 30
    scrape_count: 15

  # Scraping behavior
  scraping:
    timeout_range: "1-3"
    max_concurrent: 5
    retry_attempts: 2

  # Content analysis
  analysis:
    relevance_threshold: 7.0
    min_content_length: 100
    prefer_code_examples: true

  # Output settings
  output:
    directory: ./research-data
    create_summary: true
    create_index: true

  # Research templates
  templates:
    technical:
      description: "Technical documentation and code"
      url_count: 30
      analysis_focus: implementation

    academic:
      description: "Academic papers and theory"
      url_count: 20
      analysis_focus: theory
```

## Templates

Pre-configured templates optimize research for different needs:

### technical

- Focuses on implementation details
- Prioritizes code examples
- Higher URL count for comprehensive coverage

### academic

- Emphasizes theoretical content
- Looks for citations and references
- Filters for authoritative sources

### tutorial

- Searches for step-by-step guides
- Prioritizes beginner-friendly content
- Includes examples and exercises

### general (default)

- Balanced approach
- Mixes different content types
- Suitable for most topics

## Output Structure

Research bundles are organized with hierarchical date structure:

```
./research-data/
‚îú‚îÄ‚îÄ research_jobs.db              # Main job tracking database
‚îú‚îÄ‚îÄ latest_research.md           # Symlink to most recent bundle
‚îî‚îÄ‚îÄ 2025/
    ‚îî‚îÄ‚îÄ 07/
        ‚îî‚îÄ‚îÄ 22/
            ‚îî‚îÄ‚îÄ abc123_topic-name/
                ‚îú‚îÄ‚îÄ research.db           # Job-specific database
                ‚îú‚îÄ‚îÄ üìö_RESEARCH_BUNDLE.md # Main research bundle
                ‚îú‚îÄ‚îÄ üìä_EXECUTIVE_SUMMARY.md # Executive summary
                ‚îú‚îÄ‚îÄ research-bundle.md    # Standard bundle
                ‚îú‚îÄ‚îÄ metadata.json         # Research metadata
                ‚îî‚îÄ‚îÄ search_results.json   # Found URLs
```

### Bundle Format

```markdown
# Research: [Your Topic]

Generated on: 2024-01-20 14:30:22 Total sources: 10

---

## Table of Contents

1. [Source Title 1](#1-source-title-1)
2. [Source Title 2](#2-source-title-2) ...

---

## Summary

[Research summary and top sources]

---

## 1. Source Title

**Source:** https://example.com/article **Relevance:** 8.5/10

### Key Points:

- Important point 1
- Important point 2

### Content:

[Full content in Markdown]

---
```

## Providers

### Claude (Anthropic)

- Default provider
- Best for: Comprehensive research, nuanced analysis
- Set: `ANTHROPIC_API_KEY`

### Claude Code SDK

- Use `--provider claude-cli` for Claude Code SDK integration
- Provides proper session management and streaming
- No API key needed if using Claude Code authentication

### Gemini (Google)

- Fast and efficient
- Best for: Quick research, technical topics
- Set: `GOOGLE_API_KEY`

### CLI Tools

- Use local tools like `gemini-cli`
- Best for: Privacy, offline capability
- Example: `--provider gemini-cli`

## Tips

1. **Start broad, then narrow**: Use more URLs initially, let analysis filter
2. **Use templates**: Match template to your research goal
3. **Interactive mode**: Great for exploratory research
4. **Combine with m1f**: Feed research bundles into m1f for AI analysis

## Troubleshooting

### No API Key

```
Error: API key not set for ClaudeProvider
```

Solution: Set environment variable or pass in config

### Rate Limiting

```
Error: 429 Too Many Requests
```

Solution: Reduce `--concurrent` or increase timeout range

### Low Quality Results

- Increase `--urls` for more options
- Adjust `relevance_threshold` in config
- Try different `--template`

## Database Architecture

### Main Database (research_jobs.db)

Tracks all research jobs:

- Job ID, query, status, timestamps
- Configuration used
- Statistics (URLs found, scraped, analyzed)

### Per-Job Database (research.db)

Tracks job-specific data:

- URLs (source, status, checksums)
- Content (markdown, metadata)
- Analysis results (scores, key points)

### Smart Delay Management

The scraper implements intelligent per-host delays:

- No delay for first 3 requests to a host
- Random 1-3 second delay after threshold
- Allows fast parallel scraping of different hosts

## Future Features

- Multi-source research (GitHub, arXiv, YouTube)
- Knowledge graph building
- Research collaboration
- Custom source plugins
- Web UI
- Export to various formats (PDF, DOCX)
- Integration with note-taking tools

## Contributing

m1f-research is part of the m1f project. Contributions welcome!

- Report issues: [GitHub Issues](https://github.com/m1f/m1f/issues)
- Submit PRs: Follow m1f contribution guidelines
- Request features: Open a discussion

======= 60_research_overview.md ======
# m1f-research

AI-powered research tool that orchestrates the entire research workflow: from finding sources to creating AI-ready knowledge bundles.

## The Big Picture: Why m1f-research?

The m1f toolkit already provides powerful individual tools:
- **m1f-scrape**: Scrapes websites and downloads content
- **m1f-html2md**: Converts HTML to clean Markdown
- **m1f**: Bundles files for AI consumption

**m1f-research takes this further by automating the entire research workflow:**

1. **AI-Powered Discovery**: Instead of manually finding URLs, m1f-research uses AI (like Claude) to search for relevant sources on any topic
2. **Human-in-the-Loop**: Review and curate the discovered URLs before scraping
3. **Automated Pipeline**: Automatically scrapes approved URLs, converts to Markdown, and creates bundles
4. **AI-Ready Output**: Produces research bundles perfect for uploading to Claude's Project Knowledge or other AI tools

Think of it as your automated research assistant that:
- Takes a topic (e.g., "microservices best practices")
- Asks AI to find the best sources
- Lets you review what it found
- Downloads and processes everything
- Creates a knowledge bundle ready for AI analysis

### Key Advantage Over LLM "Research Mode"

**The crucial difference from built-in LLM research modes:** 
- **You control the sources**: Review and prioritize which URLs to include
- **Custom curation**: Remove low-quality or irrelevant sources before processing
- **Add your own sources**: Include specific URLs you trust
- **Persistent knowledge**: Create reusable knowledge bundles for your projects
- **No black box**: You see exactly what content goes into your research

## Overview

m1f-research orchestrates the complete research pipeline by intelligently combining all m1f tools:

- **Discovers** relevant URLs using AI (instead of manual searching)
- **Reviews** URLs with optional human curation
- **Scrapes** approved content using m1f-scrape
- **Converts** HTML to Markdown using m1f-html2md
- **Bundles** everything using m1f for AI consumption
- **Analyzes** content to extract key insights

### 7-Phase Workflow System

m1f-research implements a sophisticated workflow system that breaks research into 7 distinct phases:

1. **INITIALIZATION** - Setup and configuration validation
2. **QUERY_EXPANSION** - Generate multiple search query variations for comprehensive coverage
3. **URL_COLLECTION** - Search for relevant URLs using expanded queries
4. **URL_REVIEW** - Optional human review and curation of found URLs
5. **CRAWLING** - Intelligent web scraping with depth control and link following
6. **BUNDLING** - Organize scraped content into structured research bundles
7. **ANALYSIS** - Generate AI-powered insights and summaries

This workflow allows for flexible research strategies, checkpoint resumption, and fine-grained control over each phase.

## Practical Example: Research Workflow

Let's say you want to research "Python async programming best practices":

### Without m1f-research (Manual Process):
1. Google search for relevant articles
2. Open each link, evaluate quality
3. Use `m1f-scrape` on each good URL
4. Run `m1f-html2md` to convert each file
5. Use `m1f` to bundle everything
6. Upload to Claude's Project Knowledge

### With m1f-research (Automated):
```bash
# One command does it all!
m1f-research "Python async programming best practices"
```

What happens behind the scenes:
1. **AI searches** for the best sources on Python async programming
2. **You review** the URLs (optional - can be skipped)
3. **Automatically scrapes** all approved URLs
4. **Converts** everything to Markdown
5. **Creates bundle** ready for Claude: `research_bundle.md`
6. **Generates summary** with key insights

The result: A complete research bundle you can directly upload to Claude's Project Knowledge!

## Quick Start

```bash
# Basic research - let AI find sources
m1f-research "microservices best practices"

# Research with more sources
m1f-research "react state management" --urls 30 --scrape 15

# Interactive URL review (recommended for important research)
m1f-research "kubernetes security" --skip-review false

# Control query expansion
m1f-research "python asyncio" --max-queries 1  # Only use original query
m1f-research "rust ownership" --max-queries 10  # More query variations

# Provide custom query variations
m1f-research "react hooks" --custom-queries \
  "react hooks tutorial 2025" \
  "useEffect vs useLayoutEffect" \
  "custom react hooks patterns"

# Interactive query input
m1f-research "database optimization" --interactive-queries

# Use the research bundle with Claude
# The output file research_bundle.md can be uploaded directly to Claude's Project Knowledge

# List all research jobs
m1f-research --list-jobs

# Resume a job
m1f-research --resume abc123

# Filter jobs by date
m1f-research --list-jobs --date 2025-07

# Search for specific topics
m1f-research --list-jobs --search "python"
```

## Installation

m1f-research is included with the m1f toolkit. Ensure you have:

1. m1f installed with the research extension
2. An API key for your chosen LLM provider:
   - Claude: Set `ANTHROPIC_API_KEY`
   - Gemini: Set `GOOGLE_API_KEY`

## Features

### üóÑÔ∏è Job Management

- **Persistent Jobs**: All research tracked in SQLite database
- **Resume Support**: Continue interrupted research at any workflow phase
- **Advanced Filtering**: Search by date, query term
- **Pagination**: Handle large job lists efficiently
- **Phase Tracking**: Monitor progress through 7-phase workflow

### üîç Intelligent Search

- **Query Expansion**: Generate multiple search variations for comprehensive coverage
  - Control expansion with `--max-queries` (1 = no expansion)
  - Provide custom queries with `--custom-queries`
  - Interactive query input with `--interactive-queries`
- Uses LLMs to find high-quality, relevant URLs
- Manual URL support via `--urls-file`
- Focuses on authoritative sources
- Mixes different content types

### üéØ URL Review & Prioritization

This is where m1f-research shines compared to automated LLM research:

- **Interactive Review Interface**: See all discovered URLs before scraping
- **Quality Control**: Remove spam, paywalled, or low-quality sources
- **Add Custom URLs**: Include your trusted sources or internal documentation
- **Prioritize Sources**: Decide which sources are most important
- **Skip if Needed**: Use `--skip-review` for fully automated research

### üåê Advanced Crawling

- **Deep Crawling**: Follow links to specified depth levels
- **Intelligent Filtering**: Domain-based and pattern-based URL filtering
- **Site Limits**: Control maximum pages per domain

### üì• Smart Scraping

- **Per-host delays**: Only after 3+ requests to same host
- Concurrent scraping across different hosts
- Automatic HTML to Markdown conversion
- Content deduplication via checksums

### üß† Content Analysis

- **Analysis Generator**: AI-powered insights and summaries
- Relevance scoring (0-10 scale)
- Key points extraction
- Content summarization
- Duplicate detection
- Template-based analysis approaches

### üì¶ Organized Output

- **Hierarchical structure**: YYYY/MM/DD/job_id/
- Prominent bundle files (research_bundle.md)
- Clean Markdown output
- Symlink to latest research
- Phase-specific output organization

## Integration with Claude Projects

The research bundles created by m1f-research are perfect for Claude's Project Knowledge feature:

1. **Run Research**:
   ```bash
   m1f-research "your research topic"
   ```

2. **Find Your Bundle**:
   - Look in `./research-data/latest_research.md` (symlink to latest)
   - Or navigate to `./research-data/YYYY/MM/DD/job_id/research_bundle.md`

3. **Upload to Claude**:
   - Open Claude.ai or Claude desktop app
   - Create or open a project
   - Add the `research_bundle.md` to Project Knowledge
   - Claude now has deep knowledge about your research topic!

4. **Benefits**:
   - Claude can reference specific sources from your research
   - Knowledge persists across conversations in the project
   - You know exactly what information Claude is using
   - Can update research and refresh Claude's knowledge anytime

## Usage Examples

### Basic Research

```bash
# Research a programming topic
m1f-research "golang error handling"

# Output saved to: ./research-data/golang-error-handling-20240120-143022/
# Ready for upload to Claude Projects!
```

### Advanced Options

```bash
# Specify output location and name
m1f-research "kubernetes security" \
  --output ./research \
  --name k8s-security

# Use a specific template
m1f-research "react hooks" --template technical

# Skip analysis for faster results
m1f-research "python asyncio" --no-analysis

# Dry run to see what would happen
m1f-research "rust ownership" --dry-run
```

### Configuration File

```bash
# Use a custom configuration
m1f-research "database optimization" --config research.yml
```

## Configuration

### Key Command Line Options

| Option             | Description              | Default |
| ------------------ | ------------------------ | ------- |
| **Research**       |                          |         |
| `--urls`           | Number of URLs to find   | 20      |
| `--scrape`         | Number of URLs to scrape | 10      |
| `--urls-file`      | File with manual URLs    | None    |
| **Job Management** |                          |         |
| `--resume`         | Resume job by ID         | None    |
| `--list-jobs`      | List all jobs            | False   |
| `--status`         | Show job details         | None    |
| **Filtering**      |                          |         |
| `--search`         | Search jobs by query     | None    |
| `--date`           | Filter by date           | None    |
| `--limit`          | Pagination limit         | None    |
| `--offset`         | Pagination offset        | 0       |
| **Cleanup**        |                          |         |
| `--clean-raw`      | Clean job raw data       | None    |
| `--clean-all-raw`  | Clean all raw data       | False   |

See [63_cli_reference.md](63_cli_reference.md) for complete option list.

### Configuration File (.m1f.config.yml)

```yaml
research:
  # LLM settings
  llm:
    provider: claude
    model: claude-3-opus-20240229
    temperature: 0.7

  # Default counts
  defaults:
    url_count: 30
    scrape_count: 15

  # Workflow configuration
  workflow:
    expand_queries: true        # Generate search query variations
    max_queries: 5             # Maximum expanded queries
    skip_review: false         # Skip URL review interface
    crawl_depth: 0             # How many levels deep to crawl
    max_pages_per_site: 10     # Maximum pages per domain
    follow_external: false     # Follow external links
    generate_analysis: true    # Generate AI analysis
    analysis_type: "summary"   # Type of analysis to generate

  # Scraping behavior
  scraping:
    timeout_range: "1-3"
    max_concurrent: 5
    retry_attempts: 2

  # Content analysis
  analysis:
    relevance_threshold: 7.0
    min_content_length: 100
    prefer_code_examples: true

  # Output settings
  output:
    directory: ./research-data
    create_summary: true
    create_index: true

  # Research templates
  templates:
    technical:
      description: "Technical documentation and code"
      url_count: 30
      analysis_focus: implementation

    academic:
      description: "Academic papers and theory"
      url_count: 20
      analysis_focus: theory
```

## Templates

Pre-configured templates optimize research for different needs:

### technical

- Focuses on implementation details
- Prioritizes code examples
- Higher URL count for comprehensive coverage

### academic

- Emphasizes theoretical content
- Looks for citations and references
- Filters for authoritative sources

### tutorial

- Searches for step-by-step guides
- Prioritizes beginner-friendly content
- Includes examples and exercises

### general (default)

- Balanced approach
- Mixes different content types
- Suitable for most topics

## Output Structure

Research data uses hierarchical date-based organization:

```
./research-data/
‚îú‚îÄ‚îÄ research_jobs.db              # Main job database
‚îú‚îÄ‚îÄ latest_research.md           # Symlink to latest bundle
‚îî‚îÄ‚îÄ 2025/
    ‚îî‚îÄ‚îÄ 07/
        ‚îî‚îÄ‚îÄ 23/
            ‚îî‚îÄ‚îÄ abc123_topic-name/
                ‚îú‚îÄ‚îÄ research.db           # Job-specific database
                ‚îú‚îÄ‚îÄ research_bundle.md     # Main bundle
                ‚îú‚îÄ‚îÄ research_summary.md    # Summary
                ‚îú‚îÄ‚îÄ metadata.json         # Job metadata
                ‚îî‚îÄ‚îÄ search_results.json   # Found URLs
```

### Bundle Format

```markdown
# Research: [Your Topic]

Generated on: 2024-01-20 14:30:22 Total sources: 10

---

## Table of Contents

1. [Source Title 1](#1-source-title-1)
2. [Source Title 2](#2-source-title-2) ...

---

## Summary

[Research summary and top sources]

---

## 1. Source Title

**Source:** https://example.com/article **Relevance:** 8.5/10

### Key Points:

- Important point 1
- Important point 2

### Content:

[Full content in Markdown]

---
```

## Providers

### Claude (Anthropic)

- Default provider
- Best for: Comprehensive research, nuanced analysis
- Set: `ANTHROPIC_API_KEY`

### Gemini (Google)

- Fast and efficient
- Best for: Quick research, technical topics
- Set: `GOOGLE_API_KEY`

### CLI Tools

- Use local tools like `gemini-cli`
- Best for: Privacy, offline capability
- Example: `--provider gemini-cli`

## Tips

1. **Start broad, then narrow**: Use more URLs initially, let analysis filter
2. **Use templates**: Match template to your research goal
3. **Interactive mode**: Great for exploratory research
4. **Combine with m1f**: Feed research bundles into m1f for AI analysis

## Troubleshooting

### No API Key

```
Error: API key not set for ClaudeProvider
```

Solution: Set environment variable or pass in config

### Rate Limiting

```
Error: 429 Too Many Requests
```

Solution: Reduce `--concurrent` or increase timeout range

### Low Quality Results

- Increase `--urls` for more options
- Adjust `relevance_threshold` in config
- Try different `--template`

## Documentation

- [62_job_management.md](62_job_management.md) - Job persistence and filtering
  guide
- [63_cli_reference.md](63_cli_reference.md) - Complete CLI reference
- [64_api_reference.md](64_api_reference.md) - Developer API documentation
- [65_architecture.md](65_architecture.md) - Technical architecture details
- [66_examples.md](66_examples.md) - Real-world usage examples

## Future Features

- Multi-source research (GitHub, arXiv, YouTube)
- Knowledge graph building
- Research collaboration
- Export to various formats
- Job tagging system

## Contributing

m1f-research is part of the m1f project. Contributions welcome!

- Report issues: [GitHub Issues](https://github.com/m1f/m1f/issues)
- Submit PRs: Follow m1f contribution guidelines
- Request features: Open a discussion

======= 62_job_management.md ======
# Job Management in m1f-research

m1f-research uses a SQLite-based job management system that tracks all research
tasks, enabling persistence, resume functionality, and advanced filtering.

## Overview

Every research task creates a **job** with:

- Unique job ID
- SQLite databases for tracking
- Hierarchical directory structure
- Full resume capability
- Advanced search and filtering

## Job Structure

### Directory Layout

```
research-data/
‚îú‚îÄ‚îÄ research_jobs.db          # Main job tracking database
‚îú‚îÄ‚îÄ latest_research.md        # Symlink to most recent bundle
‚îî‚îÄ‚îÄ 2025/
    ‚îî‚îÄ‚îÄ 07/
        ‚îî‚îÄ‚îÄ 23/
            ‚îî‚îÄ‚îÄ abc123_query-name/
                ‚îú‚îÄ‚îÄ research.db           # Job-specific database
                ‚îú‚îÄ‚îÄ research_bundle.md    # Main research bundle
                ‚îú‚îÄ‚îÄ research_summary.md   # Research summary
                ‚îî‚îÄ‚îÄ metadata.json         # Job metadata
```

### Database Architecture

**Main Database (`research_jobs.db`)**

- Tracks all jobs across the system
- Stores job metadata and statistics
- Enables cross-job queries and filtering

**Per-Job Database (`research.db`)**

- URL tracking and status
- Content storage (markdown)
- Analysis results
- Filtering decisions

## Job Management Commands

### Listing Jobs

```bash
# List all jobs
m1f-research --list-jobs

# List with pagination
m1f-research --list-jobs --limit 10 --offset 20

# Filter by date
m1f-research --list-jobs --date 2025-07-23  # Specific day
m1f-research --list-jobs --date 2025-07     # Specific month
m1f-research --list-jobs --date 2025        # Specific year

# Search by query term
m1f-research --list-jobs --search "react"
m1f-research --list-jobs --search "tailwind"

# Combine filters
m1f-research --list-jobs --date 2025-07 --search "python" --limit 5
```

### Viewing Job Details

```bash
# Show detailed job status
m1f-research --status abc123
```

Output includes:

- Job ID and query
- Creation/update timestamps
- URL statistics
- Bundle availability
- Output directory

### Resuming Jobs

```bash
# Resume an interrupted job
m1f-research --resume abc123

# Add more URLs to existing job
m1f-research --resume abc123 --urls-file additional-urls.txt
```

## Advanced Filtering

### Pagination

Use `--limit` and `--offset` for large job lists:

```bash
# First page (10 items)
m1f-research --list-jobs --limit 10

# Second page
m1f-research --list-jobs --limit 10 --offset 10

# Third page
m1f-research --list-jobs --limit 10 --offset 20
```

### Date Filtering

Filter jobs by creation date:

```bash
# Jobs from today
m1f-research --list-jobs --date 2025-07-23

# Jobs from this month
m1f-research --list-jobs --date 2025-07

# Jobs from this year
m1f-research --list-jobs --date 2025
```

### Search Filtering

Find jobs by query content:

```bash
# Find all React-related research
m1f-research --list-jobs --search "react"

# Case-insensitive search
m1f-research --list-jobs --search "PYTHON"
```

Search terms are highlighted in the output for easy identification.

## Data Cleanup

### Cleaning Individual Jobs

Remove raw HTML data while preserving analysis:

```bash
# Clean specific job
m1f-research --clean-raw abc123
```

This removes:

- Raw HTML files
- Temporary download data

This preserves:

- Markdown content
- Analysis results
- Research bundles
- Job metadata

### Bulk Cleanup

Clean all jobs at once:

```bash
# Clean all raw data (with confirmation)
m1f-research --clean-all-raw
```

**Warning**: This action cannot be undone. You'll be prompted to confirm.

## Job Deletion

### Deleting Individual Jobs

Completely remove a job including all data:

```bash
# Delete specific job (with confirmation)
m1f-research --delete abc123

# Delete without confirmation prompt
m1f-research --delete abc123 --yes
```

When deleting a job, the system will:

1. Show job details (query, status, creation date, output path)
2. Request confirmation (unless `--yes` is used)
3. Remove the job from the database
4. Delete the entire job directory and all its contents
5. Report success or any errors encountered

**What gets deleted:**

- Database entry in `research_jobs.db`
- Job statistics in the database
- Entire job directory including:
  - Job-specific database (`research.db`)
  - Research bundle (`research_bundle.md`)
  - Research summary (`research_summary.md`)
  - All scraped content and analysis
  - Any metadata files

### Bulk Job Deletion

Delete multiple jobs based on filters:

```bash
# Delete all failed jobs
m1f-research --delete-bulk --status-filter failed

# Delete jobs from a specific month
m1f-research --delete-bulk --date 2025-01

# Delete jobs matching a search term
m1f-research --delete-bulk --search "test"

# Combine filters
m1f-research --delete-bulk --status-filter failed --date 2025-01

# Skip confirmation (use with caution!)
m1f-research --delete-bulk --status-filter failed --yes
```

**Bulk deletion process:**

1. Lists all jobs matching the filters
2. Shows a preview of jobs to be deleted (first 10)
3. Requests confirmation with total count
4. Deletes each job with progress tracking
5. Reports success/failure statistics

### Safety Features

#### Confirmation Prompts

By default, all deletion operations require confirmation:

```
Job to delete: abc123
‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ
Query: python async best practices
Status: completed
Created: 2025-07-23 14:30:22
Output: research-data/2025/07/23/abc123_python-async-best-practices

‚ö†Ô∏è  Delete job abc123 and all its data? [y/N]:
```

#### Error Handling

The deletion system handles errors gracefully:

- **Partial failures**: If filesystem deletion fails but database deletion succeeds, both are reported
- **Permission errors**: Attempts recovery with fallback methods
- **Missing jobs**: Clear error messages for non-existent job IDs
- **Failed deletions**: Detailed error reporting for troubleshooting

### Deletion vs Cleanup

| Operation | Database Entry | Job Directory | Research Bundle | Raw HTML | Recovery |
|-----------|---------------|---------------|-----------------|----------|----------|
| Clean Raw | ‚úì Kept | ‚úì Kept | ‚úì Kept | ‚úó Deleted | Can resume |
| Delete Job | ‚úó Deleted | ‚úó Deleted | ‚úó Deleted | ‚úó Deleted | Not recoverable |

**When to use cleanup:**
- Free disk space while keeping research results
- Job is complete but you want to keep the analysis
- Temporary space management

**When to use deletion:**
- Remove failed or test jobs
- Clean up old research no longer needed
- Complete removal of sensitive or outdated data
- Permanent space recovery

## Job Status

Jobs can have the following statuses:

| Status      | Description               |
| ----------- | ------------------------- |
| `active`    | Job is currently running  |
| `completed` | Job finished successfully |
| `failed`    | Job encountered errors    |

## Manual URL Management

Add URLs from a file:

```bash
# Create URL file
cat > my-urls.txt << EOF
https://example.com/article1
https://example.com/article2
EOF

# Start new job with URLs
m1f-research "my topic" --urls-file my-urls.txt

# Add to existing job
m1f-research --resume abc123 --urls-file more-urls.txt
```

## Smart Delay Management

The scraper implements intelligent per-host delays:

- **No delay** for first 3 requests to any host
- **1-3 second random delay** after 3 requests
- **Parallel scraping** across different hosts

This ensures:

- Fast scraping for diverse sources
- Respectful behavior for repeated requests
- Optimal performance

## Examples

### Research Workflow

```bash
# 1. Start research
m1f-research "python async best practices"
# Output: Job ID: abc123

# 2. Check progress
m1f-research --status abc123

# 3. Add more URLs if needed
m1f-research --resume abc123 --urls-file extra-urls.txt

# 4. View all Python research
m1f-research --list-jobs --search "python"

# 5. Clean up old data
m1f-research --clean-raw abc123

# 6. Delete old or failed jobs
m1f-research --delete old-job-id
```

### Monthly Research Review

```bash
# List all research from July 2025
m1f-research --list-jobs --date 2025-07

# Page through results
m1f-research --list-jobs --date 2025-07 --limit 20
m1f-research --list-jobs --date 2025-07 --limit 20 --offset 20

# Find specific topic
m1f-research --list-jobs --date 2025-07 --search "react hooks"
```

### Disk Space Management

```bash
# Check job sizes (future feature)
# m1f-research --list-jobs --show-size

# Clean raw data from specific job
m1f-research --clean-raw old-job-id

# Bulk cleanup of raw data
m1f-research --clean-all-raw

# Delete specific job completely
m1f-research --delete old-job-id

# Delete all failed jobs
m1f-research --delete-bulk --status-filter failed

# Delete old jobs from previous month
m1f-research --delete-bulk --date 2025-06
```

### Job Maintenance Workflow

```bash
# 1. List all jobs to review
m1f-research --list-jobs

# 2. Check failed jobs
m1f-research --list-jobs --status-filter failed

# 3. Delete all failed jobs after review
m1f-research --delete-bulk --status-filter failed

# 4. Clean raw data from completed jobs
m1f-research --clean-all-raw

# 5. Delete old test jobs
m1f-research --delete-bulk --search "test" --yes
```

## Tips

1. **Use job IDs**: Save job IDs for easy resume/reference
2. **Regular cleanup**: Clean raw data after analysis is complete
3. **Combine filters**: Use multiple filters for precise searches
4. **Manual URLs**: Supplement LLM search with your own URLs
5. **Check status**: Monitor long-running jobs with --status
6. **Review before deletion**: Always check job details before deleting
7. **Use --yes carefully**: Only use automatic confirmation in scripts you trust
8. **Delete failed jobs**: Regularly remove failed jobs to keep workspace clean
9. **Backup important research**: Export important jobs before deletion

## Future Enhancements

- Export job lists to CSV/JSON
- Job size statistics
- Automatic cleanup policies
- Job tagging system
- Cross-job deduplication
- Job templates
- Scheduled deletion policies
- Job archiving before deletion
- Undo/recovery for recently deleted jobs

======= 63_cli_reference.md ======
# m1f-research CLI Reference

Complete command-line interface reference for m1f-research.

## Synopsis

```bash
m1f-research [QUERY] [OPTIONS]
```

## Positional Arguments

### `query`

Research topic or query (required for new jobs, optional when using job
management commands)

## Options

### General Options

| Option            | Short | Description                            | Default |
| ----------------- | ----- | -------------------------------------- | ------- |
| `--help`          | `-h`  | Show help message and exit             | -       |
| `--version`       |       | Show program version                   | -       |
| `--verbose`       | `-v`  | Increase verbosity (use -vv for debug) | Warning |
| `--quiet`         | `-q`  | Suppress non-error output              | False   |
| `--dry-run`       |       | Preview without executing              | False   |
| `--yes`           | `-y`  | Answer yes to all prompts              | False   |
| `--config CONFIG` | `-c`  | Path to configuration file             | None    |

### Output Options

| Option                  | Short | Description                  | Default |
| ----------------------- | ----- | ---------------------------- | ------- |
| `--format {text,json}`  |       | Output format                | text    |
| `--no-color`            |       | Disable colored output       | False   |

### Extended Help

| Option             | Description                     |
| ------------------ | ------------------------------- |
| `--help-examples`  | Show extended usage examples    |
| `--help-filters`   | Show filtering guide            |
| `--help-providers` | Show LLM provider setup guide   |

### LLM Provider Options

| Option                | Short | Description                                                          | Default          |
| --------------------- | ----- | -------------------------------------------------------------------- | ---------------- |
| `--provider PROVIDER` | `-p`  | LLM provider: claude, claude-cli, gemini, gemini-cli, openai         | claude           |
| `--model MODEL`       | `-m`  | Specific model to use                                                | Provider default |
| `--template TEMPLATE` | `-t`  | Analysis template: general, technical, academic, tutorial, reference | general          |

### Research Options

| Option           | Short | Description                        | Default |
| ---------------- | ----- | ---------------------------------- | ------- |
| `--urls N`       |       | Number of URLs to search for       | 20      |
| `--scrape N`     |       | Maximum URLs to scrape             | 10      |
| `--concurrent N` |       | Max concurrent scraping operations | 5       |
| `--no-filter`    |       | Disable content filtering          | False   |
| `--no-analysis`  |       | Skip AI analysis (just scrape)     | False   |
| `--interactive`  | `-i`  | Start in interactive mode          | False   |

### Query Control Options

| Option                    | Description                              | Default |
| ------------------------- | ---------------------------------------- | ------- |
| `--max-queries N`         | Maximum number of query variations (1 = original only) | 5       |
| `--custom-queries Q1 Q2...` | Provide custom query variations (overrides auto-expansion) | None    |
| `--interactive-queries`   | Interactively enter custom query variations | False   |

### Workflow Options

| Option                    | Description                              | Default |
| ------------------------- | ---------------------------------------- | ------- |
| `--expand-queries`        | Enable query expansion phase             | True    |
| `--skip-review`           | Skip URL review phase                    | False   |
| `--crawl-depth N`         | How many levels deep to crawl links     | 0       |
| `--max-pages-per-site N`  | Maximum pages to crawl per domain       | 10      |
| `--follow-external`       | Follow external links during crawling   | False   |
| `--analysis-type TYPE`    | Type of analysis: summary, detailed     | summary |

### Output Options

| Option         | Short | Description                     | Default         |
| -------------- | ----- | ------------------------------- | --------------- |
| `--output DIR` | `-o`  | Output directory                | ./research-data |
| `--name NAME`  | `-n`  | Custom name for research bundle | Auto-generated  |

### Job Management

| Option             | Description                                |
| ------------------ | ------------------------------------------ |
| `--resume JOB_ID`  | Resume an existing research job            |
| `--list-jobs`      | List all research jobs                     |
| `--status JOB_ID`  | Show detailed status of a specific job     |
| `--watch JOB_ID`   | Watch job progress in real-time            |
| `--urls-file FILE` | File containing URLs to add (one per line) |

### List Filtering Options

| Option                                            | Description                 | Example             |
| ------------------------------------------------- | --------------------------- | ------------------- |
| `--limit N`                                       | Limit number of results     | `--limit 10`        |
| `--offset N`                                      | Skip N results (pagination) | `--offset 20`       |
| `--date DATE`                                     | Filter by date              | `--date 2025-07-23` |
| `--search TERM`                                   | Search jobs by query term   | `--search "react"`  |
| `--status-filter {active,completed,failed}`      | Filter by job status        | `--status-filter completed` |

### Data Management Options

| Option               | Description                          |
| -------------------- | ------------------------------------ |
| `--clean-raw JOB_ID` | Clean raw HTML data for specific job |
| `--clean-all-raw`    | Clean raw HTML data for all jobs     |
| `--export JOB_ID`    | Export job data to JSON              |
| `--delete JOB_ID`    | Delete a research job completely     |
| `--delete-bulk`      | Delete multiple jobs based on filters |

## Command Examples

### Basic Research

```bash
# Simple research
m1f-research "python async programming"

# With custom settings
m1f-research "react hooks" --urls 30 --scrape 15

# Using different provider
m1f-research "machine learning" --provider gemini --model gemini-1.5-pro

# Skip analysis for faster results
m1f-research "tailwind css" --no-analysis

# Custom output location
m1f-research "rust ownership" --output ~/research --name rust-guide
```

### Job Management

```bash
# List all jobs
m1f-research --list-jobs

# List with pagination
m1f-research --list-jobs --limit 10 --offset 0

# Filter by date
m1f-research --list-jobs --date 2025-07-23  # Specific day
m1f-research --list-jobs --date 2025-07     # Month
m1f-research --list-jobs --date 2025        # Year

# Search for jobs
m1f-research --list-jobs --search "python"

# Combined filters
m1f-research --list-jobs --date 2025-07 --search "async" --limit 5

# Check job status
m1f-research --status abc123

# Resume a job
m1f-research --resume abc123

# Add URLs to existing job
m1f-research --resume abc123 --urls-file additional-urls.txt
```

### Manual URL Management

```bash
# Start with manual URLs only
m1f-research "my topic" --urls 0 --urls-file my-urls.txt

# Combine LLM search with manual URLs
m1f-research "my topic" --urls 20 --urls-file my-urls.txt

# Add URLs to existing job
m1f-research --resume abc123 --urls-file more-urls.txt
```

### Data Cleanup

```bash
# Clean specific job
m1f-research --clean-raw abc123

# Clean all jobs (with confirmation)
m1f-research --clean-all-raw
```

### Job Deletion

```bash
# Delete specific job (with confirmation)
m1f-research --delete abc123

# Delete without confirmation
m1f-research --delete abc123 --yes

# Delete all failed jobs
m1f-research --delete-bulk --status-filter failed

# Delete jobs from specific month
m1f-research --delete-bulk --date 2025-06

# Delete jobs matching search term
m1f-research --delete-bulk --search "test"

# Combine filters for targeted deletion
m1f-research --delete-bulk --status-filter failed --date 2025-07

# Force deletion without confirmation (use carefully!)
m1f-research --delete-bulk --status-filter failed --yes
```

### Advanced Workflows

```bash
# Dry run to preview
m1f-research "test query" --dry-run

# Very verbose output for debugging
m1f-research "test query" -vv

# Interactive mode
m1f-research --interactive

# Technical analysis with high URL count
m1f-research "kubernetes networking" --template technical --urls 50 --scrape 25

# Deep crawling with query expansion
m1f-research "microservices patterns" --expand-queries --crawl-depth 2 --max-pages-per-site 15

# Skip review for automated workflows
m1f-research "python async patterns" --skip-review --max-queries 8

# Detailed analysis with external link following
m1f-research "AI research 2024" --follow-external --analysis-type detailed

# Control query expansion precisely
m1f-research "rust ownership" --max-queries 1  # No expansion, original only
m1f-research "golang channels" --max-queries 10  # More variations

# Provide custom query variations
m1f-research "react performance" --custom-queries \
  "react performance optimization 2025" \
  "react memo and useMemo patterns" \
  "react virtual DOM optimization"

# Interactive query input for careful research
m1f-research "database indexing" --interactive-queries
```

## Date Format Examples

The `--date` filter supports multiple formats:

| Format | Example      | Matches               |
| ------ | ------------ | --------------------- |
| Y-M-D  | `2025-07-23` | Specific day          |
| Y-M    | `2025-07`    | All jobs in July 2025 |
| Y      | `2025`       | All jobs in 2025      |

## Exit Codes

| Code | Meaning                      |
| ---- | ---------------------------- |
| 0    | Success                      |
| 1    | General error                |
| 130  | Interrupted by user (Ctrl+C) |

## Environment Variables

| Variable            | Description        |
| ------------------- | ------------------ |
| `ANTHROPIC_API_KEY` | API key for Claude |
| `GOOGLE_API_KEY`    | API key for Gemini |
| `OPENAI_API_KEY`    | API key for OpenAI |

## Configuration File

Create `.m1f.config.yml` for persistent settings:

```yaml
research:
  llm:
    provider: claude
    model: claude-3-opus-20240229

  defaults:
    url_count: 30
    scrape_count: 15

  # Workflow phase configuration
  workflow:
    expand_queries: true        # Generate search query variations
    max_queries: 5             # Maximum expanded queries
    skip_review: false         # Skip URL review interface
    crawl_depth: 1             # How many levels deep to crawl
    max_pages_per_site: 10     # Maximum pages per domain
    follow_external: false     # Follow external links
    generate_analysis: true    # Generate AI analysis
    analysis_type: "summary"   # Type of analysis to generate

  output:
    directory: ~/research-data
```

## Tips

1. **Save Job IDs**: Copy job IDs for easy resume/reference
2. **Use Filters**: Combine date and search for precise results
3. **Pagination**: Use limit/offset for large job lists
4. **Cleanup**: Regularly clean raw data to save space
5. **Manual URLs**: Supplement with your own curated links
6. **Delete Failed Jobs**: Remove failed jobs regularly to keep workspace clean
7. **Confirm Deletions**: Always review job details before deleting
8. **Use --yes Carefully**: Only skip confirmations in trusted scripts

======= 64_api_reference.md ======
# m1f-research API Reference

## Command Line Interface

### Basic Usage

```bash
m1f-research [OPTIONS] <query>
```

### Options

#### Search Options

- `--urls <count>` - Number of URLs to find (default: 20)
- `--scrape <count>` - Number of URLs to scrape (default: 10)
- `--template <name>` - Research template to use (default: general)

#### LLM Options

- `--provider <name>` - LLM provider (claude, gemini, cli)
- `--model <name>` - Specific model to use
- `--temperature <float>` - LLM temperature (0.0-1.0)

#### Output Options

- `--output <dir>` - Output directory (default: ./m1f/research)
- `--name <name>` - Bundle name (default: auto-generated)
- `--format <format>` - Output format (markdown, json)

#### Processing Options

- `--concurrent <count>` - Max concurrent scrapes (default: 5)
- `--timeout <range>` - Timeout range in seconds (e.g., "1-3")
- `--no-filter` - Disable URL filtering
- `--no-analysis` - Skip content analysis
- `--no-summary` - Skip summary generation

#### Other Options

- `--config <file>` - Custom config file
- `--interactive` - Interactive mode
- `--dry-run` - Preview without execution
- `--verbose` - Verbose output
- `--quiet` - Minimal output

## Python API

### Basic Usage

```python
from tools.research import ResearchOrchestrator
from tools.shared.colors import info, success

# Create orchestrator
orchestrator = ResearchOrchestrator()

# Run research
results = await orchestrator.research(
    query="microservices best practices",
    url_count=30,
    scrape_count=15
)

# Access results
info(f"Found {len(results.urls)} URLs")
info(f"Scraped {len(results.content)} pages")
success(f"Bundle saved to: {results.bundle_path}")
```

### Configuration

```python
from tools.research import ResearchConfig

config = ResearchConfig(
    llm_provider="claude",
    model="claude-3-opus-20240229",
    temperature=0.7,
    url_count=30,
    scrape_count=15,
    output_dir="./research",
    concurrent_limit=5,
    timeout_range=(1, 3)
)

orchestrator = ResearchOrchestrator(config)
```

### Custom Templates

```python
from tools.research import ResearchTemplate

template = ResearchTemplate(
    name="custom",
    description="Custom research template",
    search_prompt="Find {query} focusing on...",
    analysis_focus="implementation details",
    relevance_criteria="practical examples"
)

results = await orchestrator.research(
    query="react hooks",
    template=template
)
```

### Providers

```python
from tools.research import ClaudeProvider, GeminiProvider

# Claude provider
claude = ClaudeProvider(
    api_key="your-api-key",
    model="claude-3-opus-20240229"
)

# Gemini provider
gemini = GeminiProvider(
    api_key="your-api-key",
    model="gemini-pro"
)

# Use custom provider
orchestrator = ResearchOrchestrator(
    config=config,
    llm_provider=claude
)
```

### Scraping

```python
from tools.research import Scraper

scraper = Scraper(
    concurrent_limit=5,
    timeout_range=(1, 3),
    retry_attempts=2
)

# Scrape single URL
content = await scraper.scrape_url("https://example.com")

# Scrape multiple URLs
urls = ["https://example1.com", "https://example2.com"]
results = await scraper.scrape_urls(urls)
```

### Analysis

```python
from tools.research import Analyzer
from tools.shared.colors import info

analyzer = Analyzer(llm_provider=claude)

# Analyze content
analysis = await analyzer.analyze_content(
    content="Article content...",
    query="microservices",
    template="technical"
)

info(f"Relevance: {analysis.relevance}/10")
info(f"Key points: {analysis.key_points}")
```

### Bundle Creation

```python
from tools.research import BundleCreator

creator = BundleCreator()

# Create bundle
bundle_path = await creator.create_bundle(
    query="microservices",
    scraped_content=results,
    analysis_results=analyses,
    output_dir="./research"
)
```

## Data Models

### ResearchResult

```python
@dataclass
class ResearchResult:
    query: str
    urls: List[str]
    content: List[ScrapedContent]
    analyses: List[ContentAnalysis]
    bundle_path: Path
    metadata: Dict[str, Any]
```

### ScrapedContent

```python
@dataclass
class ScrapedContent:
    url: str
    title: str
    content: str
    scraped_at: datetime
    success: bool
    error: Optional[str]
```

### ContentAnalysis

```python
@dataclass
class ContentAnalysis:
    url: str
    relevance: float
    key_points: List[str]
    summary: str
    metadata: Dict[str, Any]
```

## Error Handling

```python
from tools.research import ResearchError, ScrapingError, AnalysisError
from tools.shared.colors import error

try:
    results = await orchestrator.research("query")
except ResearchError as e:
    error(f"Research failed: {e}")
except ScrapingError as e:
    error(f"Scraping failed: {e}")
except AnalysisError as e:
    error(f"Analysis failed: {e}")
```

## Events and Callbacks

```python
from tools.shared.colors import info, error

# Progress callback
def on_progress(stage: str, current: int, total: int):
    info(f"{stage}: {current}/{total}")

# Error callback
def on_error(error: Exception, context: Dict):
    error(f"Error in {context['stage']}: {error}")

# Use callbacks
results = await orchestrator.research(
    query="microservices",
    on_progress=on_progress,
    on_error=on_error
)
```

## Advanced Usage

### Custom URL Filters

```python
def custom_filter(url: str) -> bool:
    # Only allow specific domains
    allowed = ["docs.python.org", "realpython.com"]
    return any(domain in url for domain in allowed)

orchestrator.add_url_filter(custom_filter)
```

### Content Processors

```python
def process_content(content: str) -> str:
    # Custom content processing
    return content.replace("old_term", "new_term")

orchestrator.add_content_processor(process_content)
```

### Result Transformers

```python
def transform_results(results: ResearchResult) -> Dict:
    # Custom result transformation
    return {
        "query": results.query,
        "sources": len(results.content),
        "top_relevance": max(a.relevance for a in results.analyses)
    }

transformed = transform_results(results)
```

======= 65_architecture.md ======
# m1f-research Architecture

## Overview

The m1f-research tool is built on a modular architecture that combines web
search, content scraping, and AI-powered analysis to create comprehensive
research bundles.

## Core Components

### 1. Orchestrator (`orchestrator.py`)

- Central coordination of the research workflow
- Manages the 7-phase workflow system
- Handles configuration and state management
- Coordinates phase transitions and checkpoints

### 2. Workflow Manager (`workflow_phases.py`)

- Manages workflow phases and transitions
- Tracks phase completion and state
- Handles phase skipping based on configuration
- Provides resumption capabilities

### 3. Query Expander (`query_expander.py`)

- Generates multiple search query variations
- Uses LLM to create comprehensive query coverage
- Handles query expansion metadata
- Configurable expansion limits

### 4. URL Reviewer (`url_reviewer.py`)

- Interactive URL review and curation interface
- Allows human oversight of discovered URLs
- Supports batch operations and filtering
- Optional phase that can be skipped

### 5. Deep Crawler (`deep_crawler.py`)

- Intelligent multi-level web crawling
- Follows links to specified depth
- Respects domain limits and external link policies
- Integrates with URL filtering systems

### 6. LLM Interface (`llm_interface.py`)

- Abstraction layer for different LLM providers
- Supports Claude, Gemini, and CLI tools
- Manages API calls and response parsing

### 7. Scraper (`scraper.py`)

- Concurrent web scraping with rate limiting
- Integrates with html2md for content conversion
- Handles failures gracefully with retry logic

### 8. Analysis Generator (`analysis_generator.py`)

- Generates AI-powered insights and summaries
- Template-based analysis approaches
- Content relevance scoring and key points extraction
- Configurable analysis types

### 9. Bundle Creator (`bundle_creator.py`)

- Organizes scraped content into structured bundles
- Creates table of contents and summaries
- Formats output in clean Markdown
- Integrates phase-specific metadata

## Data Flow

### 7-Phase Workflow

```
User Query
    ‚Üì
[1] INITIALIZATION
    ‚Üì (Configuration & Setup)
Workflow Manager
    ‚Üì
[2] QUERY_EXPANSION (Optional)
    ‚Üì (Query Expander)
Expanded Queries
    ‚Üì
[3] URL_COLLECTION
    ‚Üì (LLM Search)
Discovered URLs
    ‚Üì
[4] URL_REVIEW (Optional)
    ‚Üì (URL Reviewer)
Curated URLs
    ‚Üì
[5] CRAWLING
    ‚Üì (Deep Crawler + Scraper)
Raw Content
    ‚Üì
[6] BUNDLING
    ‚Üì (Bundle Creator)
Research Bundle
    ‚Üì
[7] ANALYSIS (Optional)
    ‚Üì (Analysis Generator)
Final Research Package
```

### Phase Dependencies

- INITIALIZATION ‚Üí Required for all workflows
- QUERY_EXPANSION ‚Üí Can be skipped if `expand_queries: false`
- URL_COLLECTION ‚Üí Always required
- URL_REVIEW ‚Üí Can be skipped if `skip_review: true`
- CRAWLING ‚Üí Always required
- BUNDLING ‚Üí Always required
- ANALYSIS ‚Üí Can be skipped if `generate_analysis: false`

## Configuration System

The research tool uses a hierarchical configuration system:

1. **Default Config**: Built-in defaults
2. **User Config**: ~/.m1f.config.yml
3. **Project Config**: ./.m1f.config.yml
4. **CLI Arguments**: Command-line overrides

### Workflow Configuration

Workflow behavior is controlled by the `WorkflowConfig` class:

```python
@dataclass
class WorkflowConfig:
    expand_queries: bool = True        # Generate search query variations
    max_queries: int = 5              # Maximum expanded queries
    skip_review: bool = False         # Skip URL review interface
    crawl_depth: int = 0              # How many levels deep to crawl
    max_pages_per_site: int = 10      # Maximum pages per domain
    follow_external: bool = False     # Follow external links
    generate_analysis: bool = True    # Generate AI analysis
    analysis_type: str = "summary"    # Type of analysis to generate
```

## Templates

Templates customize the research process for different use cases:

- **Search Prompts**: How to find URLs
- **Analysis Focus**: What to extract
- **Output Format**: How to structure results

### Template Structure

```yaml
template_name:
  description: "Purpose of this template"
  search:
    focus: "What to look for"
    source_types: ["tutorial", "documentation", "discussion"]
  analysis:
    relevance_prompt: "Custom relevance criteria"
    key_points_prompt: "What to extract"
  output:
    structure: "How to organize results"
```

## Concurrency Model

The tool uses asyncio for efficient concurrent operations:

- **URL Search**: Sequential (LLM rate limits)
- **Web Scraping**: Concurrent with semaphore (default: 5)
- **Content Analysis**: Batch processing
- **Bundle Creation**: Sequential

## Error Handling

- **Graceful Degradation**: Failed scrapes don't stop the process
- **Retry Logic**: Automatic retries for transient failures
- **Fallback Providers**: Switch providers on API errors
- **Detailed Logging**: Track issues for debugging

## Security Considerations

- **URL Validation**: Prevent SSRF attacks
- **Content Sanitization**: Remove potentially harmful content
- **Rate Limiting**: Respect server resources
- **API Key Management**: Secure credential handling

## Extension Points

The architecture supports several extension mechanisms:

1. **Custom Providers**: Add new LLM providers
2. **Scraper Backends**: Integrate new scraping tools
3. **Analysis Templates**: Create domain-specific templates
4. **Output Formats**: Add new bundle formats
5. **Workflow Phases**: Add custom phases to the workflow
6. **Crawling Strategies**: Implement domain-specific crawling logic
7. **URL Filters**: Create custom URL filtering rules
8. **Analysis Generators**: Add specialized analysis types

## Performance Optimizations

- **Concurrent Scraping**: Parallel downloads
- **Streaming Processing**: Handle large content
- **Caching**: Avoid duplicate work
- **Lazy Loading**: Load components on demand

## Future Architecture Plans

- **Plugin System**: Dynamic loading of extensions
- **Distributed Scraping**: Scale across multiple machines
- **Knowledge Graph**: Build connections between research
- **Real-time Updates**: Monitor sources for changes

======= 66_examples.md ======
# m1f-research Examples

## Command Line Examples

### Basic Research

```bash
# Simple research on a topic
m1f-research "python async programming"

# Research with more sources
m1f-research "kubernetes networking" --urls 40 --scrape 20

# Use a specific template
m1f-research "react performance optimization" --template technical
```

### Different Providers

```bash
# Use Gemini instead of Claude
m1f-research "machine learning basics" --provider gemini

# Use a CLI tool
m1f-research "rust ownership" --provider gemini-cli

# Specify a particular model
m1f-research "quantum computing" --provider claude --model claude-3-opus-20240229
```

### Output Control

```bash
# Custom output location
m1f-research "microservices patterns" --output ~/research/microservices

# Named bundle
m1f-research "docker best practices" --name docker-guide

# JSON output format
m1f-research "api design" --format json
```

### Processing Options

```bash
# Faster scraping with more concurrency
m1f-research "golang concurrency" --concurrent 10

# Slower, more respectful scraping
m1f-research "web scraping ethics" --concurrent 2 --timeout "2-5"

# Skip analysis for raw content
m1f-research "css grid layouts" --no-analysis

# Skip filtering to get all URLs
m1f-research "obscure programming languages" --no-filter
```

### Interactive Mode

```bash
# Start interactive research session
m1f-research --interactive

# In interactive mode:
# > Enter research query: microservices vs monoliths
# > Number of URLs to find [20]: 30
# > Number to scrape [10]: 15
# > Template [general]: technical
# > Start research? [Y/n]: y
```

### Query Control Examples

```bash
# No query expansion - use only the original query
m1f-research "python type hints" --max-queries 1

# More query variations for comprehensive research
m1f-research "distributed systems" --max-queries 10

# Provide specific query variations
m1f-research "react hooks" --custom-queries \
  "react hooks best practices 2025" \
  "useState vs useReducer" \
  "custom hooks patterns" \
  "react hooks testing strategies"

# Interactive query input - enter variations manually
m1f-research "database optimization" --interactive-queries
# This will prompt:
# Original query: database optimization
# Enter custom query variations (one per line, empty line to finish):
# 1> database index optimization
# 2> query performance tuning
# 3> database normalization best practices
# 4> 

# Combine with other options
m1f-research "microservices" --custom-queries \
  "microservices architecture patterns" \
  "microservices vs monoliths 2025" \
  --urls 40 --scrape 20 --template technical
```

## Configuration File Examples

### Basic Configuration

```yaml
# .m1f.config.yml
research:
  llm:
    provider: claude
    temperature: 0.7

  defaults:
    url_count: 30
    scrape_count: 15

  output:
    directory: ./my-research
```

### Advanced Configuration

```yaml
# research-config.yml
research:
  llm:
    provider: gemini
    model: gemini-pro
    temperature: 0.8
    max_tokens: 4000

  defaults:
    url_count: 50
    scrape_count: 25

  scraping:
    timeout_range: "2-4"
    max_concurrent: 8
    retry_attempts: 3
    user_agent: "m1f-research/1.0"

  analysis:
    relevance_threshold: 7.5
    min_content_length: 200
    prefer_code_examples: true
    extract_metadata: true

  output:
    directory: ./research-output
    create_summary: true
    create_index: true
    include_metadata: true

  filters:
    allowed_domains:
      - "*.github.io"
      - "docs.*.com"
      - "*.readthedocs.io"
    blocked_domains:
      - "spam-site.com"
    url_patterns:
      - "*/api/*"
      - "*/reference/*"
```

### Template-Specific Config

```yaml
# technical-research.yml
research:
  templates:
    technical:
      description: "Deep technical documentation"
      url_count: 40
      scrape_count: 20

      search:
        focus: "implementation, architecture, code examples"
        prefer_sources:
          - "GitHub"
          - "official docs"
          - "technical blogs"

      analysis:
        relevance_prompt: |
          Rate based on:
          - Code examples
          - Technical depth
          - Practical applicability

        key_points_prompt: |
          Extract:
          - Core concepts
          - Implementation details
          - Best practices
          - Common pitfalls

      output:
        group_by: "subtopic"
        include_code_stats: true
```

## Workflow Examples

### Query Expansion Workflow

```bash
# Basic query expansion
m1f-research "python web frameworks" --expand-queries

# Advanced query expansion with more variations
m1f-research "machine learning model deployment" --expand-queries --max-queries 8

# Query expansion with specific focus
m1f-research "kubernetes best practices" --expand-queries --template technical
```

### URL Review Workflow

```bash
# Enable URL review for manual curation
m1f-research "AI ethics research" --skip-review false

# Automated workflow (skip review)
m1f-research "python libraries 2024" --skip-review

# Review with expanded queries
m1f-research "microservices patterns" --expand-queries --skip-review false
```

### Deep Crawling Workflow

```bash
# Single-level crawling
m1f-research "vue.js documentation" --crawl-depth 1

# Multi-level crawling with site limits
m1f-research "react ecosystem" --crawl-depth 2 --max-pages-per-site 15

# External link following
m1f-research "web development trends" --crawl-depth 1 --follow-external

# Comprehensive crawling
m1f-research "database optimization" --crawl-depth 3 --max-pages-per-site 25 --follow-external
```

### Analysis Generation Workflow

```bash
# Summary analysis (default)
m1f-research "golang best practices" --analysis-type summary

# Detailed analysis
m1f-research "system design principles" --analysis-type detailed

# Skip analysis for raw content
m1f-research "API documentation" --no-analysis
```

### Complete Workflow Examples

```bash
# Full-featured research with all phases
m1f-research "distributed systems patterns" \
  --expand-queries --max-queries 6 \
  --crawl-depth 2 --follow-external \
  --max-pages-per-site 20 \
  --analysis-type detailed \
  --skip-review false

# Minimal automated workflow
m1f-research "quick reference guide" \
  --no-expand-queries \
  --skip-review \
  --crawl-depth 0 \
  --analysis-type summary

# Academic research workflow
m1f-research "quantum computing research 2024" \
  --template academic \
  --expand-queries --max-queries 10 \
  --crawl-depth 1 \
  --analysis-type detailed \
  --skip-review false
```

## Python Script Examples

### Basic Research Script

```python
#!/usr/bin/env python3
import asyncio
from tools.research import ResearchOrchestrator
from tools.shared.colors import info, success

async def main():
    orchestrator = ResearchOrchestrator()

    results = await orchestrator.research(
        query="GraphQL best practices",
        url_count=30,
        scrape_count=15
    )

    success(f"Research complete!")
    info(f"Bundle saved to: {results.bundle_path}")
    info(f"Total sources: {len(results.content)}")
    info(f"Average relevance: {sum(a.relevance for a in results.analyses) / len(results.analyses):.1f}")

if __name__ == "__main__":
    asyncio.run(main())
```

### Custom Template Script with Workflow

```python
#!/usr/bin/env python3
import asyncio
from tools.research import ResearchOrchestrator, ResearchTemplate, ResearchConfig, WorkflowConfig
from tools.shared.colors import info

# Define custom template with workflow configuration
security_template = ResearchTemplate(
    name="security",
    description="Security-focused research",
    search_prompt="""
    Find authoritative sources about {query} focusing on:
    - Security vulnerabilities
    - Best practices for security
    - OWASP guidelines
    - Security tools and scanning
    """,
    analysis_focus="security implications",
    relevance_criteria="security relevance and actionable advice"
)

# Configure workflow for security research
workflow_config = WorkflowConfig(
    expand_queries=True,
    max_queries=8,
    skip_review=False,  # Review URLs for security relevance
    crawl_depth=2,
    max_pages_per_site=15,
    follow_external=True,
    generate_analysis=True,
    analysis_type="detailed"
)

async def main():
    config = ResearchConfig(
        query="API security",
        url_count=40,
        scrape_count=20,
        template="security",
        workflow=workflow_config
    )
    
    orchestrator = ResearchOrchestrator(config=config)
    orchestrator.register_template(security_template)

    results = await orchestrator.research_with_workflow()

    # Print security-specific summary
    info("\n=== Security Research Summary ===")
    info(f"Workflow completed with {len(results.content)} sources")
    info(f"Analysis type: {workflow_config.analysis_type}")
    
    for analysis in sorted(results.analyses, key=lambda a: a.relevance, reverse=True)[:5]:
        info(f"\n{analysis.url}")
        info(f"Relevance: {analysis.relevance}/10")
        info("Key Security Points:")
        for point in analysis.key_points[:3]:
            info(f"  - {point}")

if __name__ == "__main__":
    asyncio.run(main())
```

### Batch Research Script

```python
#!/usr/bin/env python3
import asyncio
from pathlib import Path
from tools.research import ResearchOrchestrator
from tools.shared.colors import info

async def research_topic(orchestrator, topic, output_dir):
    """Research a single topic"""
    info(f"\nResearching: {topic}")

    results = await orchestrator.research(
        query=topic,
        url_count=20,
        scrape_count=10,
        output_dir=output_dir / topic.replace(" ", "_")
    )

    return topic, results

async def main():
    topics = [
        "microservices architecture",
        "event-driven design",
        "domain-driven design",
        "CQRS pattern",
        "saga pattern"
    ]

    orchestrator = ResearchOrchestrator()
    output_dir = Path("./architecture-research")
    output_dir.mkdir(exist_ok=True)

    # Research all topics
    tasks = [research_topic(orchestrator, topic, output_dir) for topic in topics]
    results = await asyncio.gather(*tasks)

    # Create index
    with open(output_dir / "index.md", "w") as f:
        f.write("# Architecture Research\n\n")
        for topic, result in results:
            f.write(f"## {topic}\n")
            f.write(f"- Sources: {len(result.content)}\n")
            f.write(f"- Bundle: [{result.bundle_path.name}](./{result.bundle_path.relative_to(output_dir)})\n\n")

if __name__ == "__main__":
    asyncio.run(main())
```

### Advanced Workflow Pipeline Script

```python
#!/usr/bin/env python3
import asyncio
import json
from datetime import datetime
from tools.research import ResearchOrchestrator, ResearchConfig, WorkflowConfig
from tools.research.workflow_phases import WorkflowPhase
from tools.m1f import bundle_files
from tools.shared.colors import info, success, warning

async def research_with_workflow(query, workflow_config):
    """Research a topic using the 7-phase workflow"""

    info(f"Starting workflow research for: {query}")
    
    config = ResearchConfig(
        query=query,
        url_count=30,
        scrape_count=15,
        workflow=workflow_config
    )
    
    orchestrator = ResearchOrchestrator(config=config)
    
    # Monitor workflow phases
    phase_results = {}
    
    try:
        # Execute workflow with phase tracking
        research_results = await orchestrator.research_with_workflow()
        
        # Get workflow summary
        if hasattr(orchestrator, 'workflow_manager'):
            phase_summary = orchestrator.workflow_manager.get_phase_summary(research_results.job_id)
            phase_results = phase_summary
        
        info(f"Workflow completed successfully")
        info(f"Completed phases: {phase_results.get('completed_phases', [])}")
        
        return research_results, phase_results
        
    except Exception as e:
        warning(f"Workflow failed: {e}")
        return None, {"error": str(e)}

async def comprehensive_research_pipeline():
    """Run comprehensive research with different workflow configurations"""
    
    research_configs = [
        {
            "query": "microservices architecture patterns",
            "workflow": WorkflowConfig(
                expand_queries=True,
                max_queries=6,
                skip_review=False,
                crawl_depth=2,
                follow_external=True,
                generate_analysis=True,
                analysis_type="detailed"
            )
        },
        {
            "query": "python async programming",
            "workflow": WorkflowConfig(
                expand_queries=True,
                max_queries=4,
                skip_review=True,  # Automated
                crawl_depth=1,
                follow_external=False,
                generate_analysis=True,
                analysis_type="summary"
            )
        },
        {
            "query": "quick docker reference",
            "workflow": WorkflowConfig(
                expand_queries=False,  # Minimal
                skip_review=True,
                crawl_depth=0,
                generate_analysis=False
            )
        }
    ]
    
    results = []
    
    for config in research_configs:
        info(f"\n{'='*50}")
        info(f"Processing: {config['query']}")
        info(f"Workflow type: {'Comprehensive' if config['workflow'].expand_queries else 'Minimal'}")
        
        research_result, phase_result = await research_with_workflow(
            config['query'], 
            config['workflow']
        )
        
        if research_result:
            # Create detailed report
            report = {
                "query": config['query'],
                "timestamp": datetime.now().isoformat(),
                "workflow_config": {
                    "expand_queries": config['workflow'].expand_queries,
                    "crawl_depth": config['workflow'].crawl_depth,
                    "analysis_type": config['workflow'].analysis_type,
                },
                "phase_results": phase_result,
                "research_stats": {
                    "urls_found": len(research_result.urls) if research_result.urls else 0,
                    "urls_scraped": len(research_result.content) if research_result.content else 0,
                },
                "bundle_path": str(research_result.bundle_path) if research_result.bundle_path else None
            }
            
            results.append(report)
            success(f"Completed: {config['query']}")
        else:
            warning(f"Failed: {config['query']}")
    
    # Generate summary report
    summary_path = "./pipeline_summary.json"
    with open(summary_path, "w") as f:
        json.dump({
            "pipeline_run": datetime.now().isoformat(),
            "total_queries": len(research_configs),
            "successful": len(results),
            "results": results
        }, f, indent=2)
    
    info(f"\n=== Pipeline Summary ===")
    info(f"Total queries processed: {len(research_configs)}")
    info(f"Successful completions: {len(results)}")
    info(f"Summary saved to: {summary_path}")
    
    for result in results:
        info(f"\n{result['query']}:")
        info(f"  - Workflow: {result['workflow_config']}")
        info(f"  - Completed phases: {len(result['phase_results'].get('completed_phases', []))}")
        info(f"  - URLs processed: {result['research_stats']['urls_scraped']}")

if __name__ == "__main__":
    asyncio.run(comprehensive_research_pipeline())
```

## Real-World Use Cases

### 1. Technology Evaluation

```bash
# Research multiple competing technologies
m1f-research "kafka vs rabbitmq vs redis streams" --urls 50 --scrape 30 --template technical

# Deep dive into one technology
m1f-research "apache kafka internals architecture" --urls 40 --scrape 25
```

### 2. Learning New Topics

```bash
# Beginner-friendly research
m1f-research "python for beginners" --template tutorial

# Advanced topics with academic sources
m1f-research "distributed consensus algorithms" --template academic
```

### 3. Problem Solving

```bash
# Debug specific issues
m1f-research "kubernetes pod stuck terminating" --urls 30

# Find best practices
m1f-research "postgresql performance tuning large tables" --template technical
```

### 4. Documentation Collection

```bash
# Gather API documentation
m1f-research "stripe api payment intents" --template reference

# Collect migration guides
m1f-research "migrate django 3 to 4" --urls 40
```

### 5. Security Research

```bash
# Security audit preparation
m1f-research "OWASP top 10 2023 examples" --urls 50 --scrape 30

# Vulnerability research
m1f-research "log4j vulnerability explanation CVE-2021-44228"
```

## Tips and Tricks

### 1. Optimize for Quality

```bash
# More URLs, selective scraping
m1f-research "complex topic" --urls 60 --scrape 20

# This finds many options but only scrapes the best
```

### 2. Domain-Specific Research

```bash
# Create custom config for specific domains
cat > medical-research.yml << EOF
research:
  filters:
    allowed_domains:
      - "*.nih.gov"
      - "pubmed.ncbi.nlm.nih.gov"
      - "*.nature.com"
EOF

m1f-research "covid vaccine efficacy" --config medical-research.yml
```

### 3. Combine with Other Tools

```bash
# Research ‚Üí m1f bundle ‚Üí AI analysis
m1f-research "topic" --analysis-type detailed && \
m1f ./m1f/research/topic-*/ -o analysis.txt && \
cat analysis.txt | llm "Summarize the key findings"

# Multi-stage workflow research
m1f-research "react hooks patterns" --expand-queries --crawl-depth 1 && \
m1f-research --resume [job-id] --analysis-type detailed && \
m1f ./research-data/*/[job-id]/ -o react-hooks-complete.txt
```

### 4. Scheduled Research

```bash
# Daily research updates (cron job)
0 9 * * * /usr/local/bin/m1f-research "AI news today" --name "ai-news-$(date +%Y%m%d)"
```

### 5. Research Archive

```bash
# Organize research by date
m1f-research "topic" --output "./research/$(date +%Y)/$(date +%m)/$(date +%d)/"
```

======= 67_cli_improvements.md ======
# 67. CLI Improvements for m1f-research

## Overview

The m1f-research CLI has been enhanced with improved user experience features
including colored output, JSON format support, extended help system, and better
progress tracking.

## Key Improvements

### 1. Colored Output

- **Colorama integration** for cross-platform color support
- **Graceful fallback** when colorama is not available
- **Status indicators** with color coding:
  - ‚úÖ Green for success/completed
  - ‚ö†Ô∏è Yellow for warnings/active
  - ‚ùå Red for errors/failed
  - ‚ÑπÔ∏è Cyan for information
- **Formatted headers** with bold blue text
- **Progress bars** with real-time updates
- **Consistent with other m1f tools** using the same colorama pattern

### 2. Output Formats

```bash
# Default text output with colors
m1f-research --list-jobs

# JSON output for automation
m1f-research --list-jobs --format json

# Quiet mode (suppress non-error output)
m1f-research "query" --quiet

# Verbose mode for debugging
m1f-research "query" --verbose  # -v for info, -vv for debug
```

### 3. Extended Help System

```bash
# Standard help
m1f-research --help

# Extended examples
m1f-research --help-examples

# Filtering guide
m1f-research --help-filters

# Provider setup guide
m1f-research --help-providers
```

### 4. Enhanced Job Listing

```bash
# Pagination
m1f-research --list-jobs --limit 10 --offset 0

# Date filtering
m1f-research --list-jobs --date 2025-07-24  # Specific day
m1f-research --list-jobs --date 2025-07     # Specific month
m1f-research --list-jobs --date 2025        # Specific year

# Search filtering
m1f-research --list-jobs --search "python"

# Status filtering
m1f-research --list-jobs --status-filter completed

# Combined filters
m1f-research --list-jobs \
  --search "react" \
  --date 2025-07 \
  --status-filter completed \
  --limit 20
```

### 5. Progress Tracking

- **Real-time progress bars** for long operations
- **ETA calculation** for time estimates
- **Phase indicators**:
  - Searching for URLs
  - Scraping URLs
  - Analyzing content
- **Callbacks** integrated throughout the pipeline

### 6. Interactive Mode

```bash
# Start interactive mode
m1f-research --interactive

# Available commands:
research <query>     # Start new research
list                # List all jobs
status <job_id>     # Show job status
resume <job_id>     # Resume a job
help               # Show help
exit/quit          # Exit
```

### 7. Better Error Handling

- **Helpful error messages** with suggestions
- **Input validation** with clear feedback
- **Graceful handling** of interrupts (Ctrl+C)

## Implementation Details

### Output Formatter (`output.py`)

```python
class OutputFormatter:
    """Handles formatted output for m1f-research"""

    def __init__(self, format: str = 'text', verbose: int = 0, quiet: bool = False):
        self.format = format
        self.verbose = verbose
        self.quiet = quiet
```

Key methods:

- `success()`, `error()`, `warning()`, `info()` - Colored messages
- `table()` - Formatted tables with column alignment
- `progress()` - Progress bars with ETA
- `job_status()` - Formatted job information
- `confirm()` - User confirmation prompts

### Enhanced CLI (`cli.py`)

```python
class EnhancedResearchCommand:
    """Enhanced CLI with better user experience"""
```

Features:

- Argument validation
- Extended help generation
- Progress callback integration
- JSON/text output switching
- Interactive mode support

### Progress Tracking

Progress callbacks integrated at multiple levels:

- URL searching phase
- Web scraping phase
- Content analysis phase
- Bundle creation phase

## Usage Examples

### 1. Research with Progress

```bash
m1f-research "python async programming" --verbose
# Shows progress bars for each phase
```

### 2. Job Management

```bash
# List recent jobs with colors
m1f-research --list-jobs --limit 10

# Watch job progress
m1f-research --watch abc123

# Export job data
m1f-research --export abc123 > job-data.json
```

### 3. Automation

```bash
# Get completed jobs as JSON
jobs=$(m1f-research --list-jobs --status-filter completed --format json)

# Process each job
echo "$jobs" | jq -r '.[].job_id' | while read id; do
    m1f-research --export "$id" > "exports/$id.json"
done
```

### 4. Batch Operations

```bash
# Clean all raw data with confirmation
m1f-research --clean-all-raw

# Skip confirmation
m1f-research --clean-all-raw --yes
```

## Benefits

1. **Better User Experience**

   - Clear visual feedback
   - Progress tracking
   - Helpful error messages

2. **Automation Support**

   - JSON output format
   - Machine-readable responses
   - Scriptable interface

3. **Debugging Support**

   - Verbose logging levels
   - Detailed error traces
   - Dry-run mode

4. **Accessibility**
   - `--no-color` option for terminals without color support
   - Clear text alternatives for all visual elements
   - Consistent formatting

## Future Enhancements

1. **Terminal UI (TUI)**

   - Full-screen interface with panels
   - Real-time job monitoring dashboard
   - Interactive job management

2. **Additional Output Formats**

   - CSV export for job lists
   - YAML configuration export
   - HTML reports

3. **Advanced Filtering**

   - Regex support in search
   - Multiple status filters
   - Custom query builders

4. **Performance Metrics**
   - Timing information per phase
   - Resource usage tracking
   - Success rate statistics

======= 68_job_deletion_guide.md ======
# Job Deletion Guide for m1f-research

This guide covers the job deletion functionality in m1f-research, including best practices, safety features, and common workflows.

## Overview

Job deletion provides complete removal of research jobs from both the database and filesystem. Unlike cleanup operations that only remove raw data, deletion permanently removes all traces of a job.

## Why Delete Jobs?

- **Failed Jobs**: Remove jobs that encountered errors and cannot be resumed
- **Test Jobs**: Clean up experimental or test research runs
- **Outdated Research**: Remove old research that is no longer relevant
- **Space Management**: Recover disk space completely
- **Privacy**: Remove sensitive or confidential research data
- **Organization**: Keep your research workspace clean and manageable

## Deletion Commands

### Single Job Deletion

Delete a specific job by its ID:

```bash
m1f-research --delete JOB_ID
```

**Example:**
```bash
# Delete job with confirmation
m1f-research --delete abc123

# Skip confirmation (automation-friendly)
m1f-research --delete abc123 --yes
```

### Bulk Deletion

Delete multiple jobs based on filters:

```bash
m1f-research --delete-bulk [FILTERS]
```

**Available Filters:**
- `--status-filter {active,completed,failed}` - Filter by job status
- `--date DATE` - Filter by creation date (Y-M-D, Y-M, or Y format)
- `--search TERM` - Filter by search term in query
- `--yes` - Skip confirmation prompt

**Examples:**
```bash
# Delete all failed jobs
m1f-research --delete-bulk --status-filter failed

# Delete jobs from January 2025
m1f-research --delete-bulk --date 2025-01

# Delete test jobs
m1f-research --delete-bulk --search "test"

# Combine filters
m1f-research --delete-bulk --status-filter failed --date 2025-07

# Automated deletion (no confirmation)
m1f-research --delete-bulk --status-filter failed --yes
```

## What Gets Deleted?

When you delete a job, the following items are permanently removed:

### Database Entries
- Main job record in `research_jobs.db`
- Job statistics in the database
- All references and foreign key relationships

### Filesystem Data
- Entire job directory at `research-data/YYYY/MM/DD/job_id_query/`
- Job-specific database (`research.db`)
- Research bundle (`research_bundle.md`)
- Research summary (`research_summary.md`)
- All scraped HTML content
- All converted Markdown content
- Analysis results and metadata
- Any temporary or cache files

## Safety Features

### Confirmation Prompts

By default, all deletion operations show job details and require confirmation:

```
Job to delete: abc123
‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ
Query: python async best practices
Status: completed
Created: 2025-07-23 14:30:22
Output: research-data/2025/07/23/abc123_python-async-best-practices

‚ö†Ô∏è  Delete job abc123 and all its data? [y/N]:
```

For bulk deletion, you'll see a preview:

```
Jobs to delete (15 total)
‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ
1. [abc123] test query 1 (failed)
2. [def456] test query 2 (failed)
3. [ghi789] test query 3 (failed)
... and 12 more

‚ö†Ô∏è  Delete 15 jobs with filters: status=failed? [y/N]:
```

### Error Handling

The deletion system handles various error scenarios:

- **Non-existent jobs**: Clear error message if job ID not found
- **Permission errors**: Attempts recovery with fallback methods
- **Partial failures**: Reports both successful and failed operations
- **Database integrity**: Uses transactions to prevent corruption

### Progress Tracking

Bulk deletions show real-time progress:

```
Deleting 25 jobs...
[‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà] 100.0% Deletion complete
‚úÖ Successfully deleted 23 jobs
‚ùå Failed to delete 2 jobs
  - job123: Permission denied
  - job456: Directory not found
```

## Deletion vs Cleanup Comparison

| Aspect | Clean Raw (`--clean-raw`) | Delete Job (`--delete`) |
|--------|---------------------------|-------------------------|
| **Database entry** | ‚úÖ Preserved | ‚ùå Deleted |
| **Job directory** | ‚úÖ Preserved | ‚ùå Deleted |
| **Research bundle** | ‚úÖ Preserved | ‚ùå Deleted |
| **Analysis results** | ‚úÖ Preserved | ‚ùå Deleted |
| **Raw HTML** | ‚ùå Deleted | ‚ùå Deleted |
| **Can resume** | ‚úÖ Yes | ‚ùå No |
| **Recoverable** | ‚úÖ Partially | ‚ùå No |
| **Space saved** | üü° Moderate | üü¢ Maximum |

## Common Workflows

### Regular Maintenance

Clean up failed jobs weekly:

```bash
# Review failed jobs
m1f-research --list-jobs --status-filter failed

# Delete all failed jobs
m1f-research --delete-bulk --status-filter failed
```

### Monthly Cleanup

Remove old research monthly:

```bash
# List jobs from 2 months ago
m1f-research --list-jobs --date 2025-05

# Review and delete old jobs
m1f-research --delete-bulk --date 2025-05
```

### Project Cleanup

Clean up after a project:

```bash
# Find all project-related jobs
m1f-research --list-jobs --search "project-name"

# Delete project jobs
m1f-research --delete-bulk --search "project-name"
```

### Automated Cleanup Script

```bash
#!/bin/bash
# cleanup-research.sh

# Delete failed jobs older than 7 days
m1f-research --delete-bulk --status-filter failed --yes

# Clean raw data from completed jobs
m1f-research --clean-all-raw --yes

echo "Cleanup complete!"
```

## Best Practices

### Before Deleting

1. **Review job details**: Always check what you're about to delete
2. **Export important data**: Use `--export` to save job data if needed
3. **Check research bundles**: Ensure you have copies of important results
4. **Consider cleanup first**: Try `--clean-raw` if you only need space

### Safe Deletion

1. **Use filters carefully**: Test filters with `--list-jobs` first
2. **Start small**: Delete individual jobs before bulk operations
3. **Keep confirmations**: Only use `--yes` in trusted scripts
4. **Document deletions**: Keep a log of what was deleted and why

### Recovery Planning

Since deletion is permanent:

1. **Regular exports**: Export important jobs to JSON regularly
2. **Backup bundles**: Copy research bundles to a backup location
3. **Version control**: Consider committing important research to git
4. **Archive first**: Move old jobs to archive storage before deleting

## Troubleshooting

### Common Issues

**"Job not found" error:**
- Verify job ID with `--list-jobs`
- Check you're in the correct directory
- Ensure the job hasn't already been deleted

**"Permission denied" errors:**
- Check file ownership and permissions
- Run with appropriate user privileges
- Consider using `sudo` if appropriate

**Partial deletion failures:**
- Check disk space and permissions
- Review error messages for specific issues
- Manually remove remaining files if needed

### Recovery from Mistakes

Unfortunately, deletion is permanent. To minimize risk:

1. Always review before confirming
2. Test with dry runs when possible
3. Keep backups of important research
4. Use cleanup instead of deletion when unsure

## FAQ

**Q: Can I undo a deletion?**
A: No, deletion is permanent. Always confirm before deleting.

**Q: What's the difference between cleanup and deletion?**
A: Cleanup removes only raw HTML data. Deletion removes everything.

**Q: Can I delete active jobs?**
A: Yes, but it's not recommended. Stop the job first if possible.

**Q: How do I delete jobs older than X days?**
A: Use date filters, e.g., `--delete-bulk --date 2025-06` for June 2025.

**Q: Is there a way to archive before deleting?**
A: Use `--export` to save job data, then manually archive files before deletion.

**Q: Can I recover a deleted job from the database?**
A: No, deletion removes all database entries permanently.

## Command Reference

### Options
- `--delete JOB_ID` - Delete a specific job
- `--delete-bulk` - Delete multiple jobs with filters
- `--yes` - Skip confirmation prompts
- `--status-filter` - Filter by job status
- `--date` - Filter by creation date
- `--search` - Filter by query content

### Examples
```bash
# Delete specific job
m1f-research --delete abc123

# Delete with auto-confirmation
m1f-research --delete abc123 --yes

# Delete failed jobs
m1f-research --delete-bulk --status-filter failed

# Delete old jobs
m1f-research --delete-bulk --date 2025-01

# Delete test jobs
m1f-research --delete-bulk --search "test"

# Combined filters
m1f-research --delete-bulk --status-filter failed --date 2025-07 --yes
```

## Summary

Job deletion is a powerful feature for managing your research workspace. Use it wisely:

- ‚úÖ Delete failed and test jobs regularly
- ‚úÖ Review before confirming deletions
- ‚úÖ Use filters to target specific jobs
- ‚ö†Ô∏è Remember deletion is permanent
- ‚ö†Ô∏è Keep backups of important research
- ‚ö†Ô∏è Use `--yes` only in trusted automation

======= index.md ======
# Research Tool Documentation

The m1f research tool is an AI-powered research assistant that automatically
finds, scrapes, and bundles information on any topic.

## Documentation

- [README](./README.md) - Main documentation for the m1f-research tool
- [Architecture](./architecture.md) - Technical architecture and design
  decisions
- [API Reference](./api-reference.md) - Detailed API documentation
- [Examples](./examples.md) - Usage examples and recipes
- [Example Config](./example-config.yml) - Complete configuration example

## Quick Links

- **Getting Started**: See the [README](./README.md#quick-start)
- **Configuration**: See the [Configuration section](./README.md#configuration)
- **Templates**: See the [Templates section](./README.md#templates)
- **Troubleshooting**: See the
  [Troubleshooting section](./README.md#troubleshooting)

## Related Documentation

- [m1f Documentation](../01_m1f/) - Core bundler documentation
- [s1f Documentation](../02_s1f/) - File extraction tool
- [HTML2MD Documentation](../03_html2md/) - HTML to Markdown converter
- [Scraper Documentation](../04_scrape/) - Web scraping tools
- [Development Documentation](../05_development/) - Development guides and tools
