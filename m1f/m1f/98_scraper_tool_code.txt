======= path_utils.py ======
# Copyright 2025 Franz und Franz GmbH
# SPDX-License-Identifier: Apache-2.0

from pathlib import Path, PureWindowsPath


def convert_to_posix_path(path_val: str) -> str:
    """Convert a path string to POSIX style."""
    return PureWindowsPath(path_val).as_posix()


def normalize_path(path: Path | str) -> str:
    """Normalize a Path or path-like object to POSIX style."""
    return PureWindowsPath(str(path)).as_posix()

======= scrape.py ======
#!/usr/bin/env python3
# Copyright 2025 Franz und Franz GmbH
# SPDX-License-Identifier: Apache-2.0

"""Wrapper script for m1f-scrape module."""

import sys

# Try absolute imports first (for module execution), fall back to relative
try:
    from tools.scrape_tool.cli import main
except ImportError:
    # Fallback for direct script execution
    from scrape_tool.cli import main

if __name__ == "__main__":
    sys.exit(main())

======= scrape_tool/__init__.py ======
"""Web scraper tool for downloading websites."""

from .._version import __version__, __version_info__

======= scrape_tool/__main__.py ======
# Copyright 2025 Franz und Franz GmbH
# SPDX-License-Identifier: Apache-2.0

"""Entry point for m1f-scrape module."""

from .cli import main

if __name__ == "__main__":
    main()

======= scrape_tool/cli.py ======
# Copyright 2025 Franz und Franz GmbH
#
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#     http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.

"""Command-line interface for m1f-scrape."""

import argparse
import logging
import sqlite3
import sys
import time
from pathlib import Path
from typing import Optional

# Use unified colorama module
try:
    from ..shared.colors import (
        Colors,
        success,
        error,
        warning,
        info,
        header,
        COLORAMA_AVAILABLE,
        ColoredHelpFormatter,
    )
except ImportError:
    COLORAMA_AVAILABLE = False

    # Fallback formatter
    class ColoredHelpFormatter(argparse.RawDescriptionHelpFormatter):
        pass


from . import __version__
from .config import Config, ScraperBackend
from .crawlers import WebCrawler


class CustomArgumentParser(argparse.ArgumentParser):
    """Custom argument parser with better error messages."""

    def error(self, message: str) -> None:
        """Display error message with colors if available."""
        error_msg = f"ERROR: {message}"
        if COLORAMA_AVAILABLE:
            error_msg = f"{Colors.RED}ERROR: {message}{Colors.RESET}"
        self.print_usage(sys.stderr)
        print(f"\n{error_msg}", file=sys.stderr)
        print(f"\nFor detailed help, use: {self.prog} --help", file=sys.stderr)
        self.exit(2)


def cleanup_orphaned_sessions(db_path: Path) -> None:
    """Clean up sessions that were left in 'running' state.
    
    Args:
        db_path: Path to the SQLite database
    """
    if not db_path.exists():
        warning("No database found.")
        return
    
    try:
        conn = sqlite3.connect(str(db_path))
        cursor = conn.cursor()
        
        # Check if scraping_sessions table exists
        cursor.execute(
            "SELECT name FROM sqlite_master WHERE type='table' AND name='scraping_sessions'"
        )
        if not cursor.fetchone():
            warning("No sessions table found in database")
            conn.close()
            return
        
        # Find all running sessions
        cursor.execute(
            """
            SELECT id, start_url, start_time 
            FROM scraping_sessions 
            WHERE status = 'running'
            ORDER BY start_time DESC
            """
        )
        
        running_sessions = cursor.fetchall()
        
        if not running_sessions:
            info("No running sessions found")
            conn.close()
            return
        
        # Check which sessions are truly orphaned (no activity in last hour)
        from datetime import datetime, timedelta
        one_hour_ago = datetime.now() - timedelta(hours=1)
        orphaned_sessions = []
        active_sessions = []
        
        header(f"Found {len(running_sessions)} running session(s):")
        for session_id, start_url, start_time in running_sessions:
            # Get last activity time
            cursor.execute(
                """
                SELECT MAX(scraped_at), COUNT(*), 
                       COUNT(CASE WHEN error IS NULL THEN 1 END)
                FROM scraped_urls 
                WHERE session_id = ?
                """,
                (session_id,)
            )
            result = cursor.fetchone()
            last_activity, total, successful = result if result else (None, 0, 0)
            
            # Use start_time if no URLs scraped yet
            last_activity = last_activity or start_time
            
            # Convert string timestamp to datetime if needed
            if isinstance(last_activity, str):
                last_activity_dt = datetime.fromisoformat(last_activity.replace('Z', '+00:00'))
            else:
                last_activity_dt = last_activity
            
            is_orphaned = last_activity_dt < one_hour_ago
            
            info(f"  Session #{session_id}: {start_url}")
            info(f"    Started: {start_time}")
            info(f"    Last activity: {last_activity}")
            info(f"    Pages scraped: {successful}/{total}")
            
            if is_orphaned:
                info(f"    Status: ORPHANED (no activity for >1 hour)")
                orphaned_sessions.append((session_id, start_url, start_time))
            else:
                info(f"    Status: ACTIVE (recent activity)")
                active_sessions.append(session_id)
        
        if not orphaned_sessions:
            if active_sessions:
                info(f"\nAll {len(active_sessions)} session(s) appear to be actively running.")
            info("No orphaned sessions found.")
            conn.close()
            return
        
        # Ask for confirmation only for orphaned sessions
        info(f"\n{len(orphaned_sessions)} session(s) appear to be orphaned (no activity for >1 hour).")
        if active_sessions:
            info(f"{len(active_sessions)} session(s) are still active and will not be touched.")
        response = input("Mark orphaned sessions as 'interrupted'? (y/N): ")
        
        if response.lower() == 'y':
            for session_id, _, _ in orphaned_sessions:  # Only process orphaned sessions
                # Get final statistics
                cursor.execute(
                    """
                    SELECT 
                        COUNT(*) as total,
                        COUNT(CASE WHEN error IS NULL THEN 1 END) as successful,
                        COUNT(CASE WHEN error IS NOT NULL THEN 1 END) as failed
                    FROM scraped_urls 
                    WHERE session_id = ?
                    """,
                    (session_id,)
                )
                result = cursor.fetchone()
                total, successful, failed = result if result else (0, 0, 0)
                
                # Update session
                cursor.execute(
                    """
                    UPDATE scraping_sessions 
                    SET status = 'interrupted',
                        end_time = ?,
                        total_pages = ?,
                        successful_pages = ?,
                        failed_pages = ?
                    WHERE id = ?
                    """,
                    (datetime.now(), total, successful, failed, session_id)
                )
            
            conn.commit()
            success(f"Marked {len(orphaned_sessions)} orphaned session(s) as interrupted")
        else:
            info("No changes made")
        
        conn.close()
        
    except sqlite3.Error as e:
        error(f"Database error: {e}")
    except Exception as e:
        error(f"Error cleaning up sessions: {e}")


def show_scraping_sessions(db_path: Path, detailed: bool = False) -> None:
    """Show all scraping sessions from the database.
    
    Args:
        db_path: Path to the SQLite database
        detailed: If True, show detailed session information
    """
    if not db_path.exists():
        warning("No database found.")
        return
    
    try:
        conn = sqlite3.connect(str(db_path))
        cursor = conn.cursor()
        
        # Check if scraping_sessions table exists
        cursor.execute(
            "SELECT name FROM sqlite_master WHERE type='table' AND name='scraping_sessions'"
        )
        if not cursor.fetchone():
            # Fall back to old behavior if no sessions table
            cursor.execute("""
                SELECT 
                    DATE(scraped_at) as session_date,
                    MIN(TIME(scraped_at)) as start_time,
                    MAX(TIME(scraped_at)) as end_time,
                    COUNT(*) as url_count,
                    COUNT(CASE WHEN error IS NULL THEN 1 END) as successful,
                    COUNT(CASE WHEN error IS NOT NULL THEN 1 END) as failed
                FROM scraped_urls
                GROUP BY DATE(scraped_at)
                ORDER BY session_date DESC
            """)
            
            sessions = cursor.fetchall()
            
            if sessions:
                header("Scraping Sessions (Legacy):")
                info("Date       | Start    | End      | Total URLs | Success | Failed")
                info("-" * 70)
                for session in sessions:
                    date, start, end, total, success_count, failed = session
                    info(f"{date} | {start[:8] if start else 'N/A'} | {end[:8] if end else 'N/A'} | {total:10} | {success_count:7} | {failed:6}")
            else:
                warning("No scraping sessions found in database")
        else:
            # Use new sessions table
            cursor.execute("""
                SELECT 
                    id,
                    start_url,
                    start_time,
                    end_time,
                    status,
                    total_pages,
                    successful_pages,
                    failed_pages,
                    scraper_backend,
                    max_pages,
                    max_depth
                FROM scraping_sessions
                ORDER BY start_time DESC
            """)
            
            sessions = cursor.fetchall()
            
            if sessions:
                header("Scraping Sessions:")
                if detailed:
                    for session in sessions:
                        (session_id, start_url, start_time, end_time, status,
                         total, successful, failed, backend, max_pages, max_depth) = session
                        
                        info(f"\nSession #{session_id}:")
                        info(f"  URL: {start_url}")
                        info(f"  Started: {start_time}")
                        info(f"  Ended: {end_time if end_time else 'Still running'}")
                        info(f"  Status: {status}")
                        info(f"  Backend: {backend}")
                        info(f"  Pages: {successful} success, {failed} failed (total: {total})")
                        info(f"  Limits: max_pages={max_pages}, max_depth={max_depth}")
                else:
                    info("ID  | Status    | Started             | Pages | Success | Failed | URL")
                    info("-" * 100)
                    for session in sessions:
                        (session_id, start_url, start_time, end_time, status,
                         total, successful, failed, backend, _, _) = session
                        
                        # Truncate URL if too long
                        url_display = start_url[:40] + "..." if len(start_url) > 40 else start_url
                        
                        info(f"{session_id:3} | {status:9} | {start_time[:19]} | {total:5} | {successful:7} | {failed:6} | {url_display}")
            else:
                warning("No scraping sessions found in database")
        
        conn.close()
        
    except sqlite3.Error as e:
        error(f"Database error: {e}")
    except Exception as e:
        error(f"Error showing sessions: {e}")


def clear_session(db_path: Path, session_id: Optional[int] = None, delete_files: bool = False, auto_delete: bool = False) -> None:
    """Clear URLs from a specific scraping session or the last session.
    
    Args:
        db_path: Path to the SQLite database
        session_id: Specific session ID to clear, or None for the last session
        delete_files: Whether to also delete downloaded files
        auto_delete: If True, skip confirmation prompt for file deletion
    """
    if not db_path.exists():
        warning("No database found.")
        return
    
    try:
        conn = sqlite3.connect(str(db_path))
        cursor = conn.cursor()
        
        # Check if scraping_sessions table exists
        cursor.execute(
            "SELECT name FROM sqlite_master WHERE type='table' AND name='scraping_sessions'"
        )
        has_sessions_table = cursor.fetchone() is not None
        
        if has_sessions_table:
            # Use session-based deletion
            if session_id is None:
                # Find the most recent session
                cursor.execute(
                    "SELECT id FROM scraping_sessions ORDER BY start_time DESC LIMIT 1"
                )
                result = cursor.fetchone()
                if not result:
                    warning("No scraping sessions found in database")
                    conn.close()
                    return
                session_id = result[0]
            
            # Get session info
            cursor.execute(
                "SELECT start_url, start_time FROM scraping_sessions WHERE id = ?",
                (session_id,)
            )
            session_info = cursor.fetchone()
            if not session_info:
                warning(f"Session #{session_id} not found")
                conn.close()
                return
            
            start_url, start_time = session_info
            
            # Get file paths from this session
            cursor.execute(
                """SELECT target_filename 
                   FROM scraped_urls 
                   WHERE session_id = ? AND target_filename IS NOT NULL AND target_filename != ''""",
                (session_id,)
            )
            file_paths = [row[0] for row in cursor.fetchall()]
            
            # Get checksums of URLs from this session
            cursor.execute(
                """SELECT content_checksum 
                   FROM scraped_urls 
                   WHERE session_id = ? AND content_checksum IS NOT NULL""",
                (session_id,)
            )
            checksums = [row[0] for row in cursor.fetchall()]
            
            # Count URLs to be deleted
            cursor.execute(
                "SELECT COUNT(*) FROM scraped_urls WHERE session_id = ?",
                (session_id,)
            )
            url_count = cursor.fetchone()[0]
            
            # Handle file deletion if requested
            files_deleted = 0
            if delete_files and file_paths:
                # Build list of actual files to delete
                output_dir = db_path.parent
                files_to_delete = []
                for file_path in file_paths:
                    full_path = output_dir / file_path
                    if full_path.exists():
                        files_to_delete.append(full_path)
                    # Also check for metadata files
                    meta_path = full_path.with_suffix(full_path.suffix + '.meta.json')
                    if meta_path.exists():
                        files_to_delete.append(meta_path)
                
                if files_to_delete:
                    # Ask for confirmation if not auto-deleting
                    should_delete = auto_delete
                    if not auto_delete:
                        info(f"\nFound {len(files_to_delete)} files from session #{session_id}:")
                        # Show first 10 files as examples
                        for i, file in enumerate(files_to_delete[:10]):
                            info(f"  - {file.relative_to(output_dir)}")
                        if len(files_to_delete) > 10:
                            info(f"  ... and {len(files_to_delete) - 10} more files")
                        
                        response = input("\nAlso delete these downloaded files? (y/N): ")
                        should_delete = response.lower() == 'y'
                    
                    if should_delete:
                        import shutil
                        for file_path in files_to_delete:
                            try:
                                if file_path.is_dir():
                                    shutil.rmtree(file_path)
                                else:
                                    file_path.unlink()
                                files_deleted += 1
                            except Exception as e:
                                warning(f"Failed to delete {file_path}: {e}")
            
            # Delete URLs from session
            cursor.execute("DELETE FROM scraped_urls WHERE session_id = ?", (session_id,))
            
            # Delete the session record
            cursor.execute("DELETE FROM scraping_sessions WHERE id = ?", (session_id,))
            
            # Delete associated checksums
            checksum_count = 0
            if checksums:
                for checksum in checksums:
                    cursor.execute(
                        "DELETE FROM content_checksums WHERE checksum = ?",
                        (checksum,)
                    )
                    checksum_count += cursor.rowcount
            
            conn.commit()
            success(f"Cleared session #{session_id} ({url_count} URLs from {start_url} at {start_time})")
            if checksum_count > 0:
                info(f"Also cleared {checksum_count} associated content checksums")
            if files_deleted > 0:
                info(f"Deleted {files_deleted} downloaded files")
        else:
            # Fall back to date-based deletion for legacy databases
            cursor.execute(
                "SELECT MAX(DATE(scraped_at)) as last_date FROM scraped_urls"
            )
            result = cursor.fetchone()
            if not result or not result[0]:
                warning("No scraping sessions found in database")
                conn.close()
                return
            
            last_date = result[0]
            
            # Get file paths from last session
            cursor.execute(
                """SELECT target_filename 
                   FROM scraped_urls 
                   WHERE DATE(scraped_at) = ? AND target_filename IS NOT NULL AND target_filename != ''""",
                (last_date,)
            )
            file_paths = [row[0] for row in cursor.fetchall()]
            
            # Get checksums of URLs from last session
            cursor.execute(
                """SELECT content_checksum 
                   FROM scraped_urls 
                   WHERE DATE(scraped_at) = ? AND content_checksum IS NOT NULL""",
                (last_date,)
            )
            checksums = [row[0] for row in cursor.fetchall()]
            
            # Count URLs to be deleted
            cursor.execute(
                "SELECT COUNT(*) FROM scraped_urls WHERE DATE(scraped_at) = ?",
                (last_date,)
            )
            url_count = cursor.fetchone()[0]
            
            # Handle file deletion if requested (same logic as above)
            files_deleted = 0
            if delete_files and file_paths:
                output_dir = db_path.parent
                files_to_delete = []
                for file_path in file_paths:
                    full_path = output_dir / file_path
                    if full_path.exists():
                        files_to_delete.append(full_path)
                    meta_path = full_path.with_suffix(full_path.suffix + '.meta.json')
                    if meta_path.exists():
                        files_to_delete.append(meta_path)
                
                if files_to_delete:
                    should_delete = auto_delete
                    if not auto_delete:
                        info(f"\nFound {len(files_to_delete)} files from session {last_date}:")
                        for i, file in enumerate(files_to_delete[:10]):
                            info(f"  - {file.relative_to(output_dir)}")
                        if len(files_to_delete) > 10:
                            info(f"  ... and {len(files_to_delete) - 10} more files")
                        
                        response = input("\nAlso delete these downloaded files? (y/N): ")
                        should_delete = response.lower() == 'y'
                    
                    if should_delete:
                        import shutil
                        for file_path in files_to_delete:
                            try:
                                if file_path.is_dir():
                                    shutil.rmtree(file_path)
                                else:
                                    file_path.unlink()
                                files_deleted += 1
                            except Exception as e:
                                warning(f"Failed to delete {file_path}: {e}")
            
            # Delete URLs from last session
            cursor.execute("DELETE FROM scraped_urls WHERE DATE(scraped_at) = ?", (last_date,))
            
            # Delete associated checksums
            checksum_count = 0
            if checksums:
                for checksum in checksums:
                    cursor.execute(
                        "DELETE FROM content_checksums WHERE checksum = ?",
                        (checksum,)
                    )
                    checksum_count += cursor.rowcount
            
            conn.commit()
            success(f"Cleared {url_count} URLs from session {last_date}")
            if checksum_count > 0:
                info(f"Also cleared {checksum_count} associated content checksums")
            if files_deleted > 0:
                info(f"Deleted {files_deleted} downloaded files")
        
        conn.close()
        
    except sqlite3.Error as e:
        error(f"Database error: {e}")
    except Exception as e:
        error(f"Error clearing session: {e}")


def clear_urls_from_database(db_path: Path, pattern: str) -> None:
    """Clear URLs matching a pattern from the database.
    
    Also clears the associated content checksums to ensure pages can be re-scraped.
    
    Args:
        db_path: Path to the SQLite database
        pattern: Pattern to match URLs (uses SQL LIKE)
    """
    if not db_path.exists():
        warning("No database found. Nothing to clear.")
        return
    
    try:
        conn = sqlite3.connect(str(db_path))
        cursor = conn.cursor()
        
        # First get the checksums of URLs to be deleted
        cursor.execute(
            "SELECT content_checksum FROM scraped_urls WHERE url LIKE ? AND content_checksum IS NOT NULL",
            (f"%{pattern}%",)
        )
        checksums = [row[0] for row in cursor.fetchall()]
        
        # Count how many URLs will be deleted
        cursor.execute(
            "SELECT COUNT(*) FROM scraped_urls WHERE url LIKE ?",
            (f"%{pattern}%",)
        )
        url_count = cursor.fetchone()[0]
        
        if url_count == 0:
            warning(f"No URLs found matching pattern: {pattern}")
        else:
            # Delete the URLs
            cursor.execute(
                "DELETE FROM scraped_urls WHERE url LIKE ?",
                (f"%{pattern}%",)
            )
            
            # Delete associated checksums
            checksum_count = 0
            if checksums:
                # Delete checksums one by one (SQLite doesn't support DELETE with IN and many values well)
                for checksum in checksums:
                    cursor.execute(
                        "DELETE FROM content_checksums WHERE checksum = ?",
                        (checksum,)
                    )
                    checksum_count += cursor.rowcount
            
            conn.commit()
            success(f"Cleared {url_count} URLs matching pattern: {pattern}")
            if checksum_count > 0:
                info(f"Also cleared {checksum_count} associated content checksums")
        
        conn.close()
        
    except sqlite3.Error as e:
        error(f"Database error: {e}")
    except Exception as e:
        error(f"Error clearing URLs: {e}")


def show_database_info(db_path: Path, args: argparse.Namespace) -> None:
    """Show information from the scrape tracker database.

    Args:
        db_path: Path to the SQLite database
        args: Command line arguments
    """
    if not db_path.exists():
        warning("No database found. Have you scraped anything yet?")
        return

    try:
        conn = sqlite3.connect(str(db_path))
        cursor = conn.cursor()

        if args.show_db_stats:
            # Show statistics
            cursor.execute("SELECT COUNT(*) FROM scraped_urls")
            total = cursor.fetchone()[0]

            cursor.execute("SELECT COUNT(*) FROM scraped_urls WHERE error IS NULL")
            successful = cursor.fetchone()[0]

            cursor.execute("SELECT COUNT(*) FROM scraped_urls WHERE error IS NOT NULL")
            errors = cursor.fetchone()[0]

            header("Scraping Statistics:")
            info(f"Total URLs processed: {total}")
            info(f"Successfully scraped: {successful}")
            info(f"Errors encountered: {errors}")

            if total > 0:
                success_rate = (successful / total) * 100
                info(f"Success rate: {success_rate:.1f}%")

        if args.show_errors:
            # Show URLs with errors
            cursor.execute(
                "SELECT url, error FROM scraped_urls WHERE error IS NOT NULL"
            )
            errors = cursor.fetchall()

            if errors:
                header("URLs with Errors:")
                for url, error in errors:
                    info(f"{Colors.RED}✗{Colors.RESET} {url}")
                    info(f"    Error: {error}")
            else:
                success("No errors found!")

        if args.show_scraped_urls:
            # Show all scraped URLs
            cursor.execute(
                "SELECT url, status_code FROM scraped_urls ORDER BY scraped_at"
            )
            urls = cursor.fetchall()

            if urls:
                header("Scraped URLs:")
                for url, status_code in urls:
                    if status_code == 200:
                        status_icon = f"{Colors.GREEN}✓{Colors.RESET}"
                    else:
                        status_icon = f"{Colors.YELLOW}{status_code}{Colors.RESET}"
                    info(f"{status_icon} {url}")
            else:
                warning("No URLs found in database")

        conn.close()

    except sqlite3.Error as e:
        error(f"Database error: {e}")
    except Exception as e:
        error(f"Error reading database: {e}")


def create_parser() -> CustomArgumentParser:
    """Create the argument parser."""
    description = """m1f-scrape - Web Scraper Tool
============================
Download websites for offline viewing with support for multiple scraper backends.

Perfect for:
• Creating offline documentation mirrors
• Archiving websites for research
• Converting HTML sites to Markdown with m1f-html2md
• Building AI training datasets"""

    epilog = """Examples:
  %(prog)s https://example.com/docs -o ./html
  %(prog)s https://example.com/docs -o ./html --max-pages 100
  %(prog)s https://example.com/blog -o ./html --allowed-path /blog/2024/
  %(prog)s https://example.com/docs -o ./html --allowed-paths /docs/ /api/ /guides/
  %(prog)s https://example.com -o ./html --scraper httrack
  %(prog)s --show-db-stats -o ./html  # Show scraping statistics

For more information, see the documentation."""

    parser = CustomArgumentParser(
        prog="m1f-scrape",
        description=description,
        epilog=epilog,
        formatter_class=ColoredHelpFormatter,
        add_help=True,
    )

    parser.add_argument(
        "--version",
        action="version",
        version=f"%(prog)s {__version__}",
        help="Show program version and exit",
    )

    # Main arguments
    parser.add_argument(
        "url", nargs="?", help="URL to scrape (not needed for database queries)"
    )
    parser.add_argument(
        "-o",
        "--output",
        type=Path,
        required=True,
        help="Output directory for downloaded files",
    )

    # Output control group
    output_group = parser.add_argument_group("Output Control")
    output_group.add_argument(
        "-v", "--verbose", action="store_true", help="Enable verbose output"
    )
    output_group.add_argument(
        "-q", "--quiet", action="store_true", help="Suppress all output except errors"
    )

    # Scraper options group
    scraper_group = parser.add_argument_group("Scraper Options")
    scraper_group.add_argument(
        "--scraper",
        type=str,
        choices=[
            "httrack",
            "beautifulsoup",
            "bs4",
            "selectolax",
            "httpx",
            "playwright",
        ],
        default="beautifulsoup",
        help="Web scraper backend to use (default: beautifulsoup)",
    )
    scraper_group.add_argument(
        "--scraper-config",
        type=Path,
        help="Path to scraper-specific configuration file (YAML/JSON)",
    )

    # Crawl configuration group
    crawl_group = parser.add_argument_group("Crawl Configuration")
    crawl_group.add_argument(
        "--max-depth",
        type=int,
        default=5,
        help="Maximum crawl depth (default: 5, use -1 for unlimited)",
    )
    crawl_group.add_argument(
        "--max-pages",
        type=int,
        default=10000,
        help="Maximum pages to crawl (default: 10000, use -1 for unlimited)",
    )
    
    # Path restriction options (mutually exclusive)
    path_group = crawl_group.add_mutually_exclusive_group()
    path_group.add_argument(
        "--allowed-path",
        type=str,
        help="Restrict crawling to this path and subdirectories (e.g., /docs/)",
    )
    path_group.add_argument(
        "--allowed-paths",
        type=str,
        nargs="*",
        metavar="PATH",
        help="Restrict crawling to multiple paths and subdirectories (e.g., /docs/ /api/)",
    )
    
    crawl_group.add_argument(
        "--excluded-paths",
        type=str,
        nargs="*",
        metavar="PATH",
        help="URL paths to exclude from crawling (can specify multiple)",
    )

    # Request options group
    request_group = parser.add_argument_group("Request Options")
    request_group.add_argument(
        "--request-delay",
        type=float,
        default=5.0,
        help="Delay between requests in seconds (default: 5.0)",
    )
    request_group.add_argument(
        "--concurrent-requests",
        type=int,
        default=2,
        help="Number of concurrent requests (default: 2)",
    )
    request_group.add_argument(
        "--user-agent", type=str, help="Custom user agent string"
    )
    request_group.add_argument(
        "--timeout",
        type=int,
        default=30,
        help="Request timeout in seconds (default: 30)",
    )
    request_group.add_argument(
        "--retry-count",
        type=int,
        default=3,
        help="Number of retries for failed requests (default: 3)",
    )

    # Content filtering group
    filter_group = parser.add_argument_group("Content Filtering")
    filter_group.add_argument(
        "--ignore-get-params",
        action="store_true",
        help="Strip query parameters from URLs (treats /page?tab=1 and /page?tab=2 as duplicates)",
    )
    filter_group.add_argument(
        "--ignore-canonical",
        action="store_true",
        help="Disable canonical URL deduplication (keeps pages even if they specify a different canonical URL)",
    )
    filter_group.add_argument(
        "--ignore-duplicates",
        action="store_true",
        help="Disable content-based deduplication (keeps pages even if their content is identical)",
    )
    
    # Asset download options
    filter_group.add_argument(
        "--download-assets",
        action="store_true",
        help="Download linked assets like images, PDFs, CSS, JS, and other files",
    )
    filter_group.add_argument(
        "--asset-types",
        type=str,
        nargs="*",
        help="File extensions to download (e.g., .pdf .jpg .png). Default: common web assets",
    )
    filter_group.add_argument(
        "--max-asset-size",
        type=int,
        metavar="BYTES",
        help="Maximum file size for asset downloads in bytes (default: 50MB)",
    )
    filter_group.add_argument(
        "--assets-subdirectory",
        type=str,
        default="assets",
        help="Subdirectory name for storing downloaded assets (default: assets)",
    )

    # Display options group
    display_group = parser.add_argument_group("Display Options")
    display_group.add_argument(
        "--list-files",
        action="store_true",
        help="List all downloaded files after completion",
    )
    display_group.add_argument(
        "--save-urls",
        type=Path,
        metavar="FILE",
        help="Save all scraped URLs to a file (one per line)",
    )
    display_group.add_argument(
        "--save-files",
        type=Path,
        metavar="FILE",
        help="Save list of all downloaded files to a file (one per line)",
    )

    # Security options group
    security_group = parser.add_argument_group("Security Options")
    security_group.add_argument(
        "--disable-ssrf-check",
        action="store_true",
        help="Disable SSRF vulnerability checks (allows crawling private IPs)",
    )
    security_group.add_argument(
        "--force-rescrape",
        action="store_true",
        help="Force re-scraping of all URLs, ignoring the database cache",
    )

    # Database options group
    db_group = parser.add_argument_group("Database Options")
    db_group.add_argument(
        "--show-db-stats",
        action="store_true",
        help="Show scraping statistics from the database",
    )
    db_group.add_argument(
        "--show-errors",
        action="store_true",
        help="Show URLs that had errors during scraping",
    )
    db_group.add_argument(
        "--show-scraped-urls",
        action="store_true",
        help="List all scraped URLs from the database",
    )
    db_group.add_argument(
        "--clear-urls",
        type=str,
        metavar="PATTERN",
        help="Clear URLs from database matching the pattern (e.g., '/Extensions/' or 'example.com')",
    )
    db_group.add_argument(
        "--clear-last-session",
        action="store_true",
        help="Clear URLs from the last scraping session",
    )
    db_group.add_argument(
        "--clear-session",
        type=int,
        metavar="ID",
        help="Clear a specific session by its ID",
    )
    db_group.add_argument(
        "--delete-files",
        action="store_true",
        help="Also delete downloaded files when clearing sessions (skips confirmation)",
    )
    db_group.add_argument(
        "--show-sessions",
        action="store_true",
        help="Show all scraping sessions with timestamps and URL counts",
    )
    db_group.add_argument(
        "--show-sessions-detailed",
        action="store_true",
        help="Show detailed information for all scraping sessions",
    )
    db_group.add_argument(
        "--cleanup-sessions",
        action="store_true",
        help="Clean up orphaned sessions (left in 'running' state from crashes)",
    )

    return parser


def main() -> None:
    """Main entry point."""
    parser = create_parser()
    args = parser.parse_args()

    # Create configuration
    config = Config()
    config.crawler.max_depth = args.max_depth
    config.crawler.max_pages = args.max_pages
    config.crawler.allowed_path = args.allowed_path
    config.crawler.allowed_paths = args.allowed_paths
    if args.excluded_paths:
        config.crawler.excluded_paths = args.excluded_paths
    config.crawler.scraper_backend = ScraperBackend(args.scraper)
    config.crawler.request_delay = args.request_delay
    config.crawler.concurrent_requests = args.concurrent_requests
    config.crawler.timeout = args.timeout
    config.crawler.retry_count = args.retry_count
    config.crawler.respect_robots_txt = True  # Always respect robots.txt
    config.crawler.check_ssrf = not args.disable_ssrf_check

    if args.user_agent:
        config.crawler.user_agent = args.user_agent

    config.crawler.ignore_get_params = args.ignore_get_params
    config.crawler.check_canonical = not args.ignore_canonical
    config.crawler.check_content_duplicates = not args.ignore_duplicates
    config.crawler.force_rescrape = args.force_rescrape
    
    # Asset download configuration
    config.crawler.download_assets = args.download_assets
    if args.asset_types:
        config.crawler.asset_types = args.asset_types
    if args.max_asset_size:
        config.crawler.max_asset_size = args.max_asset_size
    config.crawler.assets_subdirectory = args.assets_subdirectory

    # Load scraper-specific config if provided
    if args.scraper_config:
        import yaml
        import json

        if args.scraper_config.suffix == ".json":
            with open(args.scraper_config) as f:
                config.crawler.scraper_config = json.load(f)
        else:  # Assume YAML
            with open(args.scraper_config) as f:
                config.crawler.scraper_config = yaml.safe_load(f)

    config.verbose = args.verbose
    config.quiet = args.quiet

    # Set up logging
    if args.verbose:
        logging.basicConfig(
            level=logging.INFO,
            format="%(asctime)s - %(name)s - %(levelname)s - %(message)s",
        )
    elif not args.quiet:
        logging.basicConfig(
            level=logging.WARNING, format="%(asctime)s - %(levelname)s - %(message)s"
        )

    # Check if clear-urls is requested
    if args.clear_urls:
        db_path = args.output / "scrape_tracker.db"
        clear_urls_from_database(db_path, args.clear_urls)
        return
    
    # Check if clear-last-session is requested
    if args.clear_last_session:
        db_path = args.output / "scrape_tracker.db"
        # If no --delete-files flag, ask for confirmation
        clear_session(db_path, delete_files=True, auto_delete=args.delete_files)
        return
    
    # Check if clear-session is requested
    if args.clear_session:
        db_path = args.output / "scrape_tracker.db"
        # If no --delete-files flag, ask for confirmation
        clear_session(db_path, args.clear_session, delete_files=True, auto_delete=args.delete_files)
        return
    
    # Check if cleanup-sessions is requested
    if args.cleanup_sessions:
        db_path = args.output / "scrape_tracker.db"
        cleanup_orphaned_sessions(db_path)
        return
    
    # Check if show-sessions is requested
    if args.show_sessions or args.show_sessions_detailed:
        db_path = args.output / "scrape_tracker.db"
        show_scraping_sessions(db_path, detailed=args.show_sessions_detailed)
        return
    
    # Check if only database query options are requested
    if args.show_db_stats or args.show_errors or args.show_scraped_urls:
        # Just show database info and exit
        db_path = args.output / "scrape_tracker.db"
        show_database_info(db_path, args)
        return

    # URL is required for scraping
    if not args.url:
        parser.error("URL is required for scraping")

    # Create output directory
    args.output.mkdir(parents=True, exist_ok=True)

    info(f"Scraping website: {args.url}")
    info(f"Using scraper backend: {args.scraper}")
    info("This may take a while...")
    info("Press Ctrl+C to interrupt and resume later\n")

    # Track start time for statistics
    start_time = time.time()

    try:
        # Create crawler and download the website
        crawler = WebCrawler(config.crawler)
        crawl_result = crawler.crawl_sync_with_stats(args.url, args.output)
        site_dir = crawl_result["site_dir"]
        scraped_urls = crawl_result.get("scraped_urls", [])
        errors = crawl_result.get("errors", [])
        session_files = crawl_result.get("session_files", [])
        session_id = crawl_result.get("session_id")

        # Calculate statistics for this session only
        duration = time.time() - start_time
        total_urls = len(scraped_urls) + len(errors)
        successful_urls = len(scraped_urls)
        success_rate = (successful_urls / total_urls * 100) if total_urls > 0 else 0
        avg_time_per_page = duration / total_urls if total_urls > 0 else 0

        # Display summary statistics
        header("\n" + "=" * 60)
        header(f"Scraping Summary (Session #{session_id})" if session_id else "Scraping Summary (Current Session)")
        header("=" * 60)
        success(f"✓ Successfully scraped {successful_urls} pages")
        if errors:
            warning(f"⚠ Failed to scrape {len(errors)} pages")
        info(f"Total URLs processed: {total_urls}")
        info(f"Success rate: {success_rate:.1f}%")
        info(f"Total duration: {duration:.1f} seconds")
        info(f"Average time per page: {avg_time_per_page:.2f} seconds")
        info(f"Output directory: {site_dir}")
        info(f"HTML files saved in this session: {len(session_files)}")
        if session_id:
            info(f"\nSession ID: #{session_id}")
            info(f"To clear this session: m1f-scrape --clear-session {session_id} -o {args.output}")

        # Save URLs to file if requested
        if args.save_urls:
            try:
                with open(args.save_urls, 'w', encoding='utf-8') as f:
                    for url in scraped_urls:
                        f.write(f"{url}\n")
                success(f"Saved {len(scraped_urls)} URLs to {args.save_urls}")
            except Exception as e:
                error(f"Failed to save URLs to file: {e}")

        # Save file list if requested (for this session only)
        if args.save_files:
            try:
                with open(args.save_files, 'w', encoding='utf-8') as f:
                    for html_file in sorted(session_files):
                        f.write(f"{html_file}\n")
                success(f"Saved {len(session_files)} file paths to {args.save_files}")
            except Exception as e:
                error(f"Failed to save file list: {e}")

        # List downloaded files if requested (with limit for verbose output)
        if args.list_files or config.verbose:
            if session_files:
                info("\nDownloaded files in this session:")
                files_to_show = sorted(session_files)
                max_files_to_show = 30
                
                if len(files_to_show) > max_files_to_show:
                    # Show first 15 and last 15 files
                    for html_file in files_to_show[:15]:
                        rel_path = html_file.relative_to(site_dir)
                        info(f"  - {rel_path}")
                    info(f"  ... ({len(files_to_show) - max_files_to_show} more files) ...")
                    for html_file in files_to_show[-15:]:
                        rel_path = html_file.relative_to(site_dir)
                        info(f"  - {rel_path}")
                    info(f"\nTotal: {len(files_to_show)} files downloaded in this session")
                else:
                    for html_file in files_to_show:
                        rel_path = html_file.relative_to(site_dir)
                        info(f"  - {rel_path}")
            else:
                info("\nNo new files downloaded in this session (all URLs were already scraped)")

    except KeyboardInterrupt:
        warning("\n⚠️  Scraping interrupted by user")
        info("Run the same command again to resume where you left off")
        info(f"To view session details: m1f-scrape --show-sessions -o {args.output}")
        sys.exit(0)
    except Exception as e:
        error(f"Error during scraping: {e}")
        if config.verbose:
            import traceback

            traceback.print_exc()
        sys.exit(1)


if __name__ == "__main__":
    main()

======= scrape_tool/config.py ======
# Copyright 2025 Franz und Franz GmbH
#
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#     http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.

"""Configuration models for m1f-scrape."""

from enum import Enum
from typing import Any, Dict, Optional

from pydantic import BaseModel, Field


class ScraperBackend(str, Enum):
    """Available scraper backends."""

    HTTRACK = "httrack"
    BEAUTIFULSOUP = "beautifulsoup"
    BS4 = "bs4"  # Alias for beautifulsoup
    SELECTOLAX = "selectolax"
    HTTPX = "httpx"
    PLAYWRIGHT = "playwright"


class CrawlerConfig(BaseModel):
    """Configuration for web crawler."""

    max_depth: int = Field(
        default=5,
        ge=-1,
        le=1000,
        description="Maximum crawl depth (-1 for unlimited)",
    )
    max_pages: int = Field(
        default=10000,
        ge=-1,
        le=10000000,
        description="Maximum pages to crawl (-1 for unlimited)",
    )
    follow_external_links: bool = Field(
        default=False, description="Follow links to external domains"
    )
    allowed_domains: Optional[list[str]] = Field(
        default=None, description="List of allowed domains to crawl"
    )
    allowed_path: Optional[str] = Field(
        default=None,
        description="Restrict crawling to this path/URL and its subdirectories (e.g., /docs/ or https://example.com/docs/)",
    )
    allowed_paths: Optional[list[str]] = Field(
        default=None,
        description="List of paths/URLs to restrict crawling to (alternative to allowed_path)",
    )
    excluded_paths: list[str] = Field(
        default_factory=list, description="URL paths to exclude from crawling"
    )
    scraper_backend: ScraperBackend = Field(
        default=ScraperBackend.BEAUTIFULSOUP, description="Web scraper backend to use"
    )
    scraper_config: Dict[str, Any] = Field(
        default_factory=dict, description="Backend-specific configuration"
    )
    request_delay: float = Field(
        default=5.0,
        ge=0,
        le=60,
        description="Delay between requests in seconds (default: 5s for rate limiting)",
    )
    concurrent_requests: int = Field(
        default=2,
        ge=1,
        le=20,
        description="Number of concurrent requests (default: 2 for Cloudflare)",
    )
    user_agent: Optional[str] = Field(
        default=None, description="Custom user agent string"
    )
    respect_robots_txt: bool = Field(
        default=True, description="Respect robots.txt rules"
    )
    timeout: int = Field(
        default=30, ge=1, le=300, description="Request timeout in seconds"
    )
    retry_count: int = Field(
        default=3, ge=0, le=10, description="Number of retries for failed requests"
    )
    ignore_get_params: bool = Field(
        default=False,
        description="Ignore GET parameters in URLs to avoid duplicate content",
    )
    check_canonical: bool = Field(
        default=True, description="Skip pages if canonical URL differs from current URL"
    )
    check_content_duplicates: bool = Field(
        default=True,
        description="Skip pages with duplicate content (based on text-only checksum)",
    )
    check_ssrf: bool = Field(
        default=True,
        description="Check for SSRF vulnerabilities by blocking private IP addresses",
    )
    force_rescrape: bool = Field(
        default=False,
        description="Force re-scraping of all URLs, ignoring database cache",
    )
    download_assets: bool = Field(
        default=False,
        description="Download linked assets like images, PDFs, and other files",
    )
    download_external_assets: bool = Field(
        default=True,
        description="Allow downloading assets from external domains (CDNs, etc.)",
    )
    asset_types: list[str] = Field(
        default_factory=lambda: [
            # Images (safe)
            ".jpg", ".jpeg", ".png", ".gif", ".svg", ".webp", ".ico",
            # Stylesheets and fonts (safe)
            ".css", ".woff", ".woff2", ".ttf", ".eot",
            # Documents (potentially risky but commonly needed)
            ".pdf", ".txt", ".md", ".csv",
            # Data formats (safe)
            ".json", ".xml",
            # Note: .js removed by default for security
            # Note: Office files removed by default (.doc, .docx, .xls, .xlsx, .ppt, .pptx)
            # Note: Archives removed by default (.zip, .tar, .gz, .rar, .7z)
            # Note: Media files removed by default (.mp4, .webm, .mp3, .wav, .ogg)
        ],
        description="File extensions to download when download_assets is enabled (security-filtered defaults)",
    )
    max_asset_size: int = Field(
        default=50 * 1024 * 1024,  # 50MB
        ge=0,
        le=1024 * 1024 * 1024,  # 1GB max
        description="Maximum file size in bytes for asset downloads",
    )
    assets_subdirectory: str = Field(
        default="assets",
        description="Subdirectory name for storing downloaded assets",
    )
    max_assets_per_page: int = Field(
        default=-1,  # -1 means no limit
        ge=-1,
        description="Maximum number of assets to download per page (-1 for unlimited)",
    )
    total_assets_limit: int = Field(
        default=-1,  # -1 means no limit
        ge=-1,
        description="Maximum total number of assets to download in a session (-1 for unlimited)",
    )


class Config(BaseModel):
    """Main configuration for m1f-scrape."""

    crawler: CrawlerConfig = Field(
        default_factory=CrawlerConfig, description="Crawler configuration"
    )
    verbose: bool = Field(default=False, description="Enable verbose output")
    quiet: bool = Field(default=False, description="Suppress output except errors")

======= scrape_tool/crawlers.py ======
#!/usr/bin/env python3
# Copyright 2025 Franz und Franz GmbH
#
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#     http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.

"""Web crawling functionality using configurable scraper backends."""

import asyncio
import json
import logging
import sqlite3
from datetime import datetime, timedelta
from pathlib import Path
from typing import List, Dict, Any, Optional, Set
from urllib.parse import urlparse, urljoin

from .scrapers import create_scraper, ScraperConfig, ScrapedPage
from .config import CrawlerConfig, ScraperBackend

logger = logging.getLogger(__name__)


class WebCrawler:
    """Web crawler that uses configurable scraper backends."""

    def __init__(self, config: CrawlerConfig):
        """Initialize crawler with configuration.

        Args:
            config: CrawlerConfig instance with crawling settings
        """
        self.config = config
        self._scraper_config = self._create_scraper_config()
        self._db_path: Optional[Path] = None
        self._db_conn: Optional[sqlite3.Connection] = None
        self._session_id: Optional[int] = None

    def _create_scraper_config(self) -> ScraperConfig:
        """Create scraper configuration from crawler config.

        Returns:
            ScraperConfig instance
        """
        # Convert allowed_domains from set to list
        allowed_domains = (
            list(self.config.allowed_domains) if self.config.allowed_domains else []
        )

        # Convert excluded_paths to exclude_patterns
        exclude_patterns = (
            list(self.config.excluded_paths) if self.config.excluded_paths else []
        )

        # Handle allowed paths - convert single path to list if needed
        allowed_paths_list = None
        if self.config.allowed_paths:
            allowed_paths_list = list(self.config.allowed_paths)
        elif self.config.allowed_path:
            allowed_paths_list = [self.config.allowed_path]
        
        # Create scraper config
        scraper_kwargs = {
            "max_depth": self.config.max_depth,
            "max_pages": self.config.max_pages,
            "allowed_domains": allowed_domains,
            "allowed_path": self.config.allowed_path,
            "allowed_paths": allowed_paths_list,
            "exclude_patterns": exclude_patterns,
            "respect_robots_txt": self.config.respect_robots_txt,
            "concurrent_requests": self.config.concurrent_requests,
            "request_delay": self.config.request_delay,
            "timeout": float(self.config.timeout),
            "follow_redirects": True,  # Always follow redirects
            "ignore_get_params": self.config.ignore_get_params,
            "check_canonical": self.config.check_canonical,
            "check_content_duplicates": self.config.check_content_duplicates,
            "check_ssrf": self.config.check_ssrf,
        }

        # Only add user_agent if it's not None
        if self.config.user_agent is not None:
            scraper_kwargs["user_agent"] = self.config.user_agent

        scraper_config = ScraperConfig(**scraper_kwargs)

        # Apply any backend-specific configuration
        if self.config.scraper_config:
            for key, value in self.config.scraper_config.items():
                if hasattr(scraper_config, key):
                    # Special handling for custom_headers to ensure it's a dict
                    if key == "custom_headers":
                        if value is None:
                            value = {}
                        elif not isinstance(value, dict):
                            logger.warning(
                                f"Invalid custom_headers type: {type(value)}, using empty dict"
                            )
                            value = {}
                    setattr(scraper_config, key, value)

        return scraper_config

    def _migrate_database_v2(self, cursor) -> None:
        """Migrate database to v2 with session support.
        
        TODO: Remove this migration after 2025-10 when all users have updated.
        Migration adds:
        - scraping_sessions table
        - session_id column to scraped_urls
        - Default session 1 for legacy data
        """
        # Check if migration is needed
        cursor.execute("SELECT name FROM sqlite_master WHERE type='table' AND name='scraping_sessions'")
        if cursor.fetchone() is not None:
            return  # Already migrated
        
        logger.info("Migrating database to v2 (adding session support)")
        
        # Create scraping_sessions table
        cursor.execute(
            """
            CREATE TABLE IF NOT EXISTS scraping_sessions (
                id INTEGER PRIMARY KEY AUTOINCREMENT,
                start_url TEXT NOT NULL,
                start_time TIMESTAMP NOT NULL,
                end_time TIMESTAMP,
                total_pages INTEGER DEFAULT 0,
                successful_pages INTEGER DEFAULT 0,
                failed_pages INTEGER DEFAULT 0,
                max_depth INTEGER,
                max_pages INTEGER,
                allowed_path TEXT,
                allowed_paths TEXT,
                excluded_paths TEXT,
                scraper_backend TEXT,
                request_delay REAL,
                concurrent_requests INTEGER,
                ignore_get_params BOOLEAN,
                check_canonical BOOLEAN,
                check_content_duplicates BOOLEAN,
                force_rescrape BOOLEAN,
                user_agent TEXT,
                timeout INTEGER,
                status TEXT DEFAULT 'running'
            )
        """
        )
        
        # Check if scraped_urls exists and needs session_id column
        cursor.execute("PRAGMA table_info(scraped_urls)")
        columns = [col[1] for col in cursor.fetchall()]
        
        if columns and 'session_id' not in columns:
            # Add session_id column
            cursor.execute("ALTER TABLE scraped_urls ADD COLUMN session_id INTEGER DEFAULT 1")
            
            # Create default session for existing data
            cursor.execute("""
                INSERT INTO scraping_sessions 
                (id, start_url, start_time, status, total_pages)
                VALUES (1, 'Legacy data (before session tracking)', 
                        COALESCE((SELECT MIN(scraped_at) FROM scraped_urls), datetime('now')),
                        'completed',
                        (SELECT COUNT(*) FROM scraped_urls WHERE error IS NULL))
            """)
            
            # Update all existing URLs to session 1
            cursor.execute("UPDATE scraped_urls SET session_id = 1 WHERE session_id IS NULL")
        
        self._db_conn.commit()
        logger.info("Database migration to v2 completed")

    def _migrate_database_v3(self, cursor) -> None:
        """Migrate database to v3 with allowed_paths column.
        
        Migration adds:
        - allowed_paths column to scraping_sessions table
        """
        # Check if migration is needed
        cursor.execute("PRAGMA table_info(scraping_sessions)")
        columns = [col[1] for col in cursor.fetchall()]
        
        if 'allowed_paths' in columns:
            return  # Already migrated
        
        logger.info("Migrating database to v3 (adding allowed_paths column)")
        
        try:
            # Add allowed_paths column
            cursor.execute("ALTER TABLE scraping_sessions ADD COLUMN allowed_paths TEXT")
            self._db_conn.commit()
            logger.info("Database migration to v3 completed")
        except Exception as e:
            logger.error(f"Failed to migrate database to v3: {e}")
            # Don't fail the entire operation, just log the error

    def _cleanup_orphaned_sessions(self) -> None:
        """Clean up sessions that were left in 'running' state from crashes or kills.
        
        Mark old running sessions as 'interrupted' if no URLs have been scraped 
        in the last hour (indicating the process died).
        """
        if not self._db_conn:
            return
            
        cursor = self._db_conn.cursor()
        
        # Find sessions that are still marked as running
        cursor.execute(
            """
            SELECT id, start_url, start_time 
            FROM scraping_sessions 
            WHERE status = 'running'
            """
        )
        
        running_sessions = cursor.fetchall()
        orphaned_sessions = []
        
        one_hour_ago = datetime.now() - timedelta(hours=1)
        
        # Check each running session to see if it's truly orphaned
        for session_id, start_url, start_time in running_sessions:
            # Get the most recent scraped URL timestamp for this session
            cursor.execute(
                """
                SELECT MAX(scraped_at) 
                FROM scraped_urls 
                WHERE session_id = ?
                """,
                (session_id,)
            )
            result = cursor.fetchone()
            last_activity = result[0] if result and result[0] else start_time
            
            # If no activity in the last hour, consider it orphaned
            # Convert string timestamp to datetime if needed
            if isinstance(last_activity, str):
                from datetime import datetime as dt
                last_activity = dt.fromisoformat(last_activity.replace('Z', '+00:00'))
            
            if last_activity < one_hour_ago:
                orphaned_sessions.append((session_id, start_url, start_time))
        
        for session_id, start_url, start_time in orphaned_sessions:
            # Get statistics for the orphaned session
            cursor.execute(
                """
                SELECT 
                    COUNT(*) as total,
                    COUNT(CASE WHEN error IS NULL THEN 1 END) as successful,
                    COUNT(CASE WHEN error IS NOT NULL THEN 1 END) as failed
                FROM scraped_urls 
                WHERE session_id = ?
                """,
                (session_id,)
            )
            result = cursor.fetchone()
            total, successful, failed = result if result else (0, 0, 0)
            
            # Mark as interrupted and update statistics
            cursor.execute(
                """
                UPDATE scraping_sessions 
                SET status = 'interrupted', 
                    end_time = ?,
                    total_pages = ?,
                    successful_pages = ?,
                    failed_pages = ?
                WHERE id = ?
                """,
                (start_time, total, successful, failed, session_id)
            )
            
            logger.warning(
                f"Cleaned up orphaned session #{session_id} from {start_time} "
                f"({successful} pages scraped before interruption)"
            )
        
        if orphaned_sessions:
            self._db_conn.commit()
            logger.info(f"Cleaned up {len(orphaned_sessions)} orphaned sessions")
        
        cursor.close()

    def _init_database(self, output_dir: Path) -> None:
        """Initialize SQLite database for tracking scraped URLs.

        Args:
            output_dir: Directory where the database will be created
        """
        self._db_path = output_dir / "scrape_tracker.db"
        self._db_conn = sqlite3.connect(str(self._db_path))

        # Create table if it doesn't exist
        cursor = self._db_conn.cursor()
        
        # Run migration if needed (TODO: Remove after 2025-10)
        self._migrate_database_v2(cursor)
        
        # Clean up any orphaned sessions from previous crashes
        self._cleanup_orphaned_sessions()
        
        # Run additional migration for allowed_paths column if needed
        self._migrate_database_v3(cursor)
        
        # Create current schema tables (if not created by migration)
        # Create scraping_sessions table
        cursor.execute(
            """
            CREATE TABLE IF NOT EXISTS scraping_sessions (
                id INTEGER PRIMARY KEY AUTOINCREMENT,
                start_url TEXT NOT NULL,
                start_time TIMESTAMP NOT NULL,
                end_time TIMESTAMP,
                total_pages INTEGER DEFAULT 0,
                successful_pages INTEGER DEFAULT 0,
                failed_pages INTEGER DEFAULT 0,
                max_depth INTEGER,
                max_pages INTEGER,
                allowed_path TEXT,
                allowed_paths TEXT,
                excluded_paths TEXT,
                scraper_backend TEXT,
                request_delay REAL,
                concurrent_requests INTEGER,
                ignore_get_params BOOLEAN,
                check_canonical BOOLEAN,
                check_content_duplicates BOOLEAN,
                force_rescrape BOOLEAN,
                user_agent TEXT,
                timeout INTEGER,
                status TEXT DEFAULT 'running'
            )
        """
        )
        
        # Create scraped_urls table with session_id
        cursor.execute(
            """
            CREATE TABLE IF NOT EXISTS scraped_urls (
                url TEXT PRIMARY KEY,
                session_id INTEGER,
                normalized_url TEXT,
                canonical_url TEXT,
                content_checksum TEXT,
                status_code INTEGER,
                target_filename TEXT,
                file_type TEXT DEFAULT 'html',
                file_size INTEGER,
                scraped_at TIMESTAMP,
                error TEXT,
                FOREIGN KEY (session_id) REFERENCES scraping_sessions(id)
            )
        """
        )

        # Create table for content checksums
        cursor.execute(
            """
            CREATE TABLE IF NOT EXISTS content_checksums (
                checksum TEXT PRIMARY KEY,
                first_url TEXT,
                first_seen TIMESTAMP
            )
        """
        )
        self._db_conn.commit()
        cursor.close()

    def _start_session(self, start_url: str) -> int:
        """Start a new scraping session.
        
        Args:
            start_url: The starting URL for this session
            
        Returns:
            The session ID
        """
        if not self._db_conn:
            return None
            
        cursor = self._db_conn.cursor()
        
        # Convert excluded_paths and allowed_paths to JSON strings if present
        import json
        excluded_paths_str = None
        if self.config.excluded_paths:
            excluded_paths_str = json.dumps(list(self.config.excluded_paths))
        
        allowed_paths_str = None
        if self.config.allowed_paths:
            allowed_paths_str = json.dumps(list(self.config.allowed_paths))
        elif self.config.allowed_path:
            allowed_paths_str = json.dumps([self.config.allowed_path])
        
        cursor.execute(
            """
            INSERT INTO scraping_sessions (
                start_url, start_time, max_depth, max_pages, allowed_path, allowed_paths,
                excluded_paths, scraper_backend, request_delay, concurrent_requests,
                ignore_get_params, check_canonical, check_content_duplicates,
                force_rescrape, user_agent, timeout, status
            ) VALUES (?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?)
            """,
            (
                start_url,
                datetime.now(),
                self.config.max_depth,
                self.config.max_pages,
                self.config.allowed_path,
                allowed_paths_str,
                excluded_paths_str,
                self.config.scraper_backend.value if self.config.scraper_backend else None,
                self.config.request_delay,
                self.config.concurrent_requests,
                self.config.ignore_get_params,
                self.config.check_canonical,
                self.config.check_content_duplicates,
                self.config.force_rescrape,
                self.config.user_agent,
                self.config.timeout,
                'running'
            )
        )
        self._db_conn.commit()
        
        self._session_id = cursor.lastrowid
        cursor.close()
        
        logger.info(f"Started scraping session #{self._session_id}")
        return self._session_id
    
    def _end_session(self, status: str = 'completed') -> None:
        """End the current scraping session.
        
        Args:
            status: The final status of the session (completed, interrupted, failed)
        """
        if not self._db_conn or not self._session_id:
            return
            
        cursor = self._db_conn.cursor()
        
        # Get counts from the current session
        cursor.execute(
            """
            SELECT 
                COUNT(*) as total,
                COUNT(CASE WHEN error IS NULL THEN 1 END) as successful,
                COUNT(CASE WHEN error IS NOT NULL THEN 1 END) as failed
            FROM scraped_urls 
            WHERE session_id = ?
            """,
            (self._session_id,)
        )
        result = cursor.fetchone()
        if result:
            total, successful, failed = result
        else:
            total, successful, failed = 0, 0, 0
        
        cursor.execute(
            """
            UPDATE scraping_sessions 
            SET end_time = ?, status = ?, total_pages = ?, 
                successful_pages = ?, failed_pages = ?
            WHERE id = ?
            """,
            (
                datetime.now(),
                status,
                total,
                successful,
                failed,
                self._session_id
            )
        )
        self._db_conn.commit()
        cursor.close()
        
        logger.info(f"Ended scraping session #{self._session_id} with status: {status}")

    def _close_database(self) -> None:
        """Close the database connection."""
        if self._db_conn:
            # End session if still running (should not happen in normal flow)
            if self._session_id:
                self._end_session('completed')
            self._db_conn.close()
            self._db_conn = None

    def _is_url_scraped(self, url: str) -> bool:
        """Check if a URL has already been scraped.

        Args:
            url: URL to check

        Returns:
            True if URL has been scraped, False otherwise
        """
        if not self._db_conn:
            return False

        cursor = self._db_conn.cursor()
        cursor.execute("SELECT 1 FROM scraped_urls WHERE url = ?", (url,))
        result = cursor.fetchone()
        cursor.close()
        return result is not None

    def _get_scraped_urls(self) -> Set[str]:
        """Get all URLs that have been scraped.

        Returns:
            Set of scraped URLs
        """
        if not self._db_conn:
            return set()

        cursor = self._db_conn.cursor()
        cursor.execute("SELECT url FROM scraped_urls")
        urls = {row[0] for row in cursor.fetchall()}
        cursor.close()
        return urls

    def _get_content_checksums(self) -> Set[str]:
        """Get all content checksums from previous scraping.

        Returns:
            Set of content checksums
        """
        if not self._db_conn:
            return set()

        cursor = self._db_conn.cursor()
        cursor.execute("SELECT checksum FROM content_checksums")
        checksums = {row[0] for row in cursor.fetchall()}
        cursor.close()
        return checksums

    def _record_content_checksum(self, checksum: str, url: str) -> None:
        """Record a content checksum in the database.

        Args:
            checksum: Content checksum
            url: First URL where this content was seen
        """
        if not self._db_conn:
            return

        cursor = self._db_conn.cursor()
        try:
            cursor.execute(
                """
                INSERT INTO content_checksums (checksum, first_url, first_seen)
                VALUES (?, ?, ?)
            """,
                (checksum, url, datetime.now()),
            )
            self._db_conn.commit()
        except sqlite3.IntegrityError:
            # Checksum already exists
            pass
        cursor.close()

    def _is_content_checksum_exists(self, checksum: str) -> bool:
        """Check if a content checksum already exists in the database.

        Args:
            checksum: Content checksum to check

        Returns:
            True if checksum exists, False otherwise
        """
        if not self._db_conn:
            return False

        cursor = self._db_conn.cursor()
        cursor.execute(
            "SELECT 1 FROM content_checksums WHERE checksum = ?", (checksum,)
        )
        result = cursor.fetchone()
        cursor.close()
        return result is not None

    def _record_scraped_url(
        self,
        url: str,
        status_code: Optional[int],
        target_filename: str,
        error: Optional[str] = None,
        normalized_url: Optional[str] = None,
        canonical_url: Optional[str] = None,
        content_checksum: Optional[str] = None,
        file_type: str = "html",
        file_size: Optional[int] = None,
    ) -> None:
        """Record a scraped URL in the database.

        Args:
            url: URL that was scraped
            status_code: HTTP status code
            target_filename: Path to the saved file
            error: Error message if scraping failed
            normalized_url: URL after GET parameter normalization
            canonical_url: Canonical URL from the page
            content_checksum: SHA-256 checksum of text content
            file_type: Type of file (html, image, pdf, etc.)
            file_size: Size of file in bytes
        """
        if not self._db_conn:
            return

        cursor = self._db_conn.cursor()
        cursor.execute(
            """
            INSERT OR REPLACE INTO scraped_urls 
            (url, session_id, normalized_url, canonical_url, content_checksum, 
             status_code, target_filename, file_type, file_size, scraped_at, error)
            VALUES (?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?)
        """,
            (
                url,
                self._session_id,
                normalized_url,
                canonical_url,
                content_checksum,
                status_code,
                target_filename,
                file_type,
                file_size,
                datetime.now(),
                error,
            ),
        )
        
        self._db_conn.commit()
        cursor.close()

    def _get_scraped_pages_info(self) -> List[Dict[str, Any]]:
        """Get information about previously scraped pages.

        Returns:
            List of dictionaries with url and target_filename
        """
        if not self._db_conn:
            return []

        cursor = self._db_conn.cursor()
        cursor.execute(
            """
            SELECT url, target_filename 
            FROM scraped_urls 
            WHERE error IS NULL AND target_filename != ''
        """
        )
        pages = [{"url": row[0], "filename": row[1]} for row in cursor.fetchall()]
        cursor.close()
        return pages

    async def crawl(self, start_url: str, output_dir: Path) -> Dict[str, Any]:
        """Crawl a website using the configured scraper backend.

        Args:
            start_url: Starting URL for crawling
            output_dir: Directory to store downloaded files

        Returns:
            Dictionary with crawl results including:
            - pages: List of scraped pages
            - total_pages: Total number of pages scraped
            - errors: List of any errors encountered

        Raises:
            Exception: If crawling fails
        """
        logger.info(
            f"Starting crawl of {start_url} using {self.config.scraper_backend} backend"
        )

        # Create output directory
        output_dir.mkdir(parents=True, exist_ok=True)

        # Parse URL to get domain for output structure
        parsed_url = urlparse(start_url)
        domain = parsed_url.netloc
        site_dir = output_dir / domain
        site_dir.mkdir(exist_ok=True)

        # Initialize database for tracking
        self._init_database(output_dir)
        
        # Start a new session
        self._start_session(start_url)

        # Check if this is a resume operation (skip if force_rescrape is enabled)
        scraped_urls = set()
        if not self.config.force_rescrape:
            scraped_urls = self._get_scraped_urls()
            if scraped_urls:
                logger.info(
                    f"Resuming crawl - found {len(scraped_urls)} previously scraped URLs"
                )
        else:
            logger.info("Force rescrape enabled - ignoring database cache")

        # Create scraper instance
        backend_name = self.config.scraper_backend.value
        scraper = create_scraper(backend_name, self._scraper_config)

        pages = []
        errors = []

        try:
            async with scraper:
                # Pass already scraped URLs to the scraper if it supports it
                if hasattr(scraper, "_visited_urls"):
                    scraper._visited_urls.update(scraped_urls)

                # Set up checksum callback if deduplication is enabled
                if self._scraper_config.check_content_duplicates:
                    # Pass the database connection to the scraper for checksum queries
                    if hasattr(scraper, "set_checksum_callback"):
                        scraper.set_checksum_callback(self._is_content_checksum_exists)
                        logger.info("Enabled database-backed content deduplication")

                # Pass information about scraped pages for resume functionality
                if scraped_urls and hasattr(scraper, "set_resume_info"):
                    pages_info = self._get_scraped_pages_info()
                    resume_info = []
                    for page_info in pages_info[:20]:  # Read first 20 pages for links
                        try:
                            file_path = output_dir / page_info["filename"]
                            if file_path.exists():
                                # Only read HTML files, skip binary files (images, PDFs, etc.)
                                if file_path.suffix.lower() in ['.html', '.htm', '.xhtml']:
                                    content = file_path.read_text(encoding="utf-8")
                                    resume_info.append(
                                        {"url": page_info["url"], "content": content}
                                    )
                                else:
                                    logger.debug(f"Skipping binary file for resume: {page_info['filename']}")
                        except Exception as e:
                            logger.warning(
                                f"Failed to read {page_info['filename']}: {e}"
                            )

                    if resume_info:
                        scraper.set_resume_info(resume_info)

                async for page in scraper.scrape_site(start_url):
                    # Skip if already scraped (unless force_rescrape is enabled)
                    if not self.config.force_rescrape and self._is_url_scraped(page.url):
                        logger.info(f"Skipping already scraped URL: {page.url}")
                        continue

                    # Log progress - show current URL being scraped
                    pages.append(page)
                    logger.info(f"Processing: {page.url} (page {len(pages)})")

                    # Save page to disk
                    try:
                        file_path = await self._save_page(page, site_dir)
                        # Record successful scrape with all metadata
                        self._record_scraped_url(
                            page.url,
                            page.status_code,
                            str(file_path.relative_to(output_dir)),
                            error=None,
                            normalized_url=page.normalized_url,
                            canonical_url=page.canonical_url,
                            content_checksum=page.content_checksum,
                            file_type=page.file_type,
                            file_size=page.file_size,
                        )
                        # Record content checksum if present
                        if (
                            page.content_checksum
                            and self._scraper_config.check_content_duplicates
                        ):
                            self._record_content_checksum(
                                page.content_checksum, page.url
                            )
                        
                        # Extract and download assets if enabled
                        if self.config.download_assets and not page.is_binary:
                            asset_urls = scraper.extract_asset_urls(
                                page.content, page.url, self.config.asset_types
                            )
                            
                            # Security: Limit assets per page (if configured)
                            if self.config.max_assets_per_page > 0 and len(asset_urls) > self.config.max_assets_per_page:
                                logger.warning(
                                    f"Page {page.url} has {len(asset_urls)} assets, "
                                    f"limiting to {self.config.max_assets_per_page}"
                                )
                                asset_urls = list(asset_urls)[:self.config.max_assets_per_page]
                            
                            logger.info(f"Found {len(asset_urls)} assets on {page.url}")
                            
                            # Track total assets downloaded
                            if not hasattr(self, '_total_assets_downloaded'):
                                self._total_assets_downloaded = 0
                            
                            # Track downloaded assets for this page
                            downloaded_assets = {}
                            
                            # Prepare list of assets to download
                            assets_to_download = []
                            for asset_url in asset_urls:
                                # Security: Check total assets limit (if configured)
                                if self.config.total_assets_limit > 0 and self._total_assets_downloaded + len(assets_to_download) >= self.config.total_assets_limit:
                                    logger.warning(
                                        f"Reached total assets limit of {self.config.total_assets_limit}, "
                                        f"skipping remaining assets"
                                    )
                                    break
                                
                                # Skip if already downloaded
                                if not self.config.force_rescrape and self._is_url_scraped(asset_url):
                                    logger.debug(f"Skipping already downloaded asset: {asset_url}")
                                    # Still track it for HTML update if we can find its path
                                    asset_info = self._get_scraped_url_info(asset_url)
                                    if asset_info and asset_info.get('target_filename'):
                                        downloaded_assets[asset_url] = output_dir / asset_info['target_filename']
                                    continue
                                
                                assets_to_download.append(asset_url)
                            
                            # Download assets concurrently for better performance
                            if assets_to_download:
                                logger.info(f"Downloading {len(assets_to_download)} assets concurrently")
                                
                                async def download_and_save_asset(asset_url):
                                    """Download and save a single asset."""
                                    try:
                                        logger.debug(f"Downloading asset: {asset_url}")
                                        asset_page = await scraper.download_binary_file(
                                            asset_url, self.config.max_asset_size
                                        )
                                        
                                        if asset_page:
                                            try:
                                                asset_path = await self._save_page(asset_page, site_dir)
                                                self._record_scraped_url(
                                                    asset_page.url,
                                                    asset_page.status_code,
                                                    str(asset_path.relative_to(output_dir)),
                                                    error=None,
                                                    file_type=asset_page.file_type,
                                                    file_size=asset_page.file_size,
                                                )
                                                logger.debug(f"Saved asset {asset_url} to {asset_path}")
                                                return (asset_url, asset_path)
                                            except ValueError as e:
                                                # Security exception (dangerous file, path traversal, etc.)
                                                logger.error(f"Security: Blocked asset {asset_url}: {e}")
                                            except Exception as e:
                                                logger.error(f"Failed to save asset {asset_url}: {e}")
                                        else:
                                            logger.warning(f"Failed to download asset: {asset_url}")
                                        return None
                                    except Exception as e:
                                        logger.error(f"Error downloading asset {asset_url}: {e}")
                                        return None
                                
                                # Use concurrent requests config to limit parallel downloads
                                # But cap at a reasonable number for assets (default 5, max 10)
                                max_concurrent_assets = min(
                                    self.config.concurrent_requests if hasattr(self.config, 'concurrent_requests') else 5,
                                    10
                                )
                                
                                # Download assets in batches to avoid overwhelming the server
                                for i in range(0, len(assets_to_download), max_concurrent_assets):
                                    batch = assets_to_download[i:i + max_concurrent_assets]
                                    download_tasks = [download_and_save_asset(url) for url in batch]
                                    results = await asyncio.gather(*download_tasks, return_exceptions=True)
                                    
                                    for result in results:
                                        if isinstance(result, tuple) and result:
                                            asset_url, asset_path = result
                                            downloaded_assets[asset_url] = asset_path
                                            self._total_assets_downloaded += 1
                                        elif isinstance(result, Exception):
                                            logger.error(f"Asset download task failed: {result}")
                            
                            # Update HTML file with correct asset paths if we downloaded any
                            if downloaded_assets:
                                await self._update_html_with_asset_paths(file_path, page, site_dir, downloaded_assets)
                    except Exception as e:
                        logger.error(f"Failed to save page {page.url}: {e}")
                        errors.append({"url": page.url, "error": str(e)})
                        # Record failed scrape
                        self._record_scraped_url(
                            page.url,
                            page.status_code if hasattr(page, "status_code") else None,
                            "",
                            str(e),
                        )
                        # Continue with other pages despite the error

        except Exception as e:
            logger.error(f"Crawl failed: {e}")
            if self._session_id:
                self._end_session('failed')
            raise
        finally:
            # End session with completed status if not already ended
            if self._session_id and self._db_conn:
                self._end_session('completed')
            # Always close database connection
            self._close_database()

        logger.info(
            f"Crawl completed. Scraped {len(pages)} pages with {len(errors)} errors"
        )

        return {
            "pages": pages,
            "total_pages": len(pages),
            "pages_scraped": len(pages),  # For compatibility
            "errors": errors,
            "output_dir": site_dir,
        }

    def _get_scraped_url_info(self, url: str) -> Optional[Dict[str, Any]]:
        """Get information about a previously scraped URL.
        
        Args:
            url: The URL to look up
            
        Returns:
            Dictionary with url info or None if not found
        """
        if not self._db_conn:
            return None
        
        try:
            cursor = self._db_conn.cursor()
            cursor.execute(
                "SELECT target_filename, status_code, error, scraped_at FROM scraped_urls WHERE url = ?",
                (url,)
            )
            row = cursor.fetchone()
            if row:
                return {
                    'target_filename': row[0],
                    'status_code': row[1],
                    'error': row[2],
                    'scraped_at': row[3]
                }
        except Exception as e:
            logger.error(f"Failed to get scraped URL info: {e}")
        
        return None
    
    async def _update_html_with_asset_paths(self, html_path: Path, page: ScrapedPage, site_dir: Path, downloaded_assets: Dict[str, Path]):
        """Update HTML file with correct paths to downloaded assets.
        
        Args:
            html_path: Path to the HTML file to update
            page: The original ScrapedPage object
            site_dir: The site directory
            downloaded_assets: Map of asset URLs to their local paths
        """
        try:
            # Read the current HTML content
            html_content = html_path.read_text(encoding=page.encoding)
            
            # Adjust links with the downloaded asset paths
            updated_content = self._adjust_html_links(
                html_content,
                page.url,
                html_path,
                site_dir,
                downloaded_assets
            )
            
            # Write the updated content back
            html_path.write_text(updated_content, encoding=page.encoding)
            logger.debug(f"Updated HTML file {html_path} with {len(downloaded_assets)} asset paths")
            
        except Exception as e:
            logger.error(f"Failed to update HTML with asset paths: {e}")
    
    def _adjust_html_links(self, html_content: str, original_url: str, saved_path: Path, site_dir: Path, downloaded_assets: Dict[str, Path] = None) -> str:
        """Adjust relative links in HTML content to work from the new saved location.
        
        Args:
            html_content: The HTML content to adjust
            original_url: The original URL of the page
            saved_path: Where the file will be saved
            site_dir: The site directory (domain folder)
            downloaded_assets: Optional mapping of asset URLs to their local paths
            
        Returns:
            HTML content with adjusted links
        """
        import os
        from bs4 import BeautifulSoup
        from urllib.parse import urljoin, urlparse
        
        soup = BeautifulSoup(html_content, 'html.parser')
        original_parsed = urlparse(original_url)
        
        # Calculate the relative path from saved location to site root
        try:
            rel_to_root = os.path.relpath(site_dir, saved_path.parent)
            if rel_to_root == '.':
                rel_to_root = ''
            else:
                rel_to_root = rel_to_root.replace('\\', '/') + '/'
        except ValueError:
            # If on different drives on Windows
            rel_to_root = ''
        
        # Adjust all links
        for tag_name, attr_name in [('a', 'href'), ('link', 'href'), ('script', 'src'), ('img', 'src'), 
                                     ('source', 'srcset'), ('source', 'src'), ('video', 'src'), ('audio', 'src')]:
            for tag in soup.find_all(tag_name):
                attr_value = tag.get(attr_name)
                if not attr_value:
                    continue
                    
                # Skip anchors and special protocols (but not http/https - we might have downloaded them)
                if attr_value.startswith(('#', 'mailto:', 'javascript:', 'data:')):
                    continue
                
                # Check if this is an asset we downloaded
                if downloaded_assets and self.config.download_assets:
                    # Build the absolute URL for this attribute
                    if attr_value.startswith(('http://', 'https://', '//')):
                        absolute_url = attr_value
                        if attr_value.startswith('//'):
                            absolute_url = original_parsed.scheme + ':' + attr_value
                    else:
                        # Relative URL - resolve it against the original page URL
                        absolute_url = urljoin(original_url, attr_value)
                    
                    # Check if we downloaded this asset
                    if absolute_url in downloaded_assets:
                        # Replace with path to downloaded asset
                        asset_path = downloaded_assets[absolute_url]
                        # Calculate relative path from HTML file to asset
                        try:
                            rel_path = os.path.relpath(asset_path, saved_path.parent)
                            rel_path = rel_path.replace('\\', '/')
                            tag[attr_name] = rel_path
                            continue  # Skip normal link adjustment for this asset
                        except ValueError:
                            # Different drives on Windows, use the original logic
                            pass
                
                # Skip absolute URLs after checking for downloaded assets
                if attr_value.startswith(('http://', 'https://', '//')):
                    continue
                
                # Handle relative URLs
                if attr_value.startswith('/'):
                    # Absolute path - make it relative to site root
                    new_path = rel_to_root + attr_value.lstrip('/')
                    tag[attr_name] = new_path
                elif attr_value.startswith('./'):
                    # Relative to current directory - need to adjust based on original URL structure
                    # Get the original directory path
                    orig_path_parts = original_parsed.path.rstrip('/').split('/')
                    if orig_path_parts[-1] and ('.' in orig_path_parts[-1] or not orig_path_parts[-1]):
                        # Last part is a file, remove it
                        orig_path_parts = orig_path_parts[:-1]
                    
                    # Reconstruct the path
                    relative_part = attr_value[2:]  # Remove ./
                    if orig_path_parts:
                        # The link was relative to /magazin/ or similar
                        new_path = rel_to_root + '/'.join(orig_path_parts[1:]) + '/' + relative_part
                    else:
                        new_path = rel_to_root + relative_part
                    
                    # Clean up the path
                    new_path = new_path.replace('//', '/')
                    tag[attr_name] = new_path
                elif attr_value.startswith('../'):
                    # Handle parent directory references
                    # This is complex, so for now just make it relative to root
                    cleaned = attr_value
                    levels_up = 0
                    while cleaned.startswith('../'):
                        levels_up += 1
                        cleaned = cleaned[3:]
                    
                    orig_path_parts = original_parsed.path.rstrip('/').split('/')
                    if orig_path_parts[-1] and '.' in orig_path_parts[-1]:
                        orig_path_parts = orig_path_parts[:-1]
                    
                    # Go up the required number of levels
                    if len(orig_path_parts) > levels_up:
                        base_parts = orig_path_parts[1:-levels_up] if levels_up > 0 else orig_path_parts[1:]
                        if base_parts:
                            new_path = rel_to_root + '/'.join(base_parts) + '/' + cleaned
                        else:
                            new_path = rel_to_root + cleaned
                    else:
                        new_path = rel_to_root + cleaned
                    
                    new_path = new_path.replace('//', '/')
                    tag[attr_name] = new_path
        
        return str(soup)

    async def _save_page(self, page: ScrapedPage, output_dir: Path) -> Path:
        """Save a scraped page to disk with adjusted links.

        Args:
            page: ScrapedPage instance
            output_dir: Directory to save the page

        Returns:
            Path to saved file
        """
        # Handle binary files differently
        if page.is_binary and page.binary_content:
            return await self._save_binary_file(page, output_dir)
        
        # Parse URL to create file path
        parsed = urlparse(page.url)

        # Create subdirectories based on URL path
        if parsed.path and parsed.path != "/":
            # Remove leading slash and split path
            path_parts = parsed.path.lstrip("/").split("/")

            # Handle file extension
            if path_parts[-1].endswith(".html") or "." in path_parts[-1]:
                filename = path_parts[-1]
                subdirs = path_parts[:-1]
            else:
                # Assume it's a directory, create index.html
                filename = "index.html"
                subdirs = path_parts

            # Create subdirectories with path validation
            if subdirs:
                # Sanitize subdirectory names to prevent path traversal
                safe_subdirs = []
                for part in subdirs:
                    # Remove any path traversal attempts
                    safe_part = (
                        part.replace("..", "").replace("./", "").replace("\\", "")
                    )
                    if safe_part and safe_part not in (".", ".."):
                        safe_subdirs.append(safe_part)

                if safe_subdirs:
                    subdir = output_dir / Path(*safe_subdirs)
                    subdir.mkdir(parents=True, exist_ok=True)
                    file_path = subdir / filename
                else:
                    file_path = output_dir / filename
            else:
                file_path = output_dir / filename
        else:
            # Root page
            file_path = output_dir / "index.html"

        # Ensure .html extension
        if not file_path.suffix:
            file_path = file_path.with_suffix(".html")
        elif file_path.suffix not in (".html", ".htm"):
            file_path = file_path.with_name(f"{file_path.name}.html")

        # Adjust links in HTML content before saving
        adjusted_content = self._adjust_html_links(
            page.content, 
            page.url, 
            file_path,
            output_dir.parent  # This is the site_dir (domain folder)
        )

        # Write content
        file_path.parent.mkdir(parents=True, exist_ok=True)
        file_path.write_text(adjusted_content, encoding=page.encoding)

        logger.debug(f"Saved {page.url} to {file_path}")

        # Save metadata if available
        try:
            metadata_path = file_path.with_suffix(".meta.json")
            import json

            metadata = {
                "url": page.url,
                "title": page.title,
                "encoding": page.encoding,
                "status_code": page.status_code,
                "headers": page.headers if page.headers else {},
                "metadata": page.metadata if page.metadata else {},
            }
            # Filter out None values and ensure all keys are strings
            clean_metadata = {}
            for k, v in metadata.items():
                if v is not None:
                    if isinstance(v, dict):
                        # Clean nested dictionaries - ensure no None keys
                        clean_v = {}
                        for sub_k, sub_v in v.items():
                            if sub_k is not None:
                                clean_v[str(sub_k)] = sub_v
                        clean_metadata[k] = clean_v
                    else:
                        clean_metadata[k] = v

            metadata_path.write_text(json.dumps(clean_metadata, indent=2, default=str))
        except Exception as e:
            logger.warning(f"Failed to save metadata for {page.url}: {e}")
            # Don't fail the entire page save just because metadata failed

        return file_path

    async def _save_binary_file(self, page: ScrapedPage, output_dir: Path) -> Path:
        """Save binary file to disk with security checks.
        
        Args:
            page: ScrapedPage with binary content
            output_dir: Directory to save the file
            
        Returns:
            Path to saved file
            
        Raises:
            ValueError: If file path is unsafe or file type is dangerous
        """
        import hashlib
        import re
        import os
        
        # Security: Define dangerous file extensions that should never be saved
        DANGEROUS_EXTENSIONS = {
            '.exe', '.dll', '.bat', '.cmd', '.com', '.scr', '.vbs', '.vbe',
            '.js', '.jse', '.wsf', '.wsh', '.ps1', '.psm1', '.msi', '.jar',
            '.app', '.deb', '.rpm', '.dmg', '.pkg', '.sh', '.bash', '.zsh',
            '.fish', '.ksh', '.csh', '.tcsh', '.py', '.pyc', '.pyo', '.pyw',
            '.rb', '.pl', '.php', '.asp', '.aspx', '.jsp', '.cgi'
        }
        
        # Parse URL to create file path
        parsed = urlparse(page.url)
        
        # Create assets subdirectory if configured
        assets_dir = output_dir / self.config.assets_subdirectory
        assets_dir.mkdir(parents=True, exist_ok=True)
        
        # Create file path from URL
        if parsed.path and parsed.path != "/":
            path_parts = parsed.path.lstrip("/").split("/")
            filename = path_parts[-1]
            subdirs = path_parts[:-1] if len(path_parts) > 1 else []
            
            # Security: Check for dangerous extensions
            file_ext = Path(filename).suffix.lower()
            if file_ext in DANGEROUS_EXTENSIONS:
                logger.warning(f"Blocked dangerous file type {file_ext}: {page.url}")
                raise ValueError(f"Dangerous file type {file_ext} not allowed")
            
            # Security: Sanitize filename more aggressively
            # Remove any character that could be used for path traversal or command injection
            safe_filename = re.sub(r'[^a-zA-Z0-9._-]', '_', filename)
            safe_filename = safe_filename.lstrip('.')  # Remove leading dots
            
            if not safe_filename:
                safe_filename = f"asset_{hashlib.md5(page.url.encode()).hexdigest()[:8]}"
            
            # Create subdirectories with strict sanitization
            if subdirs:
                safe_subdirs = []
                for part in subdirs:
                    # Security: More aggressive sanitization
                    safe_part = re.sub(r'[^a-zA-Z0-9._-]', '_', part)
                    safe_part = safe_part.strip('._')  # Remove leading/trailing dots and underscores
                    if safe_part and safe_part not in (".", ".."):
                        safe_subdirs.append(safe_part)
                
                if safe_subdirs:
                    subdir = assets_dir / Path(*safe_subdirs)
                    subdir.mkdir(parents=True, exist_ok=True)
                    file_path = subdir / safe_filename  # Use safe_filename instead of filename
                else:
                    file_path = assets_dir / safe_filename  # Use safe_filename
            else:
                file_path = assets_dir / safe_filename  # Use safe_filename
        else:
            # Use URL hash for files without clear names
            url_hash = hashlib.md5(page.url.encode()).hexdigest()[:8]
            extension = ""
            if page.file_type == "image":
                # Try to determine extension from content type
                content_type = page.headers.get("content-type", "")
                if "jpeg" in content_type or "jpg" in content_type:
                    extension = ".jpg"
                elif "png" in content_type:
                    extension = ".png"
                elif "gif" in content_type:
                    extension = ".gif"
                elif "webp" in content_type:
                    extension = ".webp"
                elif "svg" in content_type:
                    extension = ".svg"
            elif page.file_type == "pdf":
                extension = ".pdf"
            
            # Security: Check extension even for generated filenames
            if extension.lower() in DANGEROUS_EXTENSIONS:
                logger.warning(f"Blocked dangerous file type {extension}: {page.url}")
                raise ValueError(f"Dangerous file type {extension} not allowed")
            
            file_path = assets_dir / f"asset_{url_hash}{extension}"
        
        # Security: Final path validation - ensure file_path is within output_dir
        try:
            # Don't use resolve() on non-existent paths - it can cause issues
            # Instead, check the relative path components
            import os
            output_dir_abs = output_dir.resolve()
            
            # Get the relative path from output_dir to file_path
            try:
                rel_path = os.path.relpath(file_path, output_dir)
                # Check if path tries to escape (contains ..)
                if ".." in rel_path:
                    raise ValueError(f"Path traversal attempt detected: {rel_path}")
            except ValueError:
                # os.path.relpath raises ValueError if on different drives on Windows
                raise ValueError(f"Invalid file path: {file_path}")
                
        except Exception as e:
            logger.error(f"Path validation failed for {page.url}: {e}")
            raise ValueError(f"Invalid file path: {e}")
        
        # Security: Check file size before writing
        if page.file_size and page.file_size > self.config.max_asset_size:
            raise ValueError(f"File size {page.file_size} exceeds limit {self.config.max_asset_size}")
        
        # Write binary content
        file_path.parent.mkdir(parents=True, exist_ok=True)
        file_path.write_bytes(page.binary_content)
        
        # Save metadata
        try:
            metadata_path = file_path.with_suffix(file_path.suffix + ".meta.json")
            metadata = {
                "url": page.url,
                "file_type": page.file_type,
                "file_size": page.file_size,
                "status_code": page.status_code,
                "headers": page.headers or {},
                "checksum": hashlib.sha256(page.binary_content).hexdigest(),
            }
            
            # Add validation results if available
            if page.metadata and 'validation' in page.metadata:
                metadata['validation'] = page.metadata['validation']
            
            metadata_path.write_text(json.dumps(metadata, indent=2, default=str))
        except Exception as e:
            logger.warning(f"Failed to save metadata for binary file {page.url}: {e}")
        
        logger.debug(f"Saved binary file {page.url} to {file_path}")
        return file_path

    def find_downloaded_files(self, site_dir: Path) -> List[Path]:
        """Find all HTML files in the output directory.

        Args:
            site_dir: Directory containing downloaded files

        Returns:
            List of HTML file paths
        """
        if not site_dir.exists():
            logger.warning(f"Site directory does not exist: {site_dir}")
            return []

        html_files = []

        # Find all HTML files
        patterns = ["*.html", "*.htm"]
        for pattern in patterns:
            files = list(site_dir.rglob(pattern))
            html_files.extend(files)

        # Filter out metadata files
        filtered_files = [f for f in html_files if not f.name.endswith(".meta.json")]

        logger.info(f"Found {len(filtered_files)} HTML files in {site_dir}")
        return sorted(filtered_files)

    def crawl_sync(self, start_url: str, output_dir: Path) -> Path:
        """Synchronous version of crawl method.

        Args:
            start_url: Starting URL for crawling
            output_dir: Directory to store downloaded files

        Returns:
            Path to site directory
        """
        try:
            # Run async crawl using asyncio.run()
            result = asyncio.run(self.crawl(start_url, output_dir))
            return result["output_dir"]
        except KeyboardInterrupt:
            # Mark session as interrupted before re-raising
            if self._session_id:
                try:
                    self._end_session('interrupted')
                except:
                    pass  # Don't let DB errors mask the interrupt
            # Re-raise to let CLI handle it gracefully
            raise

    def crawl_sync_with_stats(self, start_url: str, output_dir: Path) -> Dict[str, Any]:
        """Synchronous version of crawl method that returns detailed statistics.

        Args:
            start_url: Starting URL for crawling
            output_dir: Directory to store downloaded files

        Returns:
            Dictionary containing:
            - site_dir: Path to site directory
            - scraped_urls: List of URLs scraped in this session
            - errors: List of errors encountered in this session
            - total_pages: Total number of pages scraped in this session
            - session_files: List of files created in this session
            - session_id: ID of this scraping session
        """
        try:
            # Run async crawl using asyncio.run()
            result = asyncio.run(self.crawl(start_url, output_dir))
            
            # Extract URLs from the pages scraped in this session
            pages = result.get("pages", [])
            scraped_urls = [page.url for page in pages]
            
            # Get list of files created in this session
            session_files = []
            for page in pages:
                # Reconstruct the file path for each scraped page using same logic as _save_page
                parsed = urlparse(page.url)
                # Note: result["output_dir"] already contains the domain (it's actually site_dir)
                site_dir = result["output_dir"]
                
                # Same logic as in _save_page method
                if parsed.path and parsed.path != "/":
                    path_parts = parsed.path.lstrip("/").split("/")
                    
                    # Handle file extension
                    if path_parts[-1].endswith(".html") or "." in path_parts[-1]:
                        filename = path_parts[-1]
                        subdirs = path_parts[:-1]
                    else:
                        # Assume it's a directory, create index.html
                        filename = "index.html"
                        subdirs = path_parts
                    
                    # Build the file path
                    if subdirs:
                        # Sanitize subdirectory names (same as _save_page)
                        safe_subdirs = []
                        for part in subdirs:
                            safe_part = part.replace("..", "").replace("./", "").replace("\\", "")
                            if safe_part and safe_part not in (".", ".."):
                                safe_subdirs.append(safe_part)
                        
                        if safe_subdirs:
                            file_path = site_dir / Path(*safe_subdirs) / filename
                        else:
                            file_path = site_dir / filename
                    else:
                        file_path = site_dir / filename
                else:
                    # Root page
                    file_path = site_dir / "index.html"
                
                if file_path.exists():
                    session_files.append(file_path)
            
            return {
                "site_dir": result["output_dir"],
                "scraped_urls": scraped_urls,
                "errors": result.get("errors", []),
                "total_pages": len(pages),
                "pages_scraped": result.get("pages_scraped", len(pages)),
                "session_files": session_files,
                "session_id": self._session_id,
            }
        except KeyboardInterrupt:
            # Mark session as interrupted before re-raising
            if self._session_id:
                try:
                    self._end_session('interrupted')
                except:
                    pass  # Don't let DB errors mask the interrupt
            # Re-raise to let CLI handle it gracefully
            raise

======= scrape_tool/file_validator.py ======
#!/usr/bin/env python3
# Copyright 2025 Franz und Franz GmbH
#
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#     http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.

"""File validation for downloaded assets using magic numbers and basic checks."""

import logging
from typing import Optional, Dict, Tuple
from pathlib import Path

logger = logging.getLogger(__name__)


class FileValidator:
    """Validates downloaded files by checking magic numbers and basic structure."""
    
    # Magic numbers (file signatures) for common file types
    # Format: file_extension -> (offset, bytes_to_match, description)
    MAGIC_NUMBERS = {
        # Images
        '.jpg': [(0, b'\xFF\xD8\xFF', 'JPEG image')],
        '.jpeg': [(0, b'\xFF\xD8\xFF', 'JPEG image')],
        '.png': [(0, b'\x89PNG\r\n\x1a\n', 'PNG image')],
        '.gif': [
            (0, b'GIF87a', 'GIF87a image'),
            (0, b'GIF89a', 'GIF89a image')
        ],
        '.webp': [(8, b'WEBP', 'WebP image')],  # RIFF....WEBP
        '.ico': [
            (0, b'\x00\x00\x01\x00', 'ICO icon'),
            (0, b'\x00\x00\x02\x00', 'CUR cursor')
        ],
        '.svg': [(0, b'<svg', 'SVG image'), (0, b'<?xml', 'SVG/XML image')],
        '.bmp': [(0, b'BM', 'BMP image')],
        
        # Documents
        '.pdf': [(0, b'%PDF-', 'PDF document')],
        '.doc': [(0, b'\xD0\xCF\x11\xE0\xA1\xB1\x1A\xE1', 'MS Office document')],
        '.docx': [(0, b'PK\x03\x04', 'Office Open XML')],  # Actually a ZIP
        '.xls': [(0, b'\xD0\xCF\x11\xE0\xA1\xB1\x1A\xE1', 'MS Office document')],
        '.xlsx': [(0, b'PK\x03\x04', 'Office Open XML')],
        '.ppt': [(0, b'\xD0\xCF\x11\xE0\xA1\xB1\x1A\xE1', 'MS Office document')],
        '.pptx': [(0, b'PK\x03\x04', 'Office Open XML')],
        
        # Archives
        '.zip': [(0, b'PK\x03\x04', 'ZIP archive'), (0, b'PK\x05\x06', 'Empty ZIP')],
        '.gz': [(0, b'\x1f\x8b', 'GZIP archive')],
        '.tar': [(257, b'ustar', 'TAR archive')],
        '.rar': [
            (0, b'Rar!\x1A\x07\x00', 'RAR v1.5+'),
            (0, b'Rar!\x1A\x07\x01\x00', 'RAR v5+')
        ],
        '.7z': [(0, b'7z\xBC\xAF\x27\x1C', '7-Zip archive')],
        
        # Fonts
        '.ttf': [
            (0, b'\x00\x01\x00\x00', 'TrueType font'),
            (0, b'true', 'TrueType font'),
            (0, b'OTTO', 'OpenType font')
        ],
        '.otf': [(0, b'OTTO', 'OpenType font')],
        '.woff': [(0, b'wOFF', 'WOFF font')],
        '.woff2': [(0, b'wOF2', 'WOFF2 font')],
        '.eot': [(0, b'\x00\x00\x00\x00', 'EOT font')],  # Less reliable
        
        # Media
        '.mp4': [(4, b'ftyp', 'MP4 video')],
        '.webm': [(0, b'\x1A\x45\xDF\xA3', 'WebM video')],
        '.mp3': [
            (0, b'ID3', 'MP3 with ID3'),
            (0, b'\xFF\xFB', 'MP3 audio'),
            (0, b'\xFF\xF3', 'MP3 audio'),
            (0, b'\xFF\xF2', 'MP3 audio')
        ],
        '.wav': [(0, b'RIFF', 'WAV audio'), (8, b'WAVE', 'WAV audio')],
        '.ogg': [(0, b'OggS', 'OGG media')],
        
        # Text/Code (these don't have magic numbers, need content check)
        '.html': None,
        '.htm': None,
        '.css': None,
        '.js': None,
        '.json': None,
        '.xml': None,
        '.txt': None,
        '.md': None,
        '.csv': None,
    }
    
    @classmethod
    def validate_file(cls, content: bytes, file_extension: str, 
                      content_type: Optional[str] = None) -> Dict[str, any]:
        """Validate file content against expected format.
        
        Args:
            content: Binary content of the file
            file_extension: File extension (with dot, e.g., '.jpg')
            content_type: Optional HTTP Content-Type header
            
        Returns:
            Dictionary with validation results:
            - valid: Boolean indicating if file is valid
            - format_match: Boolean if magic number matches
            - detected_type: Detected file type
            - warnings: List of warning messages
            - error: Error message if validation failed
        """
        result = {
            'valid': False,
            'format_match': False,
            'detected_type': None,
            'warnings': [],
            'error': None,
            'file_size': len(content),
        }
        
        # Check minimum size
        if len(content) == 0:
            result['error'] = 'Empty file'
            return result
        
        # Normalize extension
        ext = file_extension.lower() if file_extension else ''
        if not ext.startswith('.'):
            ext = '.' + ext
        
        # Check if we have magic numbers for this type
        if ext not in cls.MAGIC_NUMBERS:
            result['warnings'].append(f'Unknown file type: {ext}')
            result['valid'] = True  # Assume valid if we don't know the type
            return result
        
        magic_specs = cls.MAGIC_NUMBERS[ext]
        
        # Text files don't have magic numbers
        if magic_specs is None:
            return cls._validate_text_file(content, ext, result)
        
        # Check magic numbers
        for offset, expected_bytes, description in magic_specs:
            if cls._check_magic_number(content, offset, expected_bytes):
                result['format_match'] = True
                result['detected_type'] = description
                result['valid'] = True
                break
        
        if not result['format_match']:
            # Try to detect actual type
            actual_type = cls._detect_file_type(content)
            if actual_type:
                result['error'] = f'File extension {ext} does not match content (detected: {actual_type})'
                result['detected_type'] = actual_type
            else:
                result['error'] = f'Invalid {ext} file: magic number mismatch'
        
        # Additional validation for specific types
        if result['valid']:
            if ext in ['.jpg', '.jpeg', '.png', '.gif', '.webp']:
                cls._validate_image_structure(content, ext, result)
            elif ext == '.pdf':
                cls._validate_pdf_structure(content, result)
            elif ext in ['.zip', '.gz', '.tar', '.rar', '.7z']:
                cls._validate_archive_structure(content, ext, result)
        
        # Check content type mismatch
        if content_type and result['valid']:
            cls._check_content_type_match(ext, content_type, result)
        
        return result
    
    @staticmethod
    def _check_magic_number(content: bytes, offset: int, expected: bytes) -> bool:
        """Check if content matches expected bytes at offset."""
        if len(content) < offset + len(expected):
            return False
        return content[offset:offset + len(expected)] == expected
    
    @classmethod
    def _detect_file_type(cls, content: bytes) -> Optional[str]:
        """Try to detect actual file type from content."""
        # Check against all known magic numbers
        for ext, magic_specs in cls.MAGIC_NUMBERS.items():
            if magic_specs is None:
                continue
            for offset, expected_bytes, description in magic_specs:
                if cls._check_magic_number(content, offset, expected_bytes):
                    return f'{description} ({ext})'
        return None
    
    @classmethod
    def _validate_text_file(cls, content: bytes, ext: str, result: Dict) -> Dict:
        """Validate text-based files."""
        try:
            # Try to decode as UTF-8
            text = content.decode('utf-8')
            result['valid'] = True
            result['format_match'] = True
            
            # Specific checks for different text types
            if ext == '.json':
                import json
                try:
                    json.loads(text)
                    result['detected_type'] = 'JSON document'
                except json.JSONDecodeError as e:
                    result['warnings'].append(f'Invalid JSON: {str(e)[:100]}')
                    result['valid'] = False
                    
            elif ext in ['.html', '.htm']:
                result['detected_type'] = 'HTML document'
                cls._validate_html_content(text, result)
                
            elif ext == '.xml' or ext == '.svg':
                if not text.strip().startswith('<'):
                    result['warnings'].append('XML/SVG should start with <')
                result['detected_type'] = 'XML document'
                
            elif ext == '.css':
                # Basic CSS check - look for common patterns
                css_patterns = ['{', '}', ':', ';', '/*', '*/']
                if not any(p in text for p in css_patterns):
                    result['warnings'].append('File does not appear to contain CSS')
                result['detected_type'] = 'CSS stylesheet'
                
            elif ext == '.js':
                # Basic JavaScript check
                js_keywords = ['function', 'var', 'let', 'const', 'return', 
                              'if', 'for', 'while', '=>', '=', '(', ')']
                if not any(kw in text for kw in js_keywords):
                    result['warnings'].append('File does not appear to contain JavaScript')
                result['detected_type'] = 'JavaScript code'
                
            elif ext == '.csv':
                # Check for comma/tab/semicolon separators
                if ',' not in text and '\t' not in text and ';' not in text:
                    result['warnings'].append('CSV file has no visible delimiters')
                result['detected_type'] = 'CSV data'
                
            else:
                result['detected_type'] = 'Plain text'
                
        except UnicodeDecodeError:
            result['error'] = f'Invalid {ext} file: not valid UTF-8 text'
            result['valid'] = False
            
        return result
    
    @classmethod
    def _validate_html_content(cls, text: str, result: Dict):
        """Validate HTML content and check for inline binaries."""
        import re
        
        # Check basic HTML structure
        text_lower = text.lower()
        
        # Check for HTML tags
        if '<html' not in text_lower and '<!doctype' not in text_lower:
            # May be a fragment, still valid
            if not re.search(r'<[^>]+>', text):
                result['warnings'].append('No HTML tags found')
                result['valid'] = False
                return
        
        # Check for common HTML elements
        html_elements = ['<head', '<body', '<div', '<p', '<a', '<img', '<script', '<style']
        if not any(elem in text_lower for elem in html_elements):
            result['warnings'].append('Missing common HTML elements')
        
        # Check for balanced tags (basic check)
        open_tags = len(re.findall(r'<[^/][^>]*>', text))
        close_tags = len(re.findall(r'</[^>]+>', text))
        if open_tags > 0 and abs(open_tags - close_tags) > open_tags * 0.3:  # Allow 30% imbalance
            result['warnings'].append(f'Potentially unbalanced HTML tags (open: {open_tags}, close: {close_tags})')
        
        # Detect inline binaries (data: URLs)
        inline_binaries = []
        
        # Find data URLs in various contexts
        data_url_pattern = r'data:([^;,]+)(;[^,]*)?,([^"\'\s>]+)'
        data_urls = re.findall(data_url_pattern, text)
        
        for mime_type, encoding, data in data_urls:
            # Check the MIME type
            if mime_type:
                inline_binaries.append({
                    'mime_type': mime_type,
                    'encoding': encoding.strip(';') if encoding else None,
                    'size': len(data)
                })
        
        if inline_binaries:
            result['inline_binaries'] = inline_binaries
            result['warnings'].append(f'Found {len(inline_binaries)} inline binary data URLs')
            
            # Check for suspicious inline binaries
            suspicious_types = ['application/x-', 'application/octet-stream']
            for binary in inline_binaries:
                if any(susp in binary['mime_type'] for susp in suspicious_types):
                    result['warnings'].append(f'Suspicious inline binary type: {binary["mime_type"]}')
        
        # Check for external scripts/resources
        external_resources = []
        
        # Find external scripts
        script_srcs = re.findall(r'<script[^>]+src=["\']([^"\']+)["\']', text, re.IGNORECASE)
        for src in script_srcs:
            if src.startswith(('http://', 'https://', '//')):
                external_resources.append(('script', src))
        
        # Find external stylesheets
        link_hrefs = re.findall(r'<link[^>]+href=["\']([^"\']+)["\']', text, re.IGNORECASE)
        for href in link_hrefs:
            if href.startswith(('http://', 'https://', '//')):
                external_resources.append(('stylesheet', href))
        
        # Find external images
        img_srcs = re.findall(r'<img[^>]+src=["\']([^"\']+)["\']', text, re.IGNORECASE)
        for src in img_srcs:
            if not src.startswith('data:') and src.startswith(('http://', 'https://', '//')):
                external_resources.append(('image', src))
        
        if external_resources:
            result['external_resources'] = external_resources
            result['warnings'].append(f'Found {len(external_resources)} external resources')
        
        # Check for potentially malicious patterns
        malicious_patterns = [
            (r'<script[^>]*>.*?eval\s*\(', 'Potential eval() in script'),
            (r'javascript:\s*eval\s*\(', 'eval() in javascript: URL'),
            (r'on\w+\s*=\s*["\'].*?eval\s*\(', 'eval() in event handler'),
            (r'<iframe[^>]+src=["\']javascript:', 'JavaScript URL in iframe'),
            (r'<object[^>]+data=["\']javascript:', 'JavaScript URL in object'),
        ]
        
        for pattern, description in malicious_patterns:
            if re.search(pattern, text, re.IGNORECASE | re.DOTALL):
                result['warnings'].append(f'Security warning: {description}')
        
        # Try to parse with BeautifulSoup for more thorough validation
        try:
            from bs4 import BeautifulSoup
            soup = BeautifulSoup(text, 'html.parser')
            
            # Check if parsing resulted in meaningful content
            if not soup.find_all():
                result['warnings'].append('HTML parsing resulted in no elements')
            
            # Count various elements
            result['html_stats'] = {
                'total_tags': len(soup.find_all()),
                'images': len(soup.find_all('img')),
                'scripts': len(soup.find_all('script')),
                'styles': len(soup.find_all('style')),
                'links': len(soup.find_all('a')),
                'forms': len(soup.find_all('form')),
            }
            
        except Exception as e:
            result['warnings'].append(f'BeautifulSoup parsing error: {str(e)[:100]}')
    
    @staticmethod
    def _validate_image_structure(content: bytes, ext: str, result: Dict):
        """Additional validation for image files using Pillow if available."""
        # Check minimum reasonable size for images
        if len(content) < 100:
            result['warnings'].append('Suspiciously small image file')
        
        # Try to use Pillow for proper validation
        try:
            from PIL import Image
            import io
            
            # Try to open and verify the image
            try:
                img = Image.open(io.BytesIO(content))
                img.verify()  # Verify image integrity
                
                # Get image info
                result['image_info'] = {
                    'format': img.format,
                    'mode': img.mode,
                    'size': img.size if hasattr(img, 'size') else None,
                }
                
                # Check if format matches extension
                expected_formats = {
                    '.jpg': ['JPEG'],
                    '.jpeg': ['JPEG'],
                    '.png': ['PNG'],
                    '.gif': ['GIF'],
                    '.webp': ['WEBP'],
                    '.bmp': ['BMP'],
                    '.ico': ['ICO'],
                }
                
                if ext in expected_formats:
                    if img.format not in expected_formats[ext]:
                        result['warnings'].append(
                            f'Image format mismatch: {img.format} saved as {ext}'
                        )
                
            except Exception as e:
                result['warnings'].append(f'Pillow validation failed: {str(e)[:100]}')
                result['valid'] = False
                
        except ImportError:
            # Pillow not available, fall back to basic checks
            # JPEG specific
            if ext in ['.jpg', '.jpeg']:
                # Check for JPEG end marker
                if not content.endswith(b'\xFF\xD9'):
                    result['warnings'].append('JPEG file may be truncated (missing end marker)')
            
            # PNG specific
            elif ext == '.png':
                # Check for PNG end chunk
                if not content.endswith(b'\x00\x00\x00\x00IEND\xAE\x42\x60\x82'):
                    result['warnings'].append('PNG file may be truncated (missing IEND chunk)')
            
            # GIF specific
            elif ext == '.gif':
                # Check for GIF trailer
                if not content.endswith(b'\x00;') and not content.endswith(b';'):
                    result['warnings'].append('GIF file may be truncated (missing trailer)')
    
    @staticmethod
    def _validate_pdf_structure(content: bytes, result: Dict):
        """Additional validation for PDF files using PyPDF2 if available."""
        # Try to use PyPDF2 for proper validation
        try:
            import PyPDF2
            import io
            
            try:
                pdf_file = io.BytesIO(content)
                pdf_reader = PyPDF2.PdfReader(pdf_file)
                
                # Get PDF info
                result['pdf_info'] = {
                    'num_pages': len(pdf_reader.pages),
                    'is_encrypted': pdf_reader.is_encrypted,
                }
                
                # Try to access metadata
                if pdf_reader.metadata:
                    metadata = {}
                    if pdf_reader.metadata.title:
                        metadata['title'] = str(pdf_reader.metadata.title)
                    if pdf_reader.metadata.author:
                        metadata['author'] = str(pdf_reader.metadata.author)
                    if metadata:
                        result['pdf_info']['metadata'] = metadata
                
            except Exception as e:
                result['warnings'].append(f'PyPDF2 validation failed: {str(e)[:100]}')
                # Don't mark as invalid - might be encrypted or have other issues
                
        except ImportError:
            # PyPDF2 not available, fall back to basic checks
            # Check for PDF end marker
            if b'%%EOF' not in content[-1024:]:  # Check last 1KB
                result['warnings'].append('PDF file may be truncated (missing %%EOF)')
            
            # Check for basic PDF structure
            if b'endobj' not in content:
                result['warnings'].append('PDF file appears corrupted (no objects found)')
    
    @staticmethod
    def _validate_archive_structure(content: bytes, ext: str, result: Dict):
        """Additional validation for archive files."""
        # Check minimum reasonable size
        if len(content) < 22:  # Minimum ZIP size
            result['warnings'].append('Archive file too small to be valid')
        
        # ZIP specific
        if ext == '.zip':
            # Check for central directory end signature
            if b'PK\x05\x06' not in content[-65536:]:  # Check last 64KB
                result['warnings'].append('ZIP file may be corrupted (missing end record)')
    
    @staticmethod
    def _check_content_type_match(ext: str, content_type: str, result: Dict):
        """Check if file extension matches Content-Type header."""
        content_type = content_type.lower().split(';')[0].strip()
        
        expected_types = {
            '.jpg': ['image/jpeg', 'image/jpg'],
            '.jpeg': ['image/jpeg', 'image/jpg'],
            '.png': ['image/png'],
            '.gif': ['image/gif'],
            '.webp': ['image/webp'],
            '.svg': ['image/svg+xml', 'text/xml', 'application/xml'],
            '.pdf': ['application/pdf'],
            '.css': ['text/css'],
            '.js': ['application/javascript', 'text/javascript', 'application/x-javascript'],
            '.json': ['application/json', 'text/json'],
            '.xml': ['application/xml', 'text/xml'],
            '.txt': ['text/plain'],
            '.html': ['text/html', 'application/xhtml+xml'],
            '.htm': ['text/html', 'application/xhtml+xml'],
            '.zip': ['application/zip', 'application/x-zip-compressed'],
            '.gz': ['application/gzip', 'application/x-gzip'],
        }
        
        if ext in expected_types:
            if content_type not in expected_types[ext]:
                result['warnings'].append(
                    f'Content-Type mismatch: got {content_type}, expected {expected_types[ext][0]}'
                )

======= scrape_tool/utils.py ======
#!/usr/bin/env python3
# Copyright 2025 Franz und Franz GmbH
#
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#     http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.

"""Utility functions for webscraper."""

import hashlib
import re
from typing import Optional


def extract_text_from_html(html_content: str) -> str:
    """Extract plain text from HTML content for deduplication.

    This strips all HTML tags and normalizes whitespace to create
    a content fingerprint for duplicate detection.

    Args:
        html_content: HTML content to extract text from

    Returns:
        Plain text with normalized whitespace
    """
    # Remove script and style content first
    text = re.sub(
        r"<script[^>]*>.*?</script>", "", html_content, flags=re.DOTALL | re.IGNORECASE
    )
    text = re.sub(r"<style[^>]*>.*?</style>", "", text, flags=re.DOTALL | re.IGNORECASE)

    # Remove all HTML tags
    text = re.sub(r"<[^>]+>", " ", text)

    # Decode HTML entities
    text = re.sub(r"&nbsp;", " ", text)
    text = re.sub(r"&lt;", "<", text)
    text = re.sub(r"&gt;", ">", text)
    text = re.sub(r"&amp;", "&", text)
    text = re.sub(r"&quot;", '"', text)
    text = re.sub(r"&#39;", "'", text)

    # Normalize whitespace
    text = re.sub(r"\s+", " ", text)

    # Remove leading/trailing whitespace
    text = text.strip()

    return text


def calculate_content_checksum(html_content: str) -> str:
    """Calculate checksum of HTML content based on text only.

    Args:
        html_content: HTML content to calculate checksum for

    Returns:
        SHA-256 checksum of the text content
    """
    text = extract_text_from_html(html_content)
    return hashlib.sha256(text.encode("utf-8")).hexdigest()

======= scrape_tool/scrapers/__init__.py ======
"""Web scraper backends for HTML2MD."""

from typing import Dict, Type, Optional
from .base import WebScraperBase, ScraperConfig, ScrapedPage
from .beautifulsoup import BeautifulSoupScraper
from .httrack import HTTrackScraper

# Import new scrapers with error handling for optional dependencies
try:
    from .selectolax import SelectolaxScraper

    SELECTOLAX_AVAILABLE = True
except ImportError:
    SELECTOLAX_AVAILABLE = False
    SelectolaxScraper = None

# Scrapy removed - use other scrapers instead

try:
    from .playwright import PlaywrightScraper

    PLAYWRIGHT_AVAILABLE = True
except ImportError:
    PLAYWRIGHT_AVAILABLE = False
    PlaywrightScraper = None

__all__ = [
    "WebScraperBase",
    "ScraperConfig",
    "ScrapedPage",
    "create_scraper",
    "SCRAPER_REGISTRY",
    "BeautifulSoupScraper",
    "HTTrackScraper",
    "SelectolaxScraper",
    "PlaywrightScraper",
]

# Registry of available scraper backends
SCRAPER_REGISTRY: Dict[str, Type[WebScraperBase]] = {
    "beautifulsoup": BeautifulSoupScraper,
    "bs4": BeautifulSoupScraper,  # Alias
    "httrack": HTTrackScraper,
}

# Add optional scrapers if available
if SELECTOLAX_AVAILABLE:
    SCRAPER_REGISTRY["selectolax"] = SelectolaxScraper
    SCRAPER_REGISTRY["httpx"] = SelectolaxScraper  # Alias


if PLAYWRIGHT_AVAILABLE:
    SCRAPER_REGISTRY["playwright"] = PlaywrightScraper


def create_scraper(
    backend: str, config: Optional[ScraperConfig] = None
) -> WebScraperBase:
    """Factory function to create appropriate scraper instance.

    Args:
        backend: Name of the scraper backend to use
        config: Configuration for the scraper (uses defaults if not provided)

    Returns:
        Instance of the requested scraper backend

    Raises:
        ValueError: If the backend is not registered
    """
    if backend not in SCRAPER_REGISTRY:
        available = ", ".join(SCRAPER_REGISTRY.keys()) if SCRAPER_REGISTRY else "none"
        raise ValueError(
            f"Unknown scraper backend: {backend}. " f"Available backends: {available}"
        )

    if config is None:
        config = ScraperConfig()

    scraper_class = SCRAPER_REGISTRY[backend]
    return scraper_class(config)

======= scrape_tool/scrapers/base.py ======
# Copyright 2025 Franz und Franz GmbH
#
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#     http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.

"""Abstract base class for web scrapers."""

from abc import ABC, abstractmethod
from dataclasses import dataclass, field
from typing import List, Dict, Optional, AsyncGenerator, Set, Tuple
from pathlib import Path
import logging
from urllib.parse import urlparse, urljoin
from urllib.robotparser import RobotFileParser
import asyncio
import aiohttp

logger = logging.getLogger(__name__)


@dataclass
class ScraperConfig:
    """Configuration for web scrapers."""

    max_depth: int = 10
    max_pages: int = 10000
    allowed_domains: Optional[List[str]] = None
    allowed_path: Optional[str] = None
    allowed_paths: Optional[List[str]] = None
    exclude_patterns: Optional[List[str]] = None
    respect_robots_txt: bool = True
    concurrent_requests: int = 5
    request_delay: float = 0.5
    user_agent: str = (
        "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36"
    )
    custom_headers: Optional[Dict[str, str]] = None
    timeout: float = 30.0
    follow_redirects: bool = True
    verify_ssl: bool = True
    ignore_get_params: bool = False
    check_canonical: bool = True
    check_content_duplicates: bool = True
    check_ssrf: bool = True

    def __post_init__(self):
        """Initialize mutable defaults."""
        if self.allowed_domains is None:
            self.allowed_domains = []
        if self.allowed_paths is None:
            self.allowed_paths = []
        if self.exclude_patterns is None:
            self.exclude_patterns = []
        if self.custom_headers is None:
            self.custom_headers = {}


@dataclass
class ScrapedPage:
    """Represents a scraped web page."""

    url: str
    content: str
    title: Optional[str] = None
    metadata: Optional[Dict[str, str]] = None
    encoding: str = "utf-8"
    status_code: Optional[int] = None
    headers: Optional[Dict[str, str]] = None
    normalized_url: Optional[str] = None
    canonical_url: Optional[str] = None
    content_checksum: Optional[str] = None
    file_type: str = "html"  # 'html', 'image', 'pdf', 'css', 'js', etc.
    file_size: Optional[int] = None  # File size in bytes
    is_binary: bool = False  # True for binary files like images, PDFs
    binary_content: Optional[bytes] = None  # Binary content for non-HTML files

    def __post_init__(self):
        """Initialize mutable defaults."""
        if self.metadata is None:
            self.metadata = {}
        if self.headers is None:
            self.headers = {}


class WebScraperBase(ABC):
    """Abstract base class for web scrapers."""

    def __init__(self, config: ScraperConfig):
        """Initialize the scraper with configuration.

        Args:
            config: Scraper configuration
        """
        self.config = config
        self._visited_urls: Set[str] = set()
        self._robots_parsers: Dict[str, RobotFileParser] = {}
        self._robots_fetch_lock = asyncio.Lock()
        self._checksum_callback = None  # Callback to check if checksum exists
        self._allowed_path_configs: List[Tuple[Optional[str], str]] = []  # List of (domain, path) tuples

    @abstractmethod
    async def scrape_url(self, url: str) -> ScrapedPage:
        """Scrape a single URL.

        Args:
            url: URL to scrape

        Returns:
            ScrapedPage object containing the scraped content

        Raises:
            Exception: If scraping fails
        """
        pass

    @abstractmethod
    async def scrape_site(self, start_url: str) -> AsyncGenerator[ScrapedPage, None]:
        """Scrape an entire website starting from a URL.

        Args:
            start_url: URL to start crawling from

        Yields:
            ScrapedPage objects as they are scraped
        """
        pass

    def _is_private_ip(self, hostname: str) -> bool:
        """Check if hostname resolves to a private IP address.

        Args:
            hostname: Hostname or IP address to check

        Returns:
            True if the hostname resolves to a private IP, False otherwise
        """
        import socket
        import ipaddress

        try:
            # Set a timeout for DNS resolution to avoid hanging
            socket.setdefaulttimeout(2.0)
            # Get IP address from hostname
            ip = socket.gethostbyname(hostname)
            ip_obj = ipaddress.ip_address(ip)

            # Check for private networks
            if ip_obj.is_private:
                return True

            # Check for loopback
            if ip_obj.is_loopback:
                return True

            # Check for link-local
            if ip_obj.is_link_local:
                return True

            # Check for multicast
            if ip_obj.is_multicast:
                return True

            # Check for cloud metadata endpoint
            if str(ip_obj).startswith("169.254."):
                return True

            return False

        except (socket.gaierror, ValueError):
            # If we can't resolve the hostname, it's likely a normal website
            # with DNS issues or a domain that doesn't exist anymore
            # Don't treat this as a private IP
            return False

    async def validate_url(self, url: str) -> bool:
        """Validate if a URL should be scraped based on configuration and robots.txt.

        Args:
            url: URL to validate

        Returns:
            True if URL should be scraped, False otherwise
        """
        from urllib.parse import urlparse

        try:
            parsed = urlparse(url)

            # Check if URL has valid scheme
            if parsed.scheme not in ("http", "https"):
                return False

            # Extract hostname (remove port if present)
            hostname = parsed.hostname or parsed.netloc.split(":")[0]

            # Check for SSRF - block private IPs (if enabled)
            if self.config.check_ssrf and self._is_private_ip(hostname):
                logger.warning(f"Blocked URL {url} - private IP address detected")
                return False

            # Check allowed domains
            if self.config.allowed_domains:
                domain_allowed = False
                for domain in self.config.allowed_domains:
                    if domain in parsed.netloc:
                        domain_allowed = True
                        break
                if not domain_allowed:
                    return False

            # Check if URL should be excluded
            if self._should_exclude_url(url):
                return False

            return True

        except Exception as e:
            logger.error(f"Error validating URL {url}: {e}")
            return False

    async def _fetch_robots_txt(self, base_url: str) -> Optional[RobotFileParser]:
        """Fetch and parse robots.txt for a given base URL.

        Args:
            base_url: Base URL of the website

        Returns:
            RobotFileParser object or None if fetch fails
        """
        robots_url = urljoin(base_url, "/robots.txt")

        try:
            async with aiohttp.ClientSession() as session:
                async with session.get(
                    robots_url,
                    timeout=aiohttp.ClientTimeout(total=10),
                    headers={"User-Agent": self.config.user_agent},
                ) as response:
                    if response.status == 200:
                        content = await response.text()
                        parser = RobotFileParser()
                        parser.parse(content.splitlines())
                        return parser
                    else:
                        logger.debug(
                            f"No robots.txt found at {robots_url} (status: {response.status})"
                        )
                        return None
        except Exception as e:
            logger.debug(f"Error fetching robots.txt from {robots_url}: {e}")
            return None

    async def can_fetch(self, url: str) -> bool:
        """Check if URL can be fetched according to robots.txt.

        Args:
            url: URL to check

        Returns:
            True if URL can be fetched, False otherwise
        """
        if not self.config.respect_robots_txt:
            return True

        parsed = urlparse(url)
        base_url = f"{parsed.scheme}://{parsed.netloc}"

        # Check if we already have the robots.txt for this domain
        if base_url not in self._robots_parsers:
            async with self._robots_fetch_lock:
                # Double-check after acquiring lock
                if base_url not in self._robots_parsers:
                    parser = await self._fetch_robots_txt(base_url)
                    self._robots_parsers[base_url] = parser

        parser = self._robots_parsers.get(base_url)
        if parser is None:
            # No robots.txt or fetch failed - allow by default
            return True

        # Check if the URL is allowed for our user agent
        return parser.can_fetch(self.config.user_agent, url)

    def is_visited(self, url: str) -> bool:
        """Check if URL has already been visited.

        Args:
            url: URL to check

        Returns:
            True if URL has been visited, False otherwise
        """
        return url in self._visited_urls

    def mark_visited(self, url: str) -> None:
        """Mark URL as visited.

        Args:
            url: URL to mark as visited
        """
        self._visited_urls.add(url)

    def set_checksum_callback(self, callback):
        """Set callback function to check if content checksum exists.

        Args:
            callback: Function that takes a checksum string and returns bool
        """
        self._checksum_callback = callback

    def _initialize_allowed_paths(self, start_url: str) -> None:
        """Initialize allowed path configurations based on config and start URL.
        
        Args:
            start_url: The starting URL for crawling
        """
        self._allowed_path_configs.clear()
        start_parsed = urlparse(start_url)
        
        # Get list of allowed paths (new multiple paths or fallback to single path)
        allowed_paths_list = None
        if hasattr(self.config, 'allowed_paths') and self.config.allowed_paths is not None:
            # If allowed_paths is explicitly set, use it (even if empty list)
            allowed_paths_list = self.config.allowed_paths
        elif self.config.allowed_path:
            allowed_paths_list = [self.config.allowed_path]
        
        # Process allowed paths if we have any (not None and not empty)
        if allowed_paths_list is not None and len(allowed_paths_list) > 0:
            for allowed_path in allowed_paths_list:
                # Check if allowed_path is a full URL or just a path
                if allowed_path.startswith(("http://", "https://")):
                    # It's a full URL - extract domain and path
                    parsed_allowed = urlparse(allowed_path)
                    path_config = (parsed_allowed.netloc, parsed_allowed.path.rstrip("/"))
                    self._allowed_path_configs.append(path_config)
                    logger.info(f"Restricting crawl to URL: {parsed_allowed.netloc}{parsed_allowed.path.rstrip('/')}")
                else:
                    # It's just a path
                    path_config = (None, allowed_path.rstrip("/"))
                    self._allowed_path_configs.append(path_config)
                    logger.info(f"Restricting crawl to allowed path: {allowed_path.rstrip('/')}")
        else:
            # Use the start URL's path if no allowed paths specified
            # This applies when allowed_paths is None or empty list []
            base_path = start_parsed.path.rstrip("/")
            # Extract directory path (remove file name if present)
            if base_path and '/' in base_path:
                # If path ends with a file (has extension), use parent directory
                if '.' in base_path.split('/')[-1]:
                    base_path = '/'.join(base_path.split('/')[:-1])
            if base_path:
                self._allowed_path_configs.append((None, base_path))
                logger.info(f"Restricting crawl to subdirectory: {base_path}")

    def _should_exclude_url(self, url: str) -> bool:
        """Check if URL should be excluded based on patterns.
        
        Args:
            url: URL to check for exclusion
            
        Returns:
            True if URL should be excluded, False otherwise
        """
        import re
        from urllib.parse import urlparse
        
        # Check exclude_patterns (regex-based for flexibility)
        if self.config.exclude_patterns:
            for pattern in self.config.exclude_patterns:
                try:
                    # Try as regex first
                    if re.search(pattern, url):
                        logger.debug(f"URL {url} excluded by regex pattern: {pattern}")
                        return True
                except re.error:
                    # Fall back to substring matching if not valid regex
                    if pattern in url:
                        logger.debug(f"URL {url} excluded by substring pattern: {pattern}")
                        return True
        
        # Check excluded_paths (path-only matching)
        if hasattr(self.config, 'excluded_paths') and self.config.excluded_paths:
            parsed = urlparse(url)
            for excluded in self.config.excluded_paths:
                if excluded in parsed.path:
                    logger.debug(f"URL {url} excluded by path: {excluded}")
                    return True
        
        return False
    
    def _is_path_allowed(self, url: str, start_url: str) -> bool:
        """Check if URL path matches any allowed path configuration.
        
        Args:
            url: URL to check
            start_url: The starting URL (always allowed)
            
        Returns:
            True if URL is allowed, False otherwise
        """
        # Always allow the start URL
        if url == start_url:
            return True
            
        # If no allowed path configs, allow all paths
        if not self._allowed_path_configs:
            return True
            
        url_parsed = urlparse(url)
        
        # Check if URL matches any of the allowed path configurations
        for allowed_domain_config, allowed_path_config in self._allowed_path_configs:
            # If a domain is specified in the config, check it matches
            if allowed_domain_config and url_parsed.netloc != allowed_domain_config:
                continue  # Try next config
            
            # Check if the path is allowed
            if (url_parsed.path.startswith(allowed_path_config + "/") or 
                url_parsed.path == allowed_path_config):
                return True
        
        return False
    
    def _is_url_allowed(self, url: str) -> bool:
        """Check if URL is allowed based on domain and path restrictions.
        
        Args:
            url: URL to check
            
        Returns:
            True if URL is allowed, False otherwise
        """
        parsed = urlparse(url)
        
        # Check allowed domains if configured
        if hasattr(self.config, 'allowed_domains') and self.config.allowed_domains:
            if parsed.netloc not in self.config.allowed_domains:
                return False
        
        # Check allowed paths (new multiple paths logic)
        if hasattr(self.config, 'allowed_paths') and self.config.allowed_paths:
            path_allowed = any(
                parsed.path.startswith(allowed_path) 
                for allowed_path in self.config.allowed_paths
            )
            if not path_allowed:
                return False
        # Fallback to old single path for backward compatibility
        elif hasattr(self.config, 'allowed_path') and self.config.allowed_path:
            if not parsed.path.startswith(self.config.allowed_path):
                return False
        
        return True

    async def download_binary_file(self, url: str, max_size: Optional[int] = None) -> Optional[ScrapedPage]:
        """Download binary content from URL with security checks.
        
        Args:
            url: URL to download
            max_size: Maximum file size in bytes
            
        Returns:
            ScrapedPage with binary content or None if download fails
        """
        import aiohttp
        import mimetypes
        from pathlib import Path
        
        # Security: For assets, we allow external domains (CDNs, etc.)
        # Assets like images, CSS, JS often come from CDNs and should be allowed
        # Path restrictions don't apply to assets - only to crawled HTML pages
        parsed = urlparse(url)
        
        # Check if external assets are allowed
        if hasattr(self.config, 'download_external_assets') and not self.config.download_external_assets:
            # If external assets are disabled, check if it's from allowed domains
            if hasattr(self.config, 'allowed_domains') and self.config.allowed_domains:
                if parsed.netloc not in self.config.allowed_domains:
                    logger.debug(f"External asset blocked (download_external_assets=False): {url}")
                    return None
        
        # Security: Check for SSRF if enabled (this is always important)
        if self.config.check_ssrf:
            if self._is_private_ip(parsed.netloc.split(':')[0]):
                logger.warning(f"SSRF protection: Blocked private IP for {url}")
                return None
        
        try:
            # Determine file type from URL
            parsed_url = urlparse(url)
            path = parsed_url.path.lower()
            file_extension = Path(path).suffix
            mime_type, _ = mimetypes.guess_type(path)
            
            # Determine file type category
            file_type = "unknown"
            if file_extension in [".jpg", ".jpeg", ".png", ".gif", ".svg", ".webp", ".ico"]:
                file_type = "image"
            elif file_extension == ".pdf":
                file_type = "pdf"
            elif file_extension == ".css":
                file_type = "css"
            elif file_extension == ".js":
                file_type = "js"
            elif file_extension in [".woff", ".woff2", ".ttf", ".eot"]:
                file_type = "font"
            elif file_extension in [".mp4", ".webm", ".mp3", ".wav", ".ogg"]:
                file_type = "media"
            elif file_extension in [".doc", ".docx", ".xls", ".xlsx", ".ppt", ".pptx"]:
                file_type = "document"
            elif file_extension in [".zip", ".tar", ".gz", ".rar", ".7z"]:
                file_type = "archive"
            elif file_extension in [".md", ".txt", ".csv", ".json", ".xml"]:
                file_type = "text"
            
            async with aiohttp.ClientSession() as session:
                headers = {"User-Agent": self.config.user_agent} if self.config.user_agent else {}
                async with session.get(url, headers=headers, timeout=self.config.timeout) as response:
                    if response.status != 200:
                        logger.warning(f"Failed to download {url}: HTTP {response.status}")
                        return None
                    
                    # Security: Validate Content-Type header
                    content_type_header = response.headers.get('content-type', '').lower()
                    
                    # Block potentially dangerous content types
                    dangerous_content_types = [
                        'application/x-executable',
                        'application/x-msdownload', 
                        'application/x-msdos-program',
                        'application/x-sh',
                        'application/x-shellscript',
                        'application/x-httpd-php',
                        'application/x-httpd-cgi',
                    ]
                    
                    for dangerous_type in dangerous_content_types:
                        if dangerous_type in content_type_header:
                            logger.warning(f"Blocked dangerous content type {content_type_header}: {url}")
                            return None
                    
                    # Check content length
                    content_length = response.headers.get('content-length')
                    if content_length and max_size:
                        if int(content_length) > max_size:
                            logger.warning(f"File {url} too large: {content_length} bytes (max: {max_size})")
                            return None
                    
                    # Read content with size limit - read in chunks to prevent memory exhaustion
                    chunks = []
                    total_size = 0
                    chunk_size = 8192  # 8KB chunks
                    
                    async for chunk in response.content.iter_chunked(chunk_size):
                        total_size += len(chunk)
                        if max_size and total_size > max_size:
                            logger.warning(f"Downloaded file {url} exceeds size limit: {total_size} bytes")
                            return None
                        chunks.append(chunk)
                    
                    content = b''.join(chunks)
                    
                    # Validate file content
                    from ..file_validator import FileValidator
                    validation_result = FileValidator.validate_file(
                        content, file_extension, content_type_header
                    )
                    
                    if not validation_result['valid']:
                        logger.error(f"File validation failed for {url}: {validation_result['error']}")
                        # Still return the file but mark it as potentially invalid
                        # User can decide what to do with invalid files
                    
                    if validation_result.get('warnings'):
                        for warning in validation_result['warnings']:
                            logger.warning(f"File validation warning for {url}: {warning}")
                    
                    # Create ScrapedPage for binary content
                    page = ScrapedPage(
                        url=url,
                        content="",  # Empty for binary files
                        title=Path(parsed_url.path).name if parsed_url.path else "download",
                        metadata={
                            "mime_type": mime_type or "application/octet-stream",
                            "validation": validation_result
                        },
                        status_code=response.status,
                        headers=dict(response.headers),
                        file_type=file_type,
                        file_size=len(content),
                        is_binary=True,
                        binary_content=content
                    )
                    
                    return page
                    
        except asyncio.TimeoutError:
            logger.error(f"Timeout downloading {url}")
            return None
        except Exception as e:
            logger.error(f"Error downloading {url}: {e}")
            return None

    def is_asset_url(self, url: str, allowed_extensions: list[str]) -> bool:
        """Check if URL points to an asset file based on extension.
        
        Args:
            url: URL to check
            allowed_extensions: List of allowed file extensions (with dots)
            
        Returns:
            True if URL is for an allowed asset type
        """
        parsed_url = urlparse(url)
        path = parsed_url.path.lower()
        
        for ext in allowed_extensions:
            if path.endswith(ext.lower()):
                return True
        return False

    async def __aenter__(self):
        """Async context manager entry."""
        return self

    async def __aexit__(self, exc_type, exc_val, exc_tb):
        """Async context manager exit."""
        pass

======= scrape_tool/scrapers/beautifulsoup.py ======
# Copyright 2025 Franz und Franz GmbH
#
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#     http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.

"""BeautifulSoup4-based web scraper implementation."""

import asyncio
import logging
from typing import Set, AsyncGenerator, Optional, Dict, List
from urllib.parse import urljoin, urlparse, unquote
import aiohttp
from bs4 import BeautifulSoup
import chardet

from .base import WebScraperBase, ScrapedPage, ScraperConfig

logger = logging.getLogger(__name__)


class BeautifulSoupScraper(WebScraperBase):
    """BeautifulSoup4-based web scraper for simple HTML extraction."""

    def __init__(self, config: ScraperConfig):
        """Initialize the BeautifulSoup scraper.

        Args:
            config: Scraper configuration
        """
        super().__init__(config)
        self.session: Optional[aiohttp.ClientSession] = None
        self._semaphore = asyncio.Semaphore(config.concurrent_requests)
        self._resume_info: List[Dict[str, str]] = []

    def _normalize_url(self, url: str) -> str:
        """Normalize URL by removing GET parameters if configured.

        Args:
            url: URL to normalize

        Returns:
            Normalized URL
        """
        if self.config.ignore_get_params and "?" in url:
            return url.split("?")[0]
        return url

    async def __aenter__(self):
        """Create aiohttp session on entry."""
        headers = {}

        # Only add User-Agent if it's not None
        if self.config.user_agent:
            headers["User-Agent"] = self.config.user_agent

        if self.config.custom_headers:
            # Filter out None keys when updating headers
            for k, v in self.config.custom_headers.items():
                if k is not None and v is not None:
                    headers[k] = v

        # Final validation to ensure no None values
        headers = {k: v for k, v in headers.items() if k is not None and v is not None}

        timeout = aiohttp.ClientTimeout(total=self.config.timeout)
        connector = aiohttp.TCPConnector(
            ssl=self.config.verify_ssl, limit=self.config.concurrent_requests * 2
        )

        self.session = aiohttp.ClientSession(
            headers=headers, timeout=timeout, connector=connector
        )
        return self

    async def __aexit__(self, *args):
        """Close aiohttp session on exit."""
        if self.session and not self.session.closed:
            await self.session.close()
            # Small delay to allow connections to close properly
            await asyncio.sleep(0.25)

    async def scrape_url(self, url: str) -> ScrapedPage:
        """Scrape a single URL using BeautifulSoup.

        Args:
            url: URL to scrape

        Returns:
            ScrapedPage object containing the scraped content

        Raises:
            Exception: If scraping fails
        """
        if not self.session:
            raise RuntimeError("Scraper must be used as async context manager")

        async with self._semaphore:  # Limit concurrent requests
            try:
                logger.info(f"Scraping URL: {url}")

                async with self.session.get(
                    url, allow_redirects=self.config.follow_redirects
                ) as response:
                    # Get response info
                    status_code = response.status
                    # Convert headers to dict with string keys, skip None keys
                    headers = {}
                    for k, v in response.headers.items():
                        if k is not None:
                            headers[str(k)] = str(v)

                    # Handle encoding
                    content_bytes = await response.read()

                    # Try to detect encoding if not specified
                    encoding = response.charset
                    if not encoding:
                        detected = chardet.detect(content_bytes)
                        encoding = detected.get("encoding", "utf-8")
                        logger.debug(f"Detected encoding for {url}: {encoding}")

                    # Decode content
                    try:
                        content = content_bytes.decode(encoding or "utf-8")
                    except (UnicodeDecodeError, LookupError):
                        # Fallback to utf-8 with error handling
                        content = content_bytes.decode("utf-8", errors="replace")
                        encoding = "utf-8"

                    # Validate HTML content
                    content_type_header = headers.get('content-type', '').lower()
                    validation_result = None
                    
                    # Check if it's HTML (by Content-Type or by trying to parse)
                    is_html = 'text/html' in content_type_header or not content_type_header
                    
                    if is_html:
                        from ..file_validator import FileValidator
                        validation_result = FileValidator.validate_file(
                            content_bytes, '.html', content_type_header
                        )
                        
                        if not validation_result.get('valid', True):
                            logger.warning(f"HTML validation failed for {url}: {validation_result.get('error', 'Unknown error')}")
                        
                        if validation_result.get('warnings'):
                            for warning in validation_result['warnings']:
                                logger.debug(f"HTML validation warning for {url}: {warning}")
                    
                    # Parse with BeautifulSoup
                    soup = BeautifulSoup(content, "html.parser")

                    # Store metadata for database
                    normalized_url = self._normalize_url(str(response.url))
                    canonical_url = None
                    content_checksum = None

                    # Order: 1. GET parameter normalization (already done in _normalize_url)
                    # 2. Canonical URL check
                    if self.config.check_canonical:
                        canonical_link = soup.find("link", {"rel": "canonical"})
                        if canonical_link and canonical_link.get("href"):
                            canonical_url = canonical_link["href"]
                            # Make canonical URL absolute
                            canonical_url = urljoin(url, canonical_url)
                            # Normalize canonical URL too
                            normalized_canonical = self._normalize_url(canonical_url)

                            if normalized_url != normalized_canonical:
                                # Check if we should respect the canonical URL
                                should_skip = True

                                # Check allowed paths (single or multiple)
                                allowed_paths_list = []
                                if hasattr(self.config, 'allowed_paths') and self.config.allowed_paths:
                                    allowed_paths_list = self.config.allowed_paths
                                elif self.config.allowed_path:
                                    allowed_paths_list = [self.config.allowed_path]
                                
                                if allowed_paths_list:
                                    # Parse URLs to check paths
                                    current_parsed = urlparse(normalized_url)
                                    canonical_parsed = urlparse(normalized_canonical)

                                    # If current URL is within any allowed_path but canonical is outside all,
                                    # don't skip - the user explicitly wants content from allowed_path
                                    current_in_allowed = any(
                                        current_parsed.path.startswith(allowed_path)
                                        for allowed_path in allowed_paths_list
                                    )
                                    canonical_in_allowed = any(
                                        canonical_parsed.path.startswith(allowed_path)
                                        for allowed_path in allowed_paths_list
                                    )
                                    
                                    if current_in_allowed and not canonical_in_allowed:
                                        should_skip = False
                                        logger.info(
                                            f"Not skipping {url} - canonical URL {canonical_url} is outside allowed_paths {allowed_paths_list}"
                                        )

                                if should_skip:
                                    logger.info(
                                        f"Skipping {url} - canonical URL differs: {canonical_url}"
                                    )
                                    return None  # Return None to indicate skip, not an error

                    # 3. Content duplicate check
                    if self.config.check_content_duplicates:
                        from ..utils import calculate_content_checksum

                        content_checksum = calculate_content_checksum(content)

                        # Check if checksum exists using callback or fall back to database query
                        if self._checksum_callback and self._checksum_callback(
                            content_checksum
                        ):
                            logger.info(f"Skipping {url} - duplicate content detected")
                            return None  # Return None to indicate skip, not an error

                    # Extract metadata
                    title = soup.find("title")
                    title_text = title.get_text(strip=True) if title else None

                    metadata = self._extract_metadata(soup)
                    
                    # Add validation result to metadata if available
                    if validation_result:
                        metadata['html_validation'] = {
                            'valid': validation_result.get('valid', True),
                            'warnings_count': len(validation_result.get('warnings', [])),
                        }
                        
                        # Include inline binaries info if present
                        if 'inline_binaries' in validation_result:
                            metadata['html_validation']['inline_binaries_count'] = len(validation_result['inline_binaries'])
                            metadata['html_validation']['inline_binaries'] = validation_result['inline_binaries']
                        
                        # Include HTML stats if available
                        if 'html_stats' in validation_result:
                            metadata['html_validation']['stats'] = validation_result['html_stats']

                    return ScrapedPage(
                        url=str(response.url),  # Use final URL after redirects
                        content=str(soup),
                        title=title_text,
                        metadata=metadata,
                        encoding=encoding,
                        status_code=status_code,
                        headers=headers,
                        normalized_url=normalized_url,
                        canonical_url=canonical_url,
                        content_checksum=content_checksum,
                    )

            except asyncio.TimeoutError:
                logger.error(f"Timeout while scraping {url}")
                raise
            except aiohttp.ClientError as e:
                logger.error(f"Client error while scraping {url}: {e}")
                raise
            except Exception as e:
                logger.error(f"Unexpected error while scraping {url}: {e}")
                raise

    def set_resume_info(self, resume_info: List[Dict[str, str]]) -> None:
        """Set resume information for continuing a crawl.

        Args:
            resume_info: List of dicts with 'url' and 'content' keys
        """
        self._resume_info = resume_info
        logger.info(
            f"Loaded {len(resume_info)} previously scraped pages for link extraction"
        )

    async def populate_queue_from_content(
        self,
        content: str,
        url: str,
        to_visit: Set[str],
        depth_map: Dict[str, int],
        current_depth: int,
    ) -> None:
        """Extract links from content and add to queue.

        Args:
            content: HTML content to extract links from
            url: URL of the page
            to_visit: Set of URLs to visit
            depth_map: Mapping of URLs to their depth
            current_depth: Current crawl depth
        """
        if self.config.max_depth == -1 or current_depth < self.config.max_depth:
            new_urls = self._extract_links(content, url)
            for new_url in new_urls:
                normalized_new_url = self._normalize_url(new_url)
                if (
                    normalized_new_url not in self._visited_urls
                    and normalized_new_url not in to_visit
                ):
                    to_visit.add(normalized_new_url)
                    depth_map[normalized_new_url] = current_depth + 1
                    logger.debug(
                        f"Added URL to queue: {normalized_new_url} (depth: {current_depth + 1})"
                    )

    async def scrape_site(self, start_url: str) -> AsyncGenerator[ScrapedPage, None]:
        """Scrape entire website starting from URL.

        Args:
            start_url: URL to start crawling from

        Yields:
            ScrapedPage objects as they are scraped
        """
        # Parse start URL to get base domain
        start_parsed = urlparse(start_url)
        base_domain = start_parsed.netloc

        # Initialize allowed paths configuration
        self._initialize_allowed_paths(start_url)

        # If no allowed domains specified, restrict to start domain
        if not self.config.allowed_domains:
            self.config.allowed_domains = [base_domain]
            logger.info(f"Restricting crawl to domain: {base_domain}")

        # URLs to visit
        to_visit: Set[str] = {start_url}
        depth_map: Dict[str, int] = {start_url: 0}

        # Check if we're already in a context manager
        should_close_session = False
        if not self.session:
            await self.__aenter__()
            should_close_session = True

        try:
            # If we have resume info, populate the queue from previously scraped pages
            if self._resume_info:
                logger.info("Populating queue from previously scraped pages...")
                for page_info in self._resume_info:
                    url = page_info["url"]
                    content = page_info["content"]
                    # Assume depth 0 for scraped pages, their links will be depth 1
                    await self.populate_queue_from_content(
                        content, url, to_visit, depth_map, 0
                    )
                logger.info(
                    f"Found {len(to_visit)} URLs to visit after analyzing scraped pages"
                )
            pages_scraped = 0  # Track actual pages scraped, not just URLs attempted

            while to_visit and (
                self.config.max_pages == -1 or pages_scraped < self.config.max_pages
            ):
                # Get next URL
                url = to_visit.pop()

                # Skip if already visited (normalize URL first)
                normalized_url = self._normalize_url(url)
                if self.is_visited(normalized_url):
                    continue

                # Validate URL
                if not await self.validate_url(url):
                    continue

                # Check path restriction using base class method
                if not self._is_path_allowed(url, start_url):
                    logger.debug(
                        f"Skipping {url} - outside allowed paths {self._allowed_path_configs}"
                    )
                    continue

                # Check robots.txt
                if not await self.can_fetch(url):
                    logger.info(f"Skipping {url} - blocked by robots.txt")
                    continue

                # Check depth
                current_depth = depth_map.get(url, 0)
                if (
                    self.config.max_depth != -1
                    and current_depth > self.config.max_depth
                ):
                    logger.debug(
                        f"Skipping {url} - exceeds max depth {self.config.max_depth}"
                    )
                    continue

                # Mark as visited
                self.mark_visited(normalized_url)

                try:
                    # Scrape the page
                    page = await self.scrape_url(url)

                    # Skip if page is None (duplicate content or canonical mismatch)
                    if page is None:
                        continue

                    yield page
                    pages_scraped += 1  # Only increment for successfully scraped pages

                    # Extract links if not at max depth
                    await self.populate_queue_from_content(
                        page.content, url, to_visit, depth_map, current_depth
                    )

                    # Respect rate limit
                    if self.config.request_delay > 0:
                        await asyncio.sleep(self.config.request_delay)

                except Exception as e:
                    logger.error(f"Error processing {url}: {e}")
                    # Continue with other URLs
                    continue

        finally:
            # Clean up session if we created it
            # Log why crawling stopped
            if self.config.max_pages != -1 and pages_scraped >= self.config.max_pages:
                logger.info(f"Reached max_pages limit of {self.config.max_pages}")
            elif not to_visit:
                logger.info("No more URLs to visit")
            
            if should_close_session:
                await self.__aexit__(None, None, None)

        logger.info(f"Crawl complete. Visited {len(self._visited_urls)} pages")

    def _extract_metadata(self, soup: BeautifulSoup) -> Dict[str, str]:
        """Extract metadata from HTML.

        Args:
            soup: BeautifulSoup parsed HTML

        Returns:
            Dictionary of metadata key-value pairs
        """
        metadata = {}

        # Extract meta tags
        for meta in soup.find_all("meta"):
            # Try different meta tag formats
            name = meta.get("name") or meta.get("property") or meta.get("http-equiv")
            content = meta.get("content", "")

            if name is not None and content:
                # Ensure name is a string
                metadata[str(name)] = content

        # Extract other useful information
        # Canonical URL
        canonical = soup.find("link", {"rel": "canonical"})
        if canonical and canonical.get("href"):
            metadata["canonical"] = canonical["href"]

        # Author
        author = soup.find("meta", {"name": "author"})
        if author and author.get("content"):
            metadata["author"] = author["content"]

        return metadata

    def _extract_links(self, html_content: str, base_url: str) -> Set[str]:
        """Extract all links from HTML content.

        Args:
            html_content: HTML content to parse
            base_url: Base URL for resolving relative links

        Returns:
            Set of absolute URLs found in the content
        """
        links = set()
        asset_links = set()

        try:
            soup = BeautifulSoup(html_content, "html.parser")

            # Find all links (only from anchor tags, not link tags which often point to CSS)
            for tag in soup.find_all("a"):
                href = tag.get("href")
                if href:
                    # Clean and resolve URL
                    href = href.strip()
                    if href and not href.startswith(
                        ("#", "javascript:", "mailto:", "tel:")
                    ):
                        absolute_url = urljoin(base_url, href)
                        # Remove fragment
                        absolute_url = absolute_url.split("#")[0]

                        # Remove GET parameters if configured to do so
                        if self.config.ignore_get_params and "?" in absolute_url:
                            absolute_url = absolute_url.split("?")[0]

                        if absolute_url:
                            # Skip non-HTML resources
                            if not any(
                                absolute_url.endswith(ext)
                                for ext in [
                                    ".css",
                                    ".js",
                                    ".json",
                                    ".xml",
                                    ".ico",
                                    ".jpg",
                                    ".jpeg",
                                    ".png",
                                    ".gif",
                                    ".svg",
                                    ".webp",
                                    ".pdf",
                                    ".zip",
                                ]
                            ):
                                links.add(unquote(absolute_url))

        except Exception as e:
            logger.error(f"Error extracting links from {base_url}: {e}")

        return links
    
    def extract_asset_urls(self, html_content: str, base_url: str, asset_types: list[str]) -> Set[str]:
        """Extract all asset URLs from HTML content.
        
        Args:
            html_content: HTML content to parse
            base_url: Base URL for resolving relative links
            asset_types: List of file extensions to consider as assets
            
        Returns:
            Set of absolute asset URLs found in the content
        """
        asset_urls = set()
        
        try:
            soup = BeautifulSoup(html_content, "html.parser")
            
            # Extract image sources
            for img in soup.find_all("img"):
                src = img.get("src") or img.get("data-src")
                if src and not src.startswith("data:"):
                    absolute_url = urljoin(base_url, src)
                    if self.is_asset_url(absolute_url, asset_types):
                        asset_urls.add(absolute_url)
            
            # Extract stylesheet links
            for link in soup.find_all("link"):
                href = link.get("href")
                if href:
                    rel = link.get("rel", [])
                    if "stylesheet" in rel or href.endswith(".css"):
                        absolute_url = urljoin(base_url, href)
                        if self.is_asset_url(absolute_url, asset_types):
                            asset_urls.add(absolute_url)
            
            # Extract script sources
            for script in soup.find_all("script"):
                src = script.get("src")
                if src:
                    absolute_url = urljoin(base_url, src)
                    if self.is_asset_url(absolute_url, asset_types):
                        asset_urls.add(absolute_url)
            
            # Extract video/audio sources
            for tag in soup.find_all(["video", "audio", "source"]):
                src = tag.get("src")
                if src:
                    absolute_url = urljoin(base_url, src)
                    if self.is_asset_url(absolute_url, asset_types):
                        asset_urls.add(absolute_url)
            
            # Extract object/embed sources
            for tag in soup.find_all(["object", "embed"]):
                src = tag.get("data") or tag.get("src")
                if src:
                    absolute_url = urljoin(base_url, src)
                    if self.is_asset_url(absolute_url, asset_types):
                        asset_urls.add(absolute_url)
            
            # Extract downloadable links (PDFs, documents, etc.)
            for a in soup.find_all("a"):
                href = a.get("href")
                if href and not href.startswith(("#", "javascript:", "mailto:", "tel:")):
                    absolute_url = urljoin(base_url, href)
                    if self.is_asset_url(absolute_url, asset_types):
                        asset_urls.add(absolute_url)
            
        except Exception as e:
            logger.error(f"Error extracting asset URLs from {base_url}: {e}")
        
        return asset_urls

======= scrape_tool/scrapers/httrack.py ======
# Copyright 2025 Franz und Franz GmbH
#
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#     http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.

"""HTTrack-based web scraper implementation with Python fallback."""

import asyncio
import logging
import os
import shutil
import shlex
import tempfile
from pathlib import Path
from typing import AsyncGenerator, Optional
from urllib.parse import urlparse, urljoin

from .base import WebScraperBase, ScrapedPage, ScraperConfig
from .python_mirror import PythonMirrorScraper

logger = logging.getLogger(__name__)


class HTTrackScraper(PythonMirrorScraper):
    """HTTrack-based web scraper with Python fallback.

    This scraper attempts to use HTTrack for website mirroring, but falls back
    to a pure Python implementation if HTTrack is not available or fails.
    """

    def __init__(self, config: ScraperConfig):
        """Initialize the HTTrack scraper.

        Args:
            config: Scraper configuration
        """
        super().__init__(config)
        self.httrack_path = shutil.which("httrack")
        self.use_httrack = bool(self.httrack_path)
        if not self.use_httrack:
            logger.warning(
                "HTTrack not found. Using Python-based mirroring instead. "
                "For better performance, install HTTrack: "
                "apt-get install httrack (Linux) or "
                "brew install httrack (macOS)"
            )
        self.temp_dir: Optional[Path] = None

    async def __aenter__(self):
        """Create temporary directory for HTTrack output and initialize parent."""
        # Initialize parent context manager for Python fallback
        await super().__aenter__()

        # Create temp dir for HTTrack
        self.temp_dir = Path(tempfile.mkdtemp(prefix="html2md_httrack_"))
        logger.debug(f"Created temporary directory: {self.temp_dir}")
        return self

    async def __aexit__(self, *args):
        """Clean up temporary directory and parent resources."""
        # Clean up parent resources
        await super().__aexit__(*args)

        # Clean up temp directory
        if self.temp_dir and self.temp_dir.exists():
            try:
                shutil.rmtree(self.temp_dir)
                logger.debug(f"Cleaned up temporary directory: {self.temp_dir}")
            except Exception as e:
                logger.warning(f"Failed to clean up temp directory: {e}")

    async def scrape_url(self, url: str) -> ScrapedPage:
        """Scrape a single URL using HTTrack or Python fallback.

        Note: HTTrack is designed for full site mirroring, so this method
        will create a minimal mirror and extract just the requested page.
        If HTTrack fails or is not available, uses Python implementation.

        Args:
            url: URL to scrape

        Returns:
            ScrapedPage object containing the scraped content
        """
        # If HTTrack is not available, use parent Python implementation
        if not self.use_httrack:
            return await super().scrape_url(url)

        # HTTrack has issues with localhost, use Python implementation
        parsed = urlparse(url)
        if parsed.hostname in ["localhost", "127.0.0.1", "::1"]:
            logger.info(f"Using Python implementation for localhost URL: {url}")
            return await super().scrape_url(url)

        if not self.temp_dir:
            raise RuntimeError("Scraper must be used as async context manager")

        # Create a subdirectory for this specific URL
        url_hash = str(hash(url))[-8:]
        output_dir = self.temp_dir / f"single_{url_hash}"
        output_dir.mkdir(exist_ok=True)

        # Build HTTrack command for single page
        # Properly escape all arguments to prevent command injection
        cmd = [
            self.httrack_path,
            url,  # URL is validated by validate_url method
            "-O",
            str(output_dir),
            "-r1",  # Depth 1 (just this page)
            "-%P",  # No external pages
            "-p1",  # Download HTML files
            "-%e0",  # Don't download error pages
            "--quiet",  # Quiet mode
            "--disable-security-limits",
            f"--user-agent={shlex.quote(self.config.user_agent)}",  # Escape user agent
            "--timeout=" + str(int(self.config.timeout)),
        ]

        if not self.config.verify_ssl:
            cmd.append("--assume-insecure")

        if self.config.ignore_get_params:
            cmd.append("-N0")  # Don't parse query strings

        # Run HTTrack
        logger.debug(f"Running HTTrack command: {' '.join(cmd)}")
        process = await asyncio.create_subprocess_exec(
            *cmd, stdout=asyncio.subprocess.PIPE, stderr=asyncio.subprocess.PIPE
        )

        stdout, stderr = await process.communicate()

        if process.returncode != 0:
            error_msg = stderr.decode("utf-8", errors="replace")
            logger.warning(
                f"HTTrack failed: {error_msg}. Falling back to Python implementation."
            )
            # Fall back to Python implementation
            return await super().scrape_url(url)

        # Find the downloaded file
        # HTTrack creates files in a domain subdirectory
        parsed_url = urlparse(url)

        # Try multiple possible locations
        possible_files = [
            # Domain/path structure
            output_dir / parsed_url.netloc / parsed_url.path.lstrip("/"),
            output_dir / parsed_url.netloc / (parsed_url.path.lstrip("/") + ".html"),
            output_dir / parsed_url.netloc / "index.html",
            # Sometimes HTTrack puts files directly in output dir
            output_dir / "index.html",
        ]

        # If path ends with /, add index.html
        if parsed_url.path.endswith("/") or not parsed_url.path:
            possible_files.insert(
                0,
                output_dir
                / parsed_url.netloc
                / parsed_url.path.lstrip("/")
                / "index.html",
            )

        expected_file = None
        for pf in possible_files:
            if pf.exists() and pf.is_file():
                expected_file = pf
                break

        if not expected_file:
            # Try to find any HTML file in the domain directory
            domain_dir = output_dir / parsed_url.netloc
            if domain_dir.exists():
                html_files = list(domain_dir.rglob("*.html"))
                # Exclude HTTrack's own index files
                html_files = [f for f in html_files if "hts-cache" not in str(f)]
                if html_files:
                    expected_file = html_files[0]

        if not expected_file:
            # Last resort: find any HTML file
            html_files = list(output_dir.rglob("*.html"))
            # Exclude HTTrack's own files and cache
            html_files = [
                f
                for f in html_files
                if "hts-cache" not in str(f) and f.name != "index.html"
            ]
            if html_files:
                expected_file = html_files[0]
            else:
                logger.warning(
                    f"HTTrack did not download any HTML files for {url}. Falling back to Python implementation."
                )
                # Fall back to Python implementation
                return await super().scrape_url(url)

        # Read the content
        try:
            content = expected_file.read_text(encoding="utf-8")
        except UnicodeDecodeError:
            content = expected_file.read_text(encoding="latin-1")
        except Exception as e:
            logger.warning(
                f"Failed to read HTTrack output file: {e}. Falling back to Python implementation."
            )
            return await super().scrape_url(url)

        # Extract title from content
        title = None
        if "<title>" in content and "</title>" in content:
            start = content.find("<title>") + 7
            end = content.find("</title>")
            title = content[start:end].strip()

        # For single URL scraping, we don't apply deduplication checks
        # but we still extract canonical URL for metadata
        canonical_url_found = None
        if "<link" in content and "canonical" in content:
            import re

            canonical_match = re.search(
                r'<link[^>]+rel=["\']canonical["\'][^>]+href=["\']([^"\']+)["\']',
                content,
                re.IGNORECASE,
            )
            if not canonical_match:
                canonical_match = re.search(
                    r'<link[^>]+href=["\']([^"\']+)["\'][^>]+rel=["\']canonical["\']',
                    content,
                    re.IGNORECASE,
                )
            if canonical_match:
                canonical_url_found = urljoin(url, canonical_match.group(1))

        return ScrapedPage(
            url=url,
            content=content,
            title=title,
            encoding="utf-8",
            normalized_url=url,
            canonical_url=canonical_url_found,
            content_checksum=None,  # Not calculated for single URL
        )

    async def scrape_site(self, start_url: str) -> AsyncGenerator[ScrapedPage, None]:
        """Scrape entire website using HTTrack or Python fallback.

        Args:
            start_url: URL to start crawling from

        Yields:
            ScrapedPage objects as they are scraped
        """
        # Initialize allowed paths configuration
        self._initialize_allowed_paths(start_url)
        # If HTTrack is not available, use parent Python implementation
        if not self.use_httrack:
            async for page in super().scrape_site(start_url):
                yield page
            return

        # HTTrack has issues with localhost, use Python implementation
        parsed = urlparse(start_url)
        if parsed.hostname in ["localhost", "127.0.0.1", "::1"]:
            logger.info(f"Using Python implementation for localhost URL: {start_url}")
            async for page in super().scrape_site(start_url):
                yield page
            return

        if not self.temp_dir:
            raise RuntimeError("Scraper must be used as async context manager")

        output_dir = self.temp_dir / "site"
        output_dir.mkdir(exist_ok=True)

        # Build HTTrack command with conservative settings for Cloudflare
        # Calculate connection rate (max 0.5 connections per second)
        connection_rate = min(0.5, 1 / self.config.request_delay)

        # Limit concurrent connections (max 2 for Cloudflare sites)
        concurrent_connections = min(2, self.config.concurrent_requests)

        cmd = [
            self.httrack_path,
            start_url,  # URL is validated by validate_url method
            "-O",
            str(output_dir),
            f"-r{999999 if self.config.max_depth == -1 else self.config.max_depth}",  # Max depth (-1 = unlimited)
            "-%P",  # No external pages
            "--quiet",  # Quiet mode
            "--disable-security-limits",
            f"--user-agent={shlex.quote(self.config.user_agent)}",  # Escape user agent
            "--timeout=" + str(int(self.config.timeout)),
            f"--sockets={concurrent_connections}",  # Max 2 connections
            f"--connection-per-second={connection_rate:.2f}",  # Max 0.5/sec
            f"--max-files={self.config.max_pages if self.config.max_pages != -1 else 999999999}",  # Use very large number for unlimited
            "--max-rate=100000",  # Limit bandwidth to 100KB/s
            "--min-rate=1000",  # Minimum 1KB/s
        ]

        # Parse start URL for domain and path restrictions
        parsed = urlparse(start_url)
        base_path = parsed.path.rstrip("/")

        # Add domain restrictions
        if self.config.allowed_domains:
            for domain in self.config.allowed_domains:
                cmd.extend(["+*" + domain + "*"])
        else:
            # Restrict to same domain by default
            cmd.extend(["+*" + parsed.netloc + "*"])

        # Add subdirectory restriction using base class configuration
        if self._allowed_path_configs:
            allowed_domains = set()  # Track domains we're allowing
            for allowed_domain_config, allowed_path_config in self._allowed_path_configs:
                if allowed_domain_config:
                    # Full URL specified
                    logger.info(
                        f"Restricting HTTrack crawl to URL: {allowed_domain_config}{allowed_path_config}"
                    )
                    # Allow the specified URL and everything under it
                    cmd.extend([f"+*{allowed_domain_config}{allowed_path_config}/*"])
                    allowed_domains.add(allowed_domain_config)
                else:
                    # Just a path - use the start URL's domain
                    logger.info(
                        f"Restricting HTTrack crawl to allowed path: {allowed_path_config}"
                    )
                    # Allow the specified path and everything under it
                    cmd.extend([f"+*{parsed.netloc}{allowed_path_config}/*"])
                    allowed_domains.add(parsed.netloc)
            
            # Exclude everything else on each domain we're allowing from
            for domain in allowed_domains:
                cmd.extend([f"-*{domain}/*"])
        elif base_path:
            logger.info(f"Restricting HTTrack crawl to subdirectory: {base_path}")
            # Allow the base path and everything under it
            cmd.extend([f"+*{parsed.netloc}{base_path}/*"])
            # Exclude everything else on the same domain
            cmd.extend([f"-*{parsed.netloc}/*"])

        # Add exclusions
        if self.config.exclude_patterns:
            for pattern in self.config.exclude_patterns:
                cmd.extend(["-*" + pattern + "*"])

        if not self.config.verify_ssl:
            cmd.append("--assume-insecure")

        if self.config.ignore_get_params:
            cmd.append("-N0")  # Don't parse query strings

        if self.config.respect_robots_txt:
            cmd.append("--robots=3")  # Respect robots.txt

        # Run HTTrack
        logger.info(f"Starting HTTrack crawl from {start_url}")
        logger.debug(f"HTTrack command: {' '.join(cmd)}")

        process = await asyncio.create_subprocess_exec(
            *cmd, stdout=asyncio.subprocess.PIPE, stderr=asyncio.subprocess.PIPE
        )

        # Wait for HTTrack to complete
        stdout, stderr = await process.communicate()

        if process.returncode != 0:
            error_msg = stderr.decode("utf-8", errors="replace")
            logger.error(f"HTTrack failed: {error_msg}")
            # Continue to process any files that were downloaded

        # Find all downloaded HTML files
        html_files = list(output_dir.rglob("*.html"))
        logger.info(f"HTTrack downloaded {len(html_files)} HTML files")

        # Yield each file as a ScrapedPage
        for html_file in html_files:
            # Skip HTTrack's own files
            if html_file.name in ("index.html", "hts-log.txt", "hts-cache"):
                continue

            try:
                # Reconstruct URL from file path
                rel_path = html_file.relative_to(output_dir)
                parts = rel_path.parts

                # First part should be domain
                if len(parts) > 0:
                    domain = parts[0]
                    path_parts = parts[1:] if len(parts) > 1 else []

                    # Reconstruct URL
                    parsed_start = urlparse(start_url)
                    url = f"{parsed_start.scheme}://{domain}/" + "/".join(path_parts)

                    # Remove .html extension if it wasn't in original
                    if url.endswith("/index.html"):
                        url = url[:-11]  # Remove /index.html
                    elif url.endswith(".html") and ".html" not in start_url:
                        url = url[:-5]  # Remove .html

                    # Read content
                    try:
                        content = html_file.read_text(encoding="utf-8")
                    except UnicodeDecodeError:
                        content = html_file.read_text(encoding="latin-1")

                    # Store metadata for database
                    normalized_url = url.rstrip("/")
                    if self.config.ignore_get_params and "?" in normalized_url:
                        normalized_url = normalized_url.split("?")[0]
                    canonical_url_found = None
                    content_checksum = None

                    # Order: 1. GET parameter normalization (already done above)
                    # 2. Canonical URL check
                    if self.config.check_canonical:
                        # Simple regex-based extraction for canonical URL
                        import re

                        canonical_match = re.search(
                            r'<link[^>]+rel=["\']canonical["\'][^>]+href=["\']([^"\']+)["\']',
                            content,
                            re.IGNORECASE,
                        )
                        if not canonical_match:
                            # Try alternate order
                            canonical_match = re.search(
                                r'<link[^>]+href=["\']([^"\']+)["\'][^>]+rel=["\']canonical["\']',
                                content,
                                re.IGNORECASE,
                            )

                        if canonical_match:
                            canonical_url_found = canonical_match.group(1)
                            # Make canonical URL absolute
                            canonical_url_found = urljoin(url, canonical_url_found)
                            # Normalize canonical URL too
                            normalized_canonical = canonical_url_found.rstrip("/")
                            if (
                                self.config.ignore_get_params
                                and "?" in normalized_canonical
                            ):
                                normalized_canonical = normalized_canonical.split("?")[
                                    0
                                ]

                            if normalized_url != normalized_canonical:
                                # Check if we should respect the canonical URL
                                should_skip = True

                                # Check allowed paths (single or multiple)
                                allowed_paths_list = []
                                if hasattr(self.config, 'allowed_paths') and self.config.allowed_paths:
                                    allowed_paths_list = self.config.allowed_paths
                                elif self.config.allowed_path:
                                    allowed_paths_list = [self.config.allowed_path]
                                
                                if allowed_paths_list:
                                    # Parse URLs to check paths
                                    current_parsed = urlparse(normalized_url)
                                    canonical_parsed = urlparse(normalized_canonical)

                                    # If current URL is within any allowed_path but canonical is outside all,
                                    # don't skip - the user explicitly wants content from allowed_path
                                    current_in_allowed = any(
                                        current_parsed.path.startswith(allowed_path)
                                        for allowed_path in allowed_paths_list
                                    )
                                    canonical_in_allowed = any(
                                        canonical_parsed.path.startswith(allowed_path)
                                        for allowed_path in allowed_paths_list
                                    )
                                    
                                    if current_in_allowed and not canonical_in_allowed:
                                        should_skip = False
                                        logger.info(
                                            f"Not skipping {url} - canonical URL {canonical_url_found} is outside allowed_paths {allowed_paths_list}"
                                        )

                                if should_skip:
                                    logger.info(
                                        f"Skipping {url} - canonical URL differs: {canonical_url_found}"
                                    )
                                    continue  # Skip this file

                    # 3. Content duplicate check
                    if self.config.check_content_duplicates:
                        from ..utils import calculate_content_checksum

                        content_checksum = calculate_content_checksum(content)

                        # Check if checksum exists using callback
                        if self._checksum_callback and self._checksum_callback(
                            content_checksum
                        ):
                            logger.info(f"Skipping {url} - duplicate content detected")
                            continue  # Skip this file

                    # Extract title
                    title = None
                    if "<title>" in content and "</title>" in content:
                        start_idx = content.find("<title>") + 7
                        end_idx = content.find("</title>")
                        title = content[start_idx:end_idx].strip()

                    self.mark_visited(url)

                    yield ScrapedPage(
                        url=url,
                        content=content,
                        title=title,
                        encoding="utf-8",
                        normalized_url=normalized_url,
                        canonical_url=canonical_url_found,
                        content_checksum=content_checksum,
                    )

            except Exception as e:
                logger.error(f"Error processing {html_file}: {e}")
                continue

======= scrape_tool/scrapers/playwright.py ======
#!/usr/bin/env python3
# Copyright 2025 Franz und Franz GmbH
#
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#     http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.

"""Playwright scraper backend for JavaScript-heavy websites."""

import asyncio
import logging
from typing import AsyncIterator, Set, Optional, Dict, Any, TYPE_CHECKING
from urllib.parse import urljoin, urlparse
import re

try:
    from playwright.async_api import async_playwright, Browser, BrowserContext, Page

    PLAYWRIGHT_AVAILABLE = True
except ImportError:
    PLAYWRIGHT_AVAILABLE = False
    if TYPE_CHECKING:
        from playwright.async_api import Page
    else:
        Page = Any

from .base import WebScraperBase, ScrapedPage, ScraperConfig

logger = logging.getLogger(__name__)


class PlaywrightScraper(WebScraperBase):
    """Browser-based scraper using Playwright for JavaScript rendering.

    Playwright provides:
    - Full JavaScript execution and rendering
    - Support for SPAs and dynamic content
    - Multiple browser engines (Chromium, Firefox, WebKit)
    - Screenshot and PDF generation capabilities
    - Network interception and modification
    - Mobile device emulation

    Best for:
    - JavaScript-heavy websites and SPAs
    - Sites requiring user interaction (clicking, scrolling)
    - Content behind authentication
    - Visual regression testing
    """

    def __init__(self, config: ScraperConfig):
        """Initialize the Playwright scraper.

        Args:
            config: Scraper configuration

        Raises:
            ImportError: If playwright is not installed
        """
        if not PLAYWRIGHT_AVAILABLE:
            raise ImportError(
                "playwright is required for this scraper. "
                "Install with: pip install playwright && playwright install"
            )

        super().__init__(config)
        self._playwright = None
        self._browser: Optional[Browser] = None
        self._context: Optional[BrowserContext] = None
        self._visited_urls: Set[str] = set()

        # Browser configuration from scraper config
        self._browser_config = config.__dict__.get("browser_config", {})
        self._browser_type = self._browser_config.get("browser", "chromium")
        self._headless = self._browser_config.get("headless", True)
        self._viewport = self._browser_config.get(
            "viewport", {"width": 1920, "height": 1080}
        )
        self._wait_until = self._browser_config.get("wait_until", "networkidle")
        self._wait_timeout = self._browser_config.get("wait_timeout", 30000)

    async def __aenter__(self):
        """Enter async context and launch browser."""
        self._playwright = await async_playwright().start()

        # Launch browser based on type
        if self._browser_type == "firefox":
            self._browser = await self._playwright.firefox.launch(
                headless=self._headless
            )
        elif self._browser_type == "webkit":
            self._browser = await self._playwright.webkit.launch(
                headless=self._headless
            )
        else:  # Default to chromium
            self._browser = await self._playwright.chromium.launch(
                headless=self._headless
            )

        # Create browser context with custom user agent
        # Add option to control SSL validation (default to secure)
        ignore_https_errors = getattr(self.config, "ignore_https_errors", False)

        self._context = await self._browser.new_context(
            user_agent=self.config.user_agent,
            viewport=self._viewport,
            ignore_https_errors=ignore_https_errors,  # Only ignore if explicitly configured
            accept_downloads=False,
        )

        # Set default timeout
        self._context.set_default_timeout(self._wait_timeout)

        return self

    async def __aexit__(self, exc_type, exc_val, exc_tb):
        """Exit async context and cleanup."""
        if self._context:
            await self._context.close()
        if self._browser:
            await self._browser.close()
        if self._playwright:
            await self._playwright.stop()

    async def scrape_url(self, url: str) -> ScrapedPage:
        """Scrape a single URL using Playwright.

        Args:
            url: URL to scrape

        Returns:
            ScrapedPage object with scraped content

        Raises:
            Exception: If scraping fails
        """
        if not self._context:
            raise RuntimeError("Scraper not initialized. Use async context manager.")

        page = None
        try:
            # Check robots.txt before scraping
            if not await self.can_fetch(url):
                raise ValueError(f"URL {url} is blocked by robots.txt")

            # Create new page
            page = await self._context.new_page()

            # Navigate to URL
            response = await page.goto(url, wait_until=self._wait_until)

            if not response:
                raise Exception(f"Failed to navigate to {url}")

            # Wait for any additional dynamic content
            if self._browser_config.get("wait_for_selector"):
                await page.wait_for_selector(
                    self._browser_config["wait_for_selector"],
                    timeout=self._wait_timeout,
                )

            # Execute any custom JavaScript (with security warning)
            if self._browser_config.get("execute_script"):
                script = self._browser_config["execute_script"]

                # Basic validation to prevent obvious malicious scripts
                dangerous_patterns = [
                    "fetch",
                    "XMLHttpRequest",
                    "eval",
                    "Function",
                    "localStorage",
                    "sessionStorage",
                    "document.cookie",
                    "window.location",
                    "navigator",
                    "WebSocket",
                ]

                script_lower = script.lower()
                for pattern in dangerous_patterns:
                    if pattern.lower() in script_lower:
                        logger.warning(
                            f"Potentially dangerous JavaScript pattern '{pattern}' detected in script. Skipping execution."
                        )
                        break
                else:
                    # Only execute if no dangerous patterns found
                    logger.warning(
                        "Executing custom JavaScript. This feature should only be used with trusted scripts."
                    )
                    try:
                        await page.evaluate(script)
                    except Exception as e:
                        logger.error(f"Error executing custom JavaScript: {e}")

            # Get the final rendered HTML
            content = await page.content()

            # Extract title
            title = await page.title()

            # Extract metadata
            metadata = await self._extract_metadata(page)

            # Get response headers and status
            status_code = response.status
            headers = response.headers

            # Detect encoding
            encoding = "utf-8"  # Default for rendered content

            # Take screenshot if configured
            if self._browser_config.get("screenshot"):
                screenshot_path = self._browser_config.get(
                    "screenshot_path", "screenshot.png"
                )
                await page.screenshot(path=screenshot_path, full_page=True)
                metadata["screenshot"] = screenshot_path

            return ScrapedPage(
                url=page.url,  # Use final URL after redirects
                content=content,
                title=title,
                encoding=encoding,
                status_code=status_code,
                headers=headers,
                metadata=metadata,
            )

        except Exception as e:
            logger.error(f"Error scraping {url} with Playwright: {e}")
            raise
        finally:
            if page:
                await page.close()

    async def _extract_metadata(self, page: Page) -> Dict[str, Any]:
        """Extract metadata from the page."""
        metadata = {}

        # Extract meta tags
        meta_tags = await page.evaluate(
            """
            () => {
                const metadata = {};
                
                // Get description
                const desc = document.querySelector('meta[name="description"]');
                if (desc) metadata.description = desc.content;
                
                // Get keywords
                const keywords = document.querySelector('meta[name="keywords"]');
                if (keywords) metadata.keywords = keywords.content;
                
                // Get Open Graph tags
                document.querySelectorAll('meta[property^="og:"]').forEach(tag => {
                    const prop = tag.getAttribute('property');
                    if (prop && tag.content) {
                        metadata[prop] = tag.content;
                    }
                });
                
                // Get Twitter Card tags
                document.querySelectorAll('meta[name^="twitter:"]').forEach(tag => {
                    const name = tag.getAttribute('name');
                    if (name && tag.content) {
                        metadata[name] = tag.content;
                    }
                });
                
                // Get canonical URL
                const canonical = document.querySelector('link[rel="canonical"]');
                if (canonical) metadata.canonical = canonical.href;
                
                // Get page load time
                if (window.performance && window.performance.timing) {
                    const timing = window.performance.timing;
                    metadata.load_time = timing.loadEventEnd - timing.navigationStart;
                }
                
                return metadata;
            }
        """
        )

        metadata.update(meta_tags)

        # Add browser info
        metadata["browser"] = self._browser_type
        metadata["viewport"] = self._viewport

        return metadata

    async def scrape_site(self, start_url: str) -> AsyncIterator[ScrapedPage]:
        """Scrape an entire website using Playwright.

        Args:
            start_url: Starting URL for crawling

        Yields:
            ScrapedPage objects for each scraped page
        """
        if not self._context:
            raise RuntimeError("Scraper not initialized. Use async context manager.")

        # Parse start URL to get base domain
        parsed_start = urlparse(start_url)
        base_domain = parsed_start.netloc

        # Initialize allowed paths configuration
        self._initialize_allowed_paths(start_url)

        # Initialize queue
        queue = asyncio.Queue()
        await queue.put((start_url, 0))  # (url, depth)

        # Track visited URLs
        self._visited_urls.clear()
        self._visited_urls.add(start_url)

        # Semaphore for concurrent pages
        semaphore = asyncio.Semaphore(self.config.concurrent_requests)

        # Active tasks
        tasks = set()
        pages_scraped = 0

        async def process_url(url: str, depth: int):
            """Process a single URL and extract links."""
            async with semaphore:
                page = None
                try:
                    # Respect request delay
                    if self.config.request_delay > 0:
                        await asyncio.sleep(self.config.request_delay)

                    # Check robots.txt before scraping
                    if not await self.can_fetch(url):
                        logger.info(f"Skipping {url} - blocked by robots.txt")
                        return None

                    # Create new page
                    page = await self._context.new_page()

                    # Navigate to URL
                    response = await page.goto(url, wait_until=self._wait_until)

                    if not response:
                        logger.warning(f"Failed to navigate to {url}")
                        return None

                    # Wait for dynamic content
                    if self._browser_config.get("wait_for_selector"):
                        await page.wait_for_selector(
                            self._browser_config["wait_for_selector"],
                            timeout=self._wait_timeout,
                        )

                    # Get content
                    content = await page.content()
                    title = await page.title()
                    metadata = await self._extract_metadata(page)

                    # Check canonical URL if enabled
                    if self.config.check_canonical and metadata.get("canonical"):
                        canonical_url = metadata["canonical"]
                        normalized_current = self._normalize_url(page.url)
                        normalized_canonical = self._normalize_url(canonical_url)

                        if normalized_current != normalized_canonical:
                            # Check if we should respect the canonical URL
                            should_skip = True

                            # Check allowed paths using configured paths
                            if self._allowed_path_configs:
                                # Parse URLs to check paths
                                current_parsed = urlparse(normalized_current)
                                canonical_parsed = urlparse(normalized_canonical)

                                # If current URL is within any allowed_path but canonical is outside all,
                                # don't skip - the user explicitly wants content from allowed_path
                                current_in_allowed = self._is_path_allowed(normalized_current, start_url)
                                canonical_in_allowed = self._is_path_allowed(normalized_canonical, start_url)
                                
                                if current_in_allowed and not canonical_in_allowed:
                                    should_skip = False
                                    logger.info(
                                        f"Not skipping {url} - canonical URL {canonical_url} is outside allowed_paths {self._allowed_path_configs}"
                                    )

                            if should_skip:
                                logger.info(
                                    f"Skipping {url} - canonical URL differs: {canonical_url}"
                                )
                                visited.add(url)
                                return None  # Skip this page

                    # Create scraped page
                    scraped_page = ScrapedPage(
                        url=page.url,
                        content=content,
                        title=title,
                        encoding="utf-8",
                        status_code=response.status,
                        headers=response.headers,
                        metadata=metadata,
                    )

                    # Extract links if not at max depth
                    if self.config.max_depth == -1 or depth < self.config.max_depth:
                        # Get all links using JavaScript
                        links = await page.evaluate(
                            """
                            () => {
                                return Array.from(document.querySelectorAll('a[href]'))
                                    .map(a => a.href)
                                    .filter(href => href && (href.startsWith('http://') || href.startsWith('https://')));
                            }
                        """
                        )

                        for link in links:
                            # Parse URL
                            parsed_url = urlparse(link)

                            # Skip if different domain (unless allowed)
                            if self.config.allowed_domains:
                                if parsed_url.netloc not in self.config.allowed_domains:
                                    continue
                            elif parsed_url.netloc != base_domain:
                                continue

                            # Check path restriction using base class method
                            if not self._is_path_allowed(link, start_url):
                                logger.debug(
                                    f"Skipping {link} - outside allowed paths {self._allowed_path_configs}"
                                )
                                continue

                            # Skip if URL should be excluded
                            if self._should_exclude_url(link):
                                continue

                            # Skip if already visited
                            if link in self._visited_urls:
                                continue

                            # Add to queue
                            self._visited_urls.add(link)
                            await queue.put((link, depth + 1))

                    return scraped_page

                except Exception as e:
                    logger.error(f"Error processing {url}: {e}")
                    return None
                finally:
                    if page:
                        await page.close()

        # Process URLs from queue
        while not queue.empty() or tasks:
            # Start new tasks if under limit
            while not queue.empty() and len(tasks) < self.config.concurrent_requests:
                try:
                    url, depth = queue.get_nowait()

                    # Check max pages limit (skip if -1 for unlimited)
                    if (
                        self.config.max_pages != -1
                        and pages_scraped >= self.config.max_pages
                    ):
                        break

                    # Create task
                    task = asyncio.create_task(process_url(url, depth))
                    tasks.add(task)

                except asyncio.QueueEmpty:
                    break

            # Wait for at least one task to complete
            if tasks:
                done, tasks = await asyncio.wait(
                    tasks, return_when=asyncio.FIRST_COMPLETED
                )

                # Process completed tasks
                for task in done:
                    page = await task
                    if page:
                        pages_scraped += 1
                        yield page

                        # Check if we've hit the page limit (skip if -1 for unlimited)
                        if (
                            self.config.max_pages != -1
                            and pages_scraped >= self.config.max_pages
                        ):
                            # Cancel remaining tasks
                            for t in tasks:
                                t.cancel()
                            return

        logger.info(f"Playwright scraping complete. Scraped {pages_scraped} pages")

======= scrape_tool/scrapers/python_mirror.py ======
#!/usr/bin/env python3
# Copyright 2025 Franz und Franz GmbH
#
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#     http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.

"""Python-based web scraper implementation for complete website mirroring."""

import asyncio
import logging
from pathlib import Path
from typing import AsyncGenerator, Set, Dict, Optional
from urllib.parse import urlparse, urljoin, unquote
import os
import re

import aiohttp
import aiofiles
from bs4 import BeautifulSoup

from .base import WebScraperBase, ScrapedPage, ScraperConfig

logger = logging.getLogger(__name__)


class PythonMirrorScraper(WebScraperBase):
    """Python-based web scraper for complete website mirroring.

    This scraper downloads and saves web pages to disk, preserving the
    directory structure similar to HTTrack but using pure Python.
    """

    def __init__(self, config: ScraperConfig):
        """Initialize the Python mirror scraper.

        Args:
            config: Scraper configuration
        """
        super().__init__(config)
        self.session: Optional[aiohttp.ClientSession] = None
        self._semaphore = asyncio.Semaphore(config.concurrent_requests)
        self.output_dir: Optional[Path] = None

    def _normalize_url(self, url: str) -> str:
        """Normalize URL by removing GET parameters if configured.

        Args:
            url: URL to normalize

        Returns:
            Normalized URL
        """
        if self.config.ignore_get_params and "?" in url:
            return url.split("?")[0]
        return url

    async def __aenter__(self):
        """Create aiohttp session on entry."""
        headers = {}
        if self.config.user_agent:
            headers["User-Agent"] = self.config.user_agent
        if self.config.custom_headers:
            headers.update(self.config.custom_headers)

        timeout = aiohttp.ClientTimeout(total=self.config.timeout)
        connector = aiohttp.TCPConnector(
            ssl=self.config.verify_ssl, limit=self.config.concurrent_requests * 2
        )

        self.session = aiohttp.ClientSession(
            headers=headers, timeout=timeout, connector=connector
        )
        return self

    async def __aexit__(self, *args):
        """Close aiohttp session on exit."""
        if self.session and not self.session.closed:
            await self.session.close()
            await asyncio.sleep(0.25)

    def _url_to_filepath(self, url: str, output_dir: Path) -> Path:
        """Convert URL to local file path.

        Args:
            url: URL to convert
            output_dir: Base output directory

        Returns:
            Local file path
        """
        parsed = urlparse(url)

        # Create domain directory
        domain_dir = output_dir / parsed.netloc

        # Handle path
        path = parsed.path.strip("/")
        if not path or path.endswith("/"):
            path = (path or "") + "index.html"
        elif "." not in os.path.basename(path):
            # No extension, assume HTML
            path = path + ".html"

        # Create full file path
        file_path = domain_dir / path

        # Ensure parent directory exists
        file_path.parent.mkdir(parents=True, exist_ok=True)

        return file_path

    async def _save_page(self, page: ScrapedPage, output_dir: Path) -> Path:
        """Save a scraped page to disk.

        Args:
            page: Scraped page to save
            output_dir: Output directory

        Returns:
            Path to saved file
        """
        file_path = self._url_to_filepath(page.url, output_dir)

        try:
            async with aiofiles.open(file_path, "w", encoding=page.encoding) as f:
                await f.write(page.content)
            logger.info(f"Saved {page.url} to {file_path}")
            return file_path
        except Exception as e:
            logger.error(f"Error saving {page.url}: {e}")
            raise

    async def scrape_url(self, url: str) -> ScrapedPage:
        """Scrape a single URL.

        Args:
            url: URL to scrape

        Returns:
            ScrapedPage object containing the scraped content

        Raises:
            Exception: If scraping fails
        """
        if not self.session:
            raise RuntimeError("Scraper must be used as async context manager")

        async with self._semaphore:
            try:
                logger.info(f"Scraping URL: {url}")

                async with self.session.get(
                    url, allow_redirects=self.config.follow_redirects
                ) as response:
                    status_code = response.status
                    headers = dict(response.headers)

                    # Get content
                    content = await response.text()

                    # Parse with BeautifulSoup
                    soup = BeautifulSoup(content, "html.parser")

                    # Extract title
                    title = soup.find("title")
                    title_text = title.get_text(strip=True) if title else None

                    # Extract metadata
                    metadata = {}
                    for meta in soup.find_all("meta"):
                        name = meta.get("name") or meta.get("property")
                        content_value = meta.get("content", "")
                        if name and content_value:
                            metadata[str(name)] = content_value

                    return ScrapedPage(
                        url=str(response.url),
                        content=str(soup),
                        title=title_text,
                        metadata=metadata,
                        encoding=response.get_encoding() or "utf-8",
                        status_code=status_code,
                        headers=headers,
                    )

            except Exception as e:
                logger.error(f"Error scraping {url}: {e}")
                raise

    async def scrape_site(self, start_url: str) -> AsyncGenerator[ScrapedPage, None]:
        """Scrape entire website starting from URL.

        Args:
            start_url: Starting URL for crawling

        Yields:
            ScrapedPage objects for each scraped page
        """

        # Parse start URL
        parsed_start = urlparse(start_url)
        base_domain = parsed_start.netloc

        # URLs to visit
        to_visit: Set[str] = {start_url}
        visited: Set[str] = set()
        depth_map: Dict[str, int] = {start_url: 0}

        # Ensure we're in context
        if not self.session:
            await self.__aenter__()

        pages_scraped = 0

        while to_visit and (
            self.config.max_pages == -1 or pages_scraped < self.config.max_pages
        ):
            url = to_visit.pop()

            # Skip if already visited
            normalized_url = self._normalize_url(url)
            if normalized_url in visited:
                continue

            visited.add(normalized_url)

            # Check depth
            current_depth = depth_map.get(url, 0)
            if self.config.max_depth != -1 and current_depth > self.config.max_depth:
                continue

            # Check if URL should be scraped
            if not await self.validate_url(url):
                continue

            # Check robots.txt
            if not await self.can_fetch(url):
                logger.info(f"Skipping {url} - blocked by robots.txt")
                continue

            try:
                # Scrape the page
                page = await self.scrape_url(url)

                # Save to disk if output_dir is set (optional for compatibility)
                if self.output_dir:
                    await self._save_page(page, self.output_dir)

                yield page
                pages_scraped += 1

                # Extract links if not at max depth
                if self.config.max_depth == -1 or current_depth < self.config.max_depth:
                    soup = BeautifulSoup(page.content, "html.parser")

                    for tag in soup.find_all("a", href=True):
                        href = tag["href"]
                        absolute_url = urljoin(url, href)

                        # Remove fragment
                        absolute_url = absolute_url.split("#")[0]

                        # Check if same domain
                        parsed_url = urlparse(absolute_url)
                        if parsed_url.netloc != base_domain:
                            continue

                        # Skip non-HTTP URLs
                        if parsed_url.scheme not in ("http", "https"):
                            continue

                        # Add to queue
                        normalized_new = self._normalize_url(absolute_url)
                        if normalized_new not in visited:
                            to_visit.add(absolute_url)
                            depth_map[absolute_url] = current_depth + 1

                # Respect rate limit
                if self.config.request_delay > 0:
                    await asyncio.sleep(self.config.request_delay)

            except Exception as e:
                logger.error(f"Error processing {url}: {e}")
                continue

        logger.info(f"Crawl complete. Scraped {pages_scraped} pages")

    def set_output_dir(self, output_dir: Path):
        """Set the output directory for saving scraped pages.

        Args:
            output_dir: Directory to save pages to
        """
        self.output_dir = output_dir
        self.output_dir.mkdir(parents=True, exist_ok=True)

======= scrape_tool/scrapers/selectolax.py ======
#!/usr/bin/env python3
# Copyright 2025 Franz und Franz GmbH
#
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#     http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.

"""httpx + selectolax scraper backend for blazing fast HTML parsing."""

import asyncio
import logging
from typing import AsyncIterator, Set, Optional, Dict, Any
from urllib.parse import urljoin, urlparse
import re

try:
    import httpx
    from selectolax.parser import HTMLParser

    HTTPX_AVAILABLE = True
except ImportError:
    HTTPX_AVAILABLE = False

from .base import WebScraperBase, ScrapedPage, ScraperConfig

logger = logging.getLogger(__name__)


class SelectolaxScraper(WebScraperBase):
    """High-performance scraper using httpx for async HTTP and selectolax for parsing.

    This scraper is optimized for speed and low memory usage. It uses:
    - httpx: Modern async HTTP client with connection pooling
    - selectolax: Blazing fast HTML parser built on C libraries

    Best for:
    - Large-scale scraping where performance is critical
    - Simple HTML parsing without JavaScript
    - Minimal resource usage requirements
    """

    def __init__(self, config: ScraperConfig):
        """Initialize the selectolax scraper.

        Args:
            config: Scraper configuration

        Raises:
            ImportError: If httpx or selectolax are not installed
        """
        if not HTTPX_AVAILABLE:
            raise ImportError(
                "httpx and selectolax are required for this scraper. "
                "Install with: pip install httpx selectolax"
            )

        super().__init__(config)
        self._client: Optional[httpx.AsyncClient] = None
        self._visited_urls: Set[str] = set()

    def _normalize_url(self, url: str) -> str:
        """Normalize URL by removing GET parameters if configured.

        Args:
            url: URL to normalize

        Returns:
            Normalized URL
        """
        if self.config.ignore_get_params and "?" in url:
            return url.split("?")[0]
        return url

    async def __aenter__(self):
        """Enter async context and create HTTP client."""
        # Configure client with connection pooling for performance
        limits = httpx.Limits(
            max_keepalive_connections=20,
            max_connections=self.config.concurrent_requests * 2,
            keepalive_expiry=30.0,
        )

        self._client = httpx.AsyncClient(
            timeout=httpx.Timeout(self.config.timeout),
            limits=limits,
            follow_redirects=self.config.follow_redirects,
            headers={"User-Agent": self.config.user_agent},
        )

        return self

    async def __aexit__(self, exc_type, exc_val, exc_tb):
        """Exit async context and cleanup."""
        if self._client:
            await self._client.aclose()

    async def scrape_url(self, url: str) -> ScrapedPage:
        """Scrape a single URL.

        Args:
            url: URL to scrape

        Returns:
            ScrapedPage object with scraped content

        Raises:
            Exception: If scraping fails
        """
        if not self._client:
            raise RuntimeError("Scraper not initialized. Use async context manager.")

        try:
            # Check robots.txt before scraping
            if not await self.can_fetch(url):
                raise ValueError(f"URL {url} is blocked by robots.txt")

            # Make HTTP request
            response = await self._client.get(url)
            response.raise_for_status()

            # Parse HTML with selectolax
            html_parser = HTMLParser(response.text)

            # Store metadata for database
            normalized_url = self._normalize_url(str(response.url))
            canonical_url = None
            content_checksum = None

            # Order: 1. GET parameter normalization (already done in _normalize_url)
            # 2. Canonical URL check
            if self.config.check_canonical:
                canonical_link = html_parser.css_first('link[rel="canonical"]')
                if canonical_link and canonical_link.attributes.get("href"):
                    canonical_url = canonical_link.attributes["href"]
                    # Make canonical URL absolute
                    canonical_url = urljoin(url, canonical_url)
                    # Normalize canonical URL too
                    normalized_canonical = self._normalize_url(canonical_url)

                    if normalized_url != normalized_canonical:
                        # Check if we should respect the canonical URL
                        should_skip = True

                        # Check allowed paths (single or multiple)
                        allowed_paths_list = []
                        if hasattr(self.config, 'allowed_paths') and self.config.allowed_paths:
                            allowed_paths_list = self.config.allowed_paths
                        elif self.config.allowed_path:
                            allowed_paths_list = [self.config.allowed_path]
                        
                        if allowed_paths_list:
                            # Parse URLs to check paths
                            current_parsed = urlparse(normalized_url)
                            canonical_parsed = urlparse(normalized_canonical)

                            # If current URL is within any allowed_path but canonical is outside all,
                            # don't skip - the user explicitly wants content from allowed_path
                            current_in_allowed = any(
                                current_parsed.path.startswith(allowed_path)
                                for allowed_path in allowed_paths_list
                            )
                            canonical_in_allowed = any(
                                canonical_parsed.path.startswith(allowed_path)
                                for allowed_path in allowed_paths_list
                            )
                            
                            if current_in_allowed and not canonical_in_allowed:
                                should_skip = False
                                logger.info(
                                    f"Not skipping {url} - canonical URL {canonical_url} is outside allowed_paths {allowed_paths_list}"
                                )

                        if should_skip:
                            logger.info(
                                f"Skipping {url} - canonical URL differs: {canonical_url}"
                            )
                            return None  # Return None to indicate skip, not an error

            # 3. Content duplicate check
            if self.config.check_content_duplicates:
                from ..utils import calculate_content_checksum

                content_checksum = calculate_content_checksum(response.text)

                # Check if checksum exists using callback
                if self._checksum_callback and self._checksum_callback(
                    content_checksum
                ):
                    logger.info(f"Skipping {url} - duplicate content detected")
                    return None  # Return None to indicate skip, not an error

            # Extract title
            title = ""
            title_tag = html_parser.css_first("title")
            if title_tag:
                title = title_tag.text(strip=True)

            # Extract metadata
            metadata = {}

            # Get meta description
            meta_desc = html_parser.css_first('meta[name="description"]')
            if meta_desc:
                metadata["description"] = meta_desc.attributes.get("content", "")

            # Get meta keywords
            meta_keywords = html_parser.css_first('meta[name="keywords"]')
            if meta_keywords:
                metadata["keywords"] = meta_keywords.attributes.get("content", "")

            # Get Open Graph data
            for og_tag in html_parser.css('meta[property^="og:"]'):
                prop = og_tag.attributes.get("property", "")
                content = og_tag.attributes.get("content", "")
                if prop and content:
                    metadata[prop] = content

            # Detect encoding from meta tag or response
            encoding = response.encoding or "utf-8"
            meta_charset = html_parser.css_first("meta[charset]")
            if meta_charset:
                encoding = meta_charset.attributes.get("charset", encoding)
            else:
                # Check for http-equiv Content-Type
                meta_content_type = html_parser.css_first(
                    'meta[http-equiv="Content-Type"]'
                )
                if meta_content_type:
                    content = meta_content_type.attributes.get("content", "")
                    match = re.search(r"charset=([^;]+)", content)
                    if match:
                        encoding = match.group(1).strip()

            return ScrapedPage(
                url=str(response.url),  # Use final URL after redirects
                content=response.text,
                title=title,
                encoding=encoding,
                status_code=response.status_code,
                headers=dict(response.headers),
                metadata=metadata,
                normalized_url=normalized_url,
                canonical_url=canonical_url,
                content_checksum=content_checksum,
            )

        except httpx.HTTPError as e:
            logger.error(f"HTTP error scraping {url}: {e}")
            raise
        except Exception as e:
            logger.error(f"Error scraping {url}: {e}")
            raise

    async def scrape_site(self, start_url: str) -> AsyncIterator[ScrapedPage]:
        """Scrape an entire website starting from the given URL.

        Args:
            start_url: Starting URL for crawling

        Yields:
            ScrapedPage objects for each scraped page
        """
        if not self._client:
            raise RuntimeError("Scraper not initialized. Use async context manager.")

        # Parse start URL to get base domain
        parsed_start = urlparse(start_url)
        base_domain = parsed_start.netloc

        # Initialize allowed paths configuration
        self._initialize_allowed_paths(start_url)

        # Initialize queue with start URL
        queue = asyncio.Queue()
        await queue.put((start_url, 0))  # (url, depth)

        # Track visited URLs
        self._visited_urls.clear()
        normalized_start = self._normalize_url(start_url)
        self._visited_urls.add(normalized_start)

        # Semaphore for concurrent requests
        semaphore = asyncio.Semaphore(self.config.concurrent_requests)

        # Active tasks
        tasks = set()
        pages_scraped = 0

        async def process_url(url: str, depth: int):
            """Process a single URL."""
            async with semaphore:
                try:
                    # Respect request delay
                    if self.config.request_delay > 0:
                        await asyncio.sleep(self.config.request_delay)

                    # Scrape the page
                    page = await self.scrape_url(url)

                    # Skip if page is None (duplicate content or canonical mismatch)
                    if page is None:
                        return None

                    # Extract links if not at max depth
                    if self.config.max_depth == -1 or depth < self.config.max_depth:
                        html_parser = HTMLParser(page.content)

                        for link in html_parser.css("a[href]"):
                            href = link.attributes.get("href", "")
                            if not href:
                                continue

                            # Resolve relative URLs
                            absolute_url = urljoin(url, href)

                            # Parse URL
                            parsed_url = urlparse(absolute_url)

                            # Skip if different domain (unless allowed)
                            if self.config.allowed_domains:
                                if parsed_url.netloc not in self.config.allowed_domains:
                                    continue
                            elif parsed_url.netloc != base_domain:
                                continue

                            # Check path restriction using base class method
                            if not self._is_path_allowed(absolute_url, start_url):
                                continue

                            # Skip if URL should be excluded
                            if self._should_exclude_url(absolute_url):
                                continue

                            # Normalize URL
                            normalized_url = self._normalize_url(absolute_url)

                            # Skip if already visited
                            if normalized_url in self._visited_urls:
                                continue

                            # Skip non-HTTP(S) URLs
                            if parsed_url.scheme not in ("http", "https"):
                                continue

                            # Add to queue
                            self._visited_urls.add(normalized_url)
                            await queue.put((normalized_url, depth + 1))

                    return page

                except Exception as e:
                    logger.error(f"Error processing {url}: {e}")
                    return None

        # Process URLs from queue
        while not queue.empty() or tasks:
            # Start new tasks if under limit
            while not queue.empty() and len(tasks) < self.config.concurrent_requests:
                try:
                    url, depth = queue.get_nowait()

                    # Check max pages limit (skip if -1 for unlimited)
                    if (
                        self.config.max_pages != -1
                        and pages_scraped >= self.config.max_pages
                    ):
                        break

                    # Create task
                    task = asyncio.create_task(process_url(url, depth))
                    tasks.add(task)

                except asyncio.QueueEmpty:
                    break

            # Wait for at least one task to complete
            if tasks:
                done, tasks = await asyncio.wait(
                    tasks, return_when=asyncio.FIRST_COMPLETED
                )

                # Process completed tasks
                for task in done:
                    page = await task
                    if page:
                        pages_scraped += 1
                        yield page

                        # Check if we've hit the page limit (skip if -1 for unlimited)
                        if (
                            self.config.max_pages != -1
                            and pages_scraped >= self.config.max_pages
                        ):
                            # Cancel remaining tasks
                            for t in tasks:
                                t.cancel()
                            return

        logger.info(f"Scraping complete. Scraped {pages_scraped} pages")
